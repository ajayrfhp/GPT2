{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional embeddings\n",
    "\n",
    "## Why ?\n",
    "- Self attention model do not have inherent notion of position unlike RNNs. \n",
    "- Position, ordering matters in language. Same word in different order could mean different things. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa1c9ac3c0e4acf9845ccd0bed84b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4370b798cf435f9408c1dea674b031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db64542ce7444a038ed8d6eb47be0455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e116a91e8045de9d1f44123251536b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d70155066c49919ce429868b5a3d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b89865cbf49f587b31ecdf5bebd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837082e2e349477987ae2332ade6483e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 1536])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "text = \"The dog chased a different dog\"\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "embeddings = model.embed_tokens(tokens)\n",
    "print(embeddings.shape)  # (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desired properties for awesome positional encoding\n",
    "- Give unique encoding for each position in a given sequence. Token at position 5 has same encoding regardless of sequence length\n",
    "- Straight forward relationship between 2 encoded positions. If we know encoding for token at position p, it should be easy to infer encoding for same token if it occurs at p + k. \n",
    "- Generalized to different sequence length. \n",
    "- Deterministic\n",
    "- Extends naturally for multi models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer position encoding\n",
    "- Just add integer of position to each component of token embedding. It should work for known sequence lengths\n",
    "- Token integer will be on a different scale to the actual embedding. \n",
    "- If we normalize based on length, tokens in same position in different sequence will get different embedding\n",
    "- So this does not really work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary positional encoding\n",
    "- Encode position as a binary vector, stretch and add to embedding vector\n",
    "- Counting is jumpy and discrete. We need something smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed sinusoidal embedding \n",
    "- Each component of positional embedding vector is drawn alternatively from sine and cosine curves\n",
    "- For a given embedding dimension 2i, PE(x) = sin(x/10000^(2i/d))\n",
    "- No learned parameters, fixed sin and cosine embedding. Extrapolates to longer sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute positional embedding\n",
    "- Positional information is encoded through a trainable embedding matrix that converts integer positions into embedding vectors. \n",
    "- Different dimensions encode position information captured at different frequencies\n",
    "- Easy to implement with standard embedding layers. It has poor sequence length extrapolation because it lacks knowledge of relative positioning. \n",
    "\n",
    "\n",
    "\n",
    "### Relative positional embedding \n",
    "- Encodes relative distance between tokens rather than absolute positions. \n",
    "\n",
    "### Rotary positional embedding\n",
    "- Has both relative and absolute positions. \n",
    "\n",
    "\n",
    "## Resources\n",
    "- GO through https://huggingface.co/blog/designing-positional-encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Ajay\n",
      "[15496, 616, 1438, 318, 22028, 323]\n",
      "torch.Size([1, 5])\n",
      "tensor([[[-0.7183,  0.0627],\n",
      "         [-0.6489,  1.2228],\n",
      "         [ 0.7233, -0.0964],\n",
      "         [-0.4918,  0.6275],\n",
      "         [-0.1155, -0.1441]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "embedding_dim = 2\n",
    "max_context_length = 5\n",
    "\n",
    "input_text = \"Hello my name is Ajay\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(input_text)\n",
    "\n",
    "input_tokens = tokenizer.encode(input_text)\n",
    "print(input_tokens)\n",
    "\n",
    "input_tensor = torch.tensor(input_tokens[:max_context_length]).unsqueeze(0)\n",
    "print(input_tensor.shape)\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "input_embedding = token_embedding_layer(input_tensor)\n",
    "\n",
    "print(input_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of absolute position embedding vector: torch.Size([1, 5, 2])\n",
      "tensor([[[ 0.7474, -0.4037],\n",
      "         [-0.2403, -1.0787],\n",
      "         [ 0.7312,  0.1028],\n",
      "         [ 0.0545,  0.0521],\n",
      "         [ 0.6613,  0.8048]]], grad_fn=<EmbeddingBackward0>)\n",
      "Shape of input plus absolute position embedding: torch.Size([1, 5, 2])\n",
      "tensor([[[ 0.0291, -0.3410],\n",
      "         [-0.8892,  0.1441],\n",
      "         [ 1.4545,  0.0065],\n",
      "         [-0.4373,  0.6796],\n",
      "         [ 0.5458,  0.6607]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "absolute_position_embedding_layer = nn.Embedding(max_context_length, embedding_dim)\n",
    "\n",
    "absolute_position_embedding = absolute_position_embedding_layer(torch.arange(max_context_length).unsqueeze(0))\n",
    "\n",
    "print(f\"Shape of absolute position embedding vector: {absolute_position_embedding.shape}\")\n",
    "\n",
    "print(absolute_position_embedding)\n",
    "\n",
    "input_plus_absolute_position_embedding = input_embedding + absolute_position_embedding\n",
    "\n",
    "print(f\"Shape of input plus absolute position embedding: {input_plus_absolute_position_embedding.shape}\")\n",
    "\n",
    "print(input_plus_absolute_position_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
