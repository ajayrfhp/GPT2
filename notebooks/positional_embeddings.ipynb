{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional embeddings\n",
    "\n",
    "## Why ?\n",
    "- Self attention model do not have inherent notion of position unlike RNNs. \n",
    "- Position, ordering matters in language. Same word in different order could mean different things. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desired properties for awesome positional encoding\n",
    "- Give unique encoding for each position in a given sequence. Token at position 5 has same encoding regardless of sequence length\n",
    "- Straight forward relationship between 2 encoded positions. If we know encoding for token at position p, it should be easy to infer encoding for same token if it occurs at p + k. \n",
    "- Generalized to different sequence length. \n",
    "- Deterministic\n",
    "- Extends naturally for multi models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer position encoding\n",
    "- Just add integer of position to each component of token embedding. It should work for known sequence lengths\n",
    "- Token integer will be on a different scale to the actual embedding. \n",
    "- If we normalize based on length, tokens in same position in different sequence will get different embedding\n",
    "- So this does not really work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary positional encoding\n",
    "- Encode position as a binary vector, stretch and add to embedding vector\n",
    "- Counting is jumpy and discrete. We need something smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed sinusoidal embedding \n",
    "- Each component of positional embedding vector is drawn alternatively from sine and cosine curves\n",
    "- For a given embedding dimension 2i, PE(x) = sin(x/10000^(2i/d))\n",
    "- No learned parameters, fixed sin and cosine embedding. Extrapolates to longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Ajay. Hello my name is Ajay\n",
      "[15496, 616, 1438, 318, 22028, 323, 13, 18435, 616, 1438, 318, 22028, 323]\n",
      "torch.Size([1, 13])\n",
      "['Hello', 'Ġmy', 'Ġname', 'Ġis', 'ĠAj', 'ay', '.', 'ĠHello', 'Ġmy', 'Ġname', 'Ġis', 'ĠAj', 'ay']\n",
      "Dot product of token 1 and token 2: -0.18977676331996918\n",
      "Dot product of token 8 and token 9: -0.18977676331996918\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "embedding_dim = 2\n",
    "max_context_length = 100\n",
    "\n",
    "input_text = \"Hello my name is Ajay. Hello my name is Ajay\"\n",
    "# GPT2 model id\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(input_text)\n",
    "\n",
    "input_tokens = tokenizer.encode(input_text)\n",
    "print(input_tokens)\n",
    "\n",
    "input_tensor = torch.tensor(input_tokens[:max_context_length]).unsqueeze(0)\n",
    "print(input_tensor.shape)\n",
    "\n",
    "input_tokens_decoded = tokenizer.convert_ids_to_tokens(input_tensor[0].tolist())\n",
    "print(input_tokens_decoded)\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "input_embedding = token_embedding_layer(input_tensor)\n",
    "\n",
    "input_embedding_dot_product_1_2 = torch.dot(input_embedding[0][1], input_embedding[0][2])\n",
    "input_embedding_dot_product_8_9 = torch.dot(input_embedding[0][8], input_embedding[0][9])\n",
    "\n",
    "print(f\"Dot product of token 1 and token 2: {input_embedding_dot_product_1_2}\")\n",
    "print(f\"Dot product of token 8 and token 9: {input_embedding_dot_product_8_9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ġmy', 'Ġname', 'Ġmy', 'Ġname')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens_decoded[1], input_tokens_decoded[2], input_tokens_decoded[8], input_tokens_decoded[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute positional embedding\n",
    "- Positional information is encoded through a trainable embedding matrix that converts integer positions into embedding vectors. \n",
    "- Different dimensions encode position information captured at different frequencies\n",
    "- Easy to implement with standard embedding layers. It has poor sequence length extrapolation because it lacks knowledge of relative positioning.\n",
    "- Although the same set of tokens is present 1,2 and 8, 9, absolute positional encoding grants different scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of absolute position embedding vector: torch.Size([1, 13, 2])\n",
      "Shape of input plus absolute position embedding: torch.Size([1, 13, 2])\n",
      "Dot product between vectors 1 and 2: 0.1645088940858841\n",
      "Dot product between vectors 7 and 8: 0.35054731369018555\n"
     ]
    }
   ],
   "source": [
    "seq_length = input_embedding.shape[1]\n",
    "absolute_position_embedding_layer = nn.Embedding(seq_length, embedding_dim)\n",
    "\n",
    "absolute_position_embedding = absolute_position_embedding_layer(torch.arange(seq_length).unsqueeze(0))\n",
    "\n",
    "print(f\"Shape of absolute position embedding vector: {absolute_position_embedding.shape}\")\n",
    "\n",
    "\n",
    "input_plus_absolute_position_embedding = input_embedding + absolute_position_embedding\n",
    "\n",
    "print(f\"Shape of input plus absolute position embedding: {input_plus_absolute_position_embedding.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# get dot product between vectors 1 and 2\n",
    "dot_product_12 = torch.dot(input_plus_absolute_position_embedding[0][1], input_plus_absolute_position_embedding[0][2])\n",
    "dot_product_89 = torch.dot(input_plus_absolute_position_embedding[0][8], input_plus_absolute_position_embedding[0][9])\n",
    "print(f\"Dot product between vectors 1 and 2: {dot_product_12}\")\n",
    "print(f\"Dot product between vectors 7 and 8: {dot_product_89}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative positional embedding \n",
    "- Encodes relative distance between tokens rather than absolute positions. \n",
    "\n",
    "### Rotary positional embedding\n",
    "- Rotate the embedding vector based on a rotation angle that is a function of the position of the word in the sentence. \n",
    "Has both relative and absolute positional embedding.\n",
    "    - A word pair that is present in different points in sentence should have same dot product score\n",
    "\n",
    "\n",
    "\n",
    "- GO through https://huggingface.co/blog/designing-positional-encoding?\n",
    "- In Rope, token vector in position i is rotated by io and token vector in position j is rotated by jo. \n",
    "- dot_product(t_i, t_j) = t_i * t_j * cos(i-j)\n",
    "    - Only depends on distance between i and j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ROPE embedding vector: torch.Size([1, 13, 2])\n"
     ]
    }
   ],
   "source": [
    "rope_layer = RotaryPositionalEmbeddings(dim=2, max_seq_len=13)\n",
    "rope_embedding = rope_layer(input_embedding.unsqueeze(dim=2)).squeeze(dim=2)\n",
    "print(f\"Shape of ROPE embedding vector: {rope_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between vectors 1 and 2: 0.30431175231933594\n",
      "Dot product between vectors 8 and 9: 0.30431169271469116\n"
     ]
    }
   ],
   "source": [
    "dot_product_12_rope = torch.dot(rope_embedding[0][1], rope_embedding[0][2])\n",
    "dot_product_78_rope = torch.dot(rope_embedding[0][8], rope_embedding[0][9])\n",
    "print(f\"Dot product between vectors 1 and 2: {dot_product_12_rope}\")\n",
    "print(f\"Dot product between vectors 8 and 9: {dot_product_78_rope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROPE provides same dot product score to vectors 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
