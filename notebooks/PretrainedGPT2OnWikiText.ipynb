{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Understand how a pretrained GPT2 performs on wikitext-2-raw dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 23767\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "# filter out empty lines\n",
    "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "val_dataset = val_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "test_dataset = test_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt2 tokenizer with autotokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "wrapped_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding side to left and pad token to eos token\n",
    "wrapped_tokenizer.padding_side = \"left\"\n",
    "wrapped_tokenizer.pad_token = wrapped_tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n",
      "\n",
      "[1081, 262, 17871, 5321, 8720, 466, 407, 2152, 837, 262, 6727, 304, 29232, 684, 286, 262, 7096, 666, 5407, 14561, 262, 3721, 286, 19756, 2853, 12455, 287, 1502, 284, 3758, 606, 319, 10566, 326, 561, 4306, 787, 7096, 544, 4425, 1986, 287, 262, 1175, 764, 2893, 379, 1661, 428, 2499, 284, 511, 4621, 837, 884, 355, 257, 4388, 753, 24197, 656, 11773, 7674, 837, 584, 6266, 2728, 1728, 1866, 286, 262, 46588, 358, 1049, 17087, 764, 1881, 884, 2888, 837, 37307, 3686, 837, 4329, 523, 37530, 326, 339, 450, 392, 684, 465, 1281, 290, 22448, 656, 262, 9803, 286, 2199, 321, 414, 12552, 837, 7223, 284, 262, 7306, 286, 360, 5605, 6248, 10404, 5150, 416, 511, 3554, 837, 44864, 559, 764, 1629, 262, 976, 640, 837, 4847, 1626, 7096, 666, 5407, 9455, 1445, 284, 28602, 262, 17871, 5321, 287, 1502, 284, 1805, 511, 898, 5353, 764, 367, 6302, 416, 1111, 7681, 290, 5775, 837, 290, 5929, 351, 262, 4931, 286, 257, 35731, 1626, 511, 9803, 837, 262, 46588, 358, 16459, 1445, 284, 1394, 2405, 6776, 981, 379, 262, 976, 640, 1907, 284, 1037, 262, 7096, 666, 1175, 3626, 764, 770, 4477, 1566, 262, 17871, 5321, 705, 82, 25771, 3818, 837, 30552, 9325, 732, 837, 508, 550, 587, 4030, 739, 2156, 3251, 837, 318, 30037, 284, 262, 3139, 1748, 286, 8790, 70, 47847, 287, 1502, 284, 1944, 2370, 43507, 803, 262, 34730, 5795, 290, 15651, 262, 1103, 35731, 837, 262, 7096, 666, 3611, 326, 550, 5371, 20642, 286, 4700, 888, 764, 220, 198]\n",
      "['ĠAs', 'Ġthe', 'ĠNam', 'eless', 'Ġofficially', 'Ġdo', 'Ġnot', 'Ġexist', 'Ġ,', 'Ġthe', 'Ġupper', 'Ġe', 'chel', 'ons', 'Ġof', 'Ġthe', 'ĠGall', 'ian', 'ĠArmy', 'Ġexploit', 'Ġthe', 'Ġconcept', 'Ġof', 'Ġplausible', 'Ġden', 'iability', 'Ġin', 'Ġorder', 'Ġto', 'Ġsend', 'Ġthem', 'Ġon', 'Ġmissions', 'Ġthat', 'Ġwould', 'Ġotherwise', 'Ġmake', 'ĠGall', 'ia', 'Ġlose', 'Ġface', 'Ġin', 'Ġthe', 'Ġwar', 'Ġ.', 'ĠWhile', 'Ġat', 'Ġtimes', 'Ġthis', 'Ġworks', 'Ġto', 'Ġtheir', 'Ġadvantage', 'Ġ,', 'Ġsuch', 'Ġas', 'Ġa', 'Ġsuccessful', 'Ġinc', 'ursion', 'Ġinto', 'ĠImperial', 'Ġterritory', 'Ġ,', 'Ġother', 'Ġorders', 'Ġcause', 'Ġcertain', 'Ġmembers', 'Ġof', 'Ġthe', 'Ġ422', 'nd', 'Ġgreat', 'Ġdistress', 'Ġ.', 'ĠOne', 'Ġsuch', 'Ġmember', 'Ġ,', 'ĠGus', 'urg', 'Ġ,', 'Ġbecomes', 'Ġso', 'Ġenraged', 'Ġthat', 'Ġhe', 'Ġab', 'and', 'ons', 'Ġhis', 'Ġpost', 'Ġand', 'Ġdefects', 'Ġinto', 'Ġthe', 'Ġranks', 'Ġof', 'ĠCal', 'am', 'ity', 'ĠRaven', 'Ġ,', 'Ġattached', 'Ġto', 'Ġthe', 'Ġideal', 'Ġof', 'ĠD', 'arc', 'sen', 'Ġindependence', 'Ġproposed', 'Ġby', 'Ġtheir', 'Ġleader', 'Ġ,', 'ĠDah', 'au', 'Ġ.', 'ĠAt', 'Ġthe', 'Ġsame', 'Ġtime', 'Ġ,', 'Ġelements', 'Ġwithin', 'ĠGall', 'ian', 'ĠArmy', 'ĠCommand', 'Ġmove', 'Ġto', 'Ġerase', 'Ġthe', 'ĠNam', 'eless', 'Ġin', 'Ġorder', 'Ġto', 'Ġprotect', 'Ġtheir', 'Ġown', 'Ġinterests', 'Ġ.', 'ĠH', 'ounded', 'Ġby', 'Ġboth', 'Ġallies', 'Ġand', 'Ġenemies', 'Ġ,', 'Ġand', 'Ġcombined', 'Ġwith', 'Ġthe', 'Ġpresence', 'Ġof', 'Ġa', 'Ġtraitor', 'Ġwithin', 'Ġtheir', 'Ġranks', 'Ġ,', 'Ġthe', 'Ġ422', 'nd', 'Ġdesperately', 'Ġmove', 'Ġto', 'Ġkeep', 'Ġthemselves', 'Ġalive', 'Ġwhile', 'Ġat', 'Ġthe', 'Ġsame', 'Ġtime', 'Ġfight', 'Ġto', 'Ġhelp', 'Ġthe', 'ĠGall', 'ian', 'Ġwar', 'Ġeffort', 'Ġ.', 'ĠThis', 'Ġcontinues', 'Ġuntil', 'Ġthe', 'ĠNam', 'eless', \"Ġ'\", 's', 'Ġcommanding', 'Ġofficer', 'Ġ,', 'ĠRamsey', 'ĠCro', 'we', 'Ġ,', 'Ġwho', 'Ġhad', 'Ġbeen', 'Ġkept', 'Ġunder', 'Ġhouse', 'Ġarrest', 'Ġ,', 'Ġis', 'Ġescorted', 'Ġto', 'Ġthe', 'Ġcapital', 'Ġcity', 'Ġof', 'ĠRand', 'g', 'riz', 'Ġin', 'Ġorder', 'Ġto', 'Ġpresent', 'Ġevidence', 'Ġexoner', 'ating', 'Ġthe', 'Ġweary', 'Ġsoldiers', 'Ġand', 'Ġexpose', 'Ġthe', 'Ġreal', 'Ġtraitor', 'Ġ,', 'Ġthe', 'ĠGall', 'ian', 'ĠGeneral', 'Ġthat', 'Ġhad', 'Ġaccused', 'ĠKurt', 'Ġof', 'ĠTre', 'ason', 'Ġ.', 'Ġ', 'Ċ']\n",
      " As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n",
      "\n",
      " As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible deniability in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , Gusurg , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . Hounded by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of Randgriz in order to present evidence exonerating the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = train_dataset['text'][10]\n",
    "print(raw_text)\n",
    "\n",
    "tokens = wrapped_tokenizer(raw_text, max_length=MAX_SEQ_LEN)['input_ids']\n",
    "print(tokens)\n",
    "\n",
    "tokens_to_text = wrapped_tokenizer.convert_ids_to_tokens(tokens)\n",
    "print(tokens_to_text)\n",
    "\n",
    "decoded_tokens = wrapped_tokenizer.decode(tokens)\n",
    "print(decoded_tokens)\n",
    "print(wrapped_tokenizer.decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'output_ids'],\n",
       "    num_rows: 25377\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from utils import slide_window\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f5e16d61a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    output_ids = torch.stack([torch.tensor(item[\"output_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"output_ids\": output_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "batch_size = 30\n",
    "train_torch_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_torch_dataloader = DataLoader(tokenized_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_torch_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "train_torch_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "    \"vocab_size\": wrapped_tokenizer.vocab_size,\n",
    "    \"context_length\": MAX_SEQ_LEN,\n",
    "    \"device\": torch.device(\"cuda\"),\n",
    "    \"num_epochs\": 2,\n",
    "    \"model_path\": \"../model_files/gpt2_pretrained.pth\",\n",
    "    \"num_train_batches\" : num_train_batches,\n",
    "    \"learning_rate\" : 1e-4,\n",
    "    \"num_test_batches\" : num_val_batches,\n",
    "}\n",
    "\n",
    "# move gpt2 to GPU\n",
    "gpt2.to(config[\"device\"])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 86 Average test loss: 9.090373039245605\n",
      "Test loss without mask: at epoch 0 8.355369200651673 Test perplexity without mask: 4252.9560546875\n",
      "At epoch 2 batch 1 of num_batches 86 Average test loss: 9.090373039245605\n",
      "Test loss without mask: at epoch 1 8.355369200651673 Test perplexity without mask: 4252.9560546875\n"
     ]
    }
   ],
   "source": [
    "from utils import train \n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, train_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dual Destinies was given a digital @-@ only release ->  on June 1, 2016.\n",
      "\n",
      "The digital version of the game was released on June 1,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=20):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=MAX_SEQ_LEN, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        pred = model(input_ids).logits\n",
    "        next_token_logits = pred[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -MAX_SEQ_LEN:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\" Dual Destinies was given a digital @-@ only release\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_dataset[\"text\"][np.random.randint(0, len(train_dataset[\"text\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dual Destinies was given a digital @-@ only release just before it was announced on Sept. 2, 2015.[1] Since then, a number of players have criticized this game's content, including a comment for The Escapist that the digital version would lack \"originality.\" In response to some criticism, Steam has also removed a digital version of the game.\n",
      "\n",
      "In the original story, the Escapist talked with some of the main characters of \"The Escapists,\" all\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = gpt2.generate(\n",
    "    input_ids=wrapped_tokenizer(\" Dual Destinies was given a digital @-@ only release\", return_tensors=\"pt\").input_ids.to(config[\"device\"]),\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "generated_text = wrapped_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
