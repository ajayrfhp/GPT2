{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257,  3666,  3290,   318, 13779]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "tokenizer(\"My dog is cute\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'output_ids'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    inpt_text = examples['text']\n",
    "    inpt_text = [text[:-1] for text in inpt_text]\n",
    "    output_text = [text[1:] for text in inpt_text]\n",
    "    examples['input_ids'] = tokenizer(inpt_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")['input_ids']\n",
    "    examples['output_ids'] = tokenizer(output_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")['input_ids']\n",
    "    return examples\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# move gpt2 to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt2.to(device)\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As far as I am concerned, I will ->  spend the highly-paying part of my free\n"
     ]
    }
   ],
   "source": [
    "def generate_text(starting_text, model, tokenizer):\n",
    "    input_encoding = tokenizer(starting_text, return_tensors=\"pt\")\n",
    "    output_text = \"\"\n",
    "    output_tokens = min(100, tokenizer.model_max_length, len(input_encoding['input_ids'][0]))\n",
    "    for i in range(output_tokens):\n",
    "        text = starting_text + output_text\n",
    "        input_encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = input_encoding['input_ids'].to(device)\n",
    "        input_attention_mask = input_encoding['attention_mask'].to(device)\n",
    "        next_token_logits = model(input_ids, attention_mask=input_attention_mask).logits[:, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "        next_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        output_text += next_text\n",
    "    print(f\"{starting_text} -> {output_text}\")\n",
    "\n",
    "generate_text(\"As far as I am concerned, I will\", gpt2, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you, you are not a good ->  chef, any chef, for learning is\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"Thank you, you are not a good\", gpt2, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
