{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = [\"abcd\" * 10]*10\n",
    "train_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "test_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "val_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "\n",
    "# convert train_dataset, test_dataset, val_dataset to huggingface datasets\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdabcdabcdabcdabcdabcdabcdabcdabcdabcd'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "# get gpt2 tokenizer\n",
    "wrapped_tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "\n",
    "# set padding token\n",
    "wrapped_tokenizer.pad_token = wrapped_tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.tokenize(\"abcdabcd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8aab73f6984f6f91261fc345d6e096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514265c0d94e4b49b9f3a733e307c569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a787f81ca5d45318801d9da97f9e0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_words', 'output_words', 'input_ids_raw', 'output_ids_raw', 'input_ids', 'output_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def slide_window(text_batch):\n",
    "    text_batch['input_words'] = []\n",
    "    text_batch['output_words'] = []\n",
    "    text_batch['input_ids_raw'] = []\n",
    "    text_batch['output_ids_raw'] = []\n",
    "\n",
    "    text_batch['input_ids'] = []\n",
    "    text_batch['output_ids'] = []\n",
    "    text_batch['attention_mask'] = []\n",
    "\n",
    "    for text in text_batch['text']:\n",
    "        \n",
    "        tokens = wrapped_tokenizer.tokenize(text)\n",
    "        tokens.append(wrapped_tokenizer.eos_token)  # add eos token to the end of the tokens\n",
    "        input_tokens = tokens[:-1]\n",
    "        output_tokens = tokens[1:]\n",
    "        \n",
    "        text_batch['input_words'].append(input_tokens)\n",
    "        text_batch['output_words'].append(output_tokens)\n",
    "\n",
    "        input_ids_raw = wrapped_tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        output_ids_raw = wrapped_tokenizer.convert_tokens_to_ids(output_tokens)\n",
    "\n",
    "        text_batch['input_ids_raw'].append(input_ids_raw)\n",
    "        text_batch['output_ids_raw'].append(output_ids_raw)\n",
    "\n",
    "        # pad, truncate, and convert to tensor\n",
    "        input_ids = wrapped_tokenizer.pad({\"input_ids\": input_ids_raw}, padding=\"max_length\", max_length=100, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        output_ids = wrapped_tokenizer.pad({\"input_ids\": output_ids_raw}, padding=\"max_length\", max_length=100, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        attention_mask = [0] * (100 - len(input_ids_raw)) + [1] * len(input_ids_raw)\n",
    "\n",
    "        assert len(attention_mask) == 100\n",
    "        text_batch['input_ids'].append(input_ids)\n",
    "        text_batch['output_ids'].append(output_ids)\n",
    "        text_batch['attention_mask'].append(attention_mask)\n",
    "\n",
    "    return text_batch \n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True)\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd'],\n",
       " ['cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  '<|endoftext|>'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset['input_words'][0], tokenized_train_dataset['output_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f4b7da7a150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids'], item['attention_mask']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    attention_mask = [item[2] for item in batch]\n",
    "    input_ids_list = torch.tensor(input_ids)\n",
    "    output_ids_list = torch.tensor(output_ids)\n",
    "    attention_mask_list = torch.tensor(attention_mask)\n",
    "    return input_ids_list, output_ids_list, attention_mask_list\n",
    "\n",
    "batch_size = 20\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 100]), torch.Size([10, 100]), torch.Size([10, 100]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids, attention_masks = batch\n",
    "input_ids.shape, output_ids.shape, attention_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(50257, 128)\n",
       "  (position_embedding): Embedding(128, 128)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (out_project): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (out_project): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=128, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 128,\n",
    "        \"heads\": 2,\n",
    "        \"layers\": 2,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 128,\n",
    "        \"device\": torch.device(\"cuda:0\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 50,\n",
    "        \"model_path\": \"../model_files/gpt2_emotion.pth\",\n",
    "        \"num_train_batches\" : num_train_batches\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "At epoch 1 batch 1 of num_batches 0Average batch loss: 10.932890892028809\n",
      "Test loss without mask: at epoch 0 10.892751693725586 Test perplexity without mask: 53785.09765625\n",
      "torch.Size([])\n",
      "At epoch 2 batch 1 of num_batches 0Average batch loss: 10.892603874206543\n",
      "Test loss without mask: at epoch 1 10.853691101074219 Test perplexity without mask: 51724.71875\n",
      "torch.Size([])\n",
      "At epoch 3 batch 1 of num_batches 0Average batch loss: 10.853998184204102\n",
      "Test loss without mask: at epoch 2 10.812539100646973 Test perplexity without mask: 49639.34765625\n",
      "torch.Size([])\n",
      "At epoch 4 batch 1 of num_batches 0Average batch loss: 10.813957214355469\n",
      "Test loss without mask: at epoch 3 10.77685260772705 Test perplexity without mask: 47899.12890625\n",
      "torch.Size([])\n",
      "At epoch 5 batch 1 of num_batches 0Average batch loss: 10.775415420532227\n",
      "Test loss without mask: at epoch 4 10.737105369567871 Test perplexity without mask: 46032.61328125\n",
      "torch.Size([])\n",
      "At epoch 6 batch 1 of num_batches 0Average batch loss: 10.739124298095703\n",
      "Test loss without mask: at epoch 5 10.69753360748291 Test perplexity without mask: 44246.58984375\n",
      "torch.Size([])\n",
      "At epoch 7 batch 1 of num_batches 0Average batch loss: 10.697148323059082\n",
      "Test loss without mask: at epoch 6 10.65820026397705 Test perplexity without mask: 42540.0078125\n",
      "torch.Size([])\n",
      "At epoch 8 batch 1 of num_batches 0Average batch loss: 10.65791130065918\n",
      "Test loss without mask: at epoch 7 10.621384620666504 Test perplexity without mask: 41002.34765625\n",
      "torch.Size([])\n",
      "At epoch 9 batch 1 of num_batches 0Average batch loss: 10.621249198913574\n",
      "Test loss without mask: at epoch 8 10.580595016479492 Test perplexity without mask: 39363.53125\n",
      "torch.Size([])\n",
      "At epoch 10 batch 1 of num_batches 0Average batch loss: 10.581965446472168\n",
      "Test loss without mask: at epoch 9 10.542728424072266 Test perplexity without mask: 37900.8359375\n",
      "torch.Size([])\n",
      "At epoch 11 batch 1 of num_batches 0Average batch loss: 10.540027618408203\n",
      "Test loss without mask: at epoch 10 10.500842094421387 Test perplexity without mask: 36346.09765625\n",
      "torch.Size([])\n",
      "At epoch 12 batch 1 of num_batches 0Average batch loss: 10.500018119812012\n",
      "Test loss without mask: at epoch 11 10.459808349609375 Test perplexity without mask: 34884.8671875\n",
      "torch.Size([])\n",
      "At epoch 13 batch 1 of num_batches 0Average batch loss: 10.463104248046875\n",
      "Test loss without mask: at epoch 12 10.423168182373047 Test perplexity without mask: 33629.8125\n",
      "torch.Size([])\n",
      "At epoch 14 batch 1 of num_batches 0Average batch loss: 10.422032356262207\n",
      "Test loss without mask: at epoch 13 10.380780220031738 Test perplexity without mask: 32234.1015625\n",
      "torch.Size([])\n",
      "At epoch 15 batch 1 of num_batches 0Average batch loss: 10.378972053527832\n",
      "Test loss without mask: at epoch 14 10.339703559875488 Test perplexity without mask: 30936.857421875\n",
      "torch.Size([])\n",
      "At epoch 16 batch 1 of num_batches 0Average batch loss: 10.339010238647461\n",
      "Test loss without mask: at epoch 15 10.297083854675293 Test perplexity without mask: 29646.041015625\n",
      "torch.Size([])\n",
      "At epoch 17 batch 1 of num_batches 0Average batch loss: 10.295636177062988\n",
      "Test loss without mask: at epoch 16 10.25534725189209 Test perplexity without mask: 28434.181640625\n",
      "torch.Size([])\n",
      "At epoch 18 batch 1 of num_batches 0Average batch loss: 10.255773544311523\n",
      "Test loss without mask: at epoch 17 10.212015151977539 Test perplexity without mask: 27228.380859375\n",
      "torch.Size([])\n",
      "At epoch 19 batch 1 of num_batches 0Average batch loss: 10.210457801818848\n",
      "Test loss without mask: at epoch 18 10.166934967041016 Test perplexity without mask: 26028.177734375\n",
      "torch.Size([])\n",
      "At epoch 20 batch 1 of num_batches 0Average batch loss: 10.166121482849121\n",
      "Test loss without mask: at epoch 19 10.123847961425781 Test perplexity without mask: 24930.517578125\n",
      "torch.Size([])\n",
      "At epoch 21 batch 1 of num_batches 0Average batch loss: 10.126107215881348\n",
      "Test loss without mask: at epoch 20 10.076001167297363 Test perplexity without mask: 23765.759765625\n",
      "torch.Size([])\n",
      "At epoch 22 batch 1 of num_batches 0Average batch loss: 10.077710151672363\n",
      "Test loss without mask: at epoch 21 10.027410507202148 Test perplexity without mask: 22638.572265625\n",
      "torch.Size([])\n",
      "At epoch 23 batch 1 of num_batches 0Average batch loss: 10.027986526489258\n",
      "Test loss without mask: at epoch 22 9.985625267028809 Test perplexity without mask: 21712.10546875\n",
      "torch.Size([])\n",
      "At epoch 24 batch 1 of num_batches 0Average batch loss: 9.981886863708496\n",
      "Test loss without mask: at epoch 23 9.931107521057129 Test perplexity without mask: 20560.099609375\n",
      "torch.Size([])\n",
      "At epoch 25 batch 1 of num_batches 0Average batch loss: 9.936200141906738\n",
      "Test loss without mask: at epoch 24 9.886435508728027 Test perplexity without mask: 19661.849609375\n",
      "torch.Size([])\n",
      "At epoch 26 batch 1 of num_batches 0Average batch loss: 9.883573532104492\n",
      "Test loss without mask: at epoch 25 9.838024139404297 Test perplexity without mask: 18732.666015625\n",
      "torch.Size([])\n",
      "At epoch 27 batch 1 of num_batches 0Average batch loss: 9.832857131958008\n",
      "Test loss without mask: at epoch 26 9.784996032714844 Test perplexity without mask: 17765.1875\n",
      "torch.Size([])\n",
      "At epoch 28 batch 1 of num_batches 0Average batch loss: 9.78034496307373\n",
      "Test loss without mask: at epoch 27 9.73342514038086 Test perplexity without mask: 16872.244140625\n",
      "torch.Size([])\n",
      "At epoch 29 batch 1 of num_batches 0Average batch loss: 9.7314453125\n",
      "Test loss without mask: at epoch 28 9.679769515991211 Test perplexity without mask: 15990.810546875\n",
      "torch.Size([])\n",
      "At epoch 30 batch 1 of num_batches 0Average batch loss: 9.675727844238281\n",
      "Test loss without mask: at epoch 29 9.617066383361816 Test perplexity without mask: 15018.92578125\n",
      "torch.Size([])\n",
      "At epoch 31 batch 1 of num_batches 0Average batch loss: 9.621101379394531\n",
      "Test loss without mask: at epoch 30 9.563654899597168 Test perplexity without mask: 14237.7890625\n",
      "torch.Size([])\n",
      "At epoch 32 batch 1 of num_batches 0Average batch loss: 9.565290451049805\n",
      "Test loss without mask: at epoch 31 9.508736610412598 Test perplexity without mask: 13476.95703125\n",
      "torch.Size([])\n",
      "At epoch 33 batch 1 of num_batches 0Average batch loss: 9.510476112365723\n",
      "Test loss without mask: at epoch 32 9.450653076171875 Test perplexity without mask: 12716.4677734375\n",
      "torch.Size([])\n",
      "At epoch 34 batch 1 of num_batches 0Average batch loss: 9.449453353881836\n",
      "Test loss without mask: at epoch 33 9.393272399902344 Test perplexity without mask: 12007.328125\n",
      "torch.Size([])\n",
      "At epoch 35 batch 1 of num_batches 0Average batch loss: 9.389983177185059\n",
      "Test loss without mask: at epoch 34 9.33215618133545 Test perplexity without mask: 11295.4599609375\n",
      "torch.Size([])\n",
      "At epoch 36 batch 1 of num_batches 0Average batch loss: 9.331771850585938\n",
      "Test loss without mask: at epoch 35 9.276029586791992 Test perplexity without mask: 10678.947265625\n",
      "torch.Size([])\n",
      "At epoch 37 batch 1 of num_batches 0Average batch loss: 9.275644302368164\n",
      "Test loss without mask: at epoch 36 9.221251487731934 Test perplexity without mask: 10109.708984375\n",
      "torch.Size([])\n",
      "At epoch 38 batch 1 of num_batches 0Average batch loss: 9.21850299835205\n",
      "Test loss without mask: at epoch 37 9.165885925292969 Test perplexity without mask: 9565.19140625\n",
      "torch.Size([])\n",
      "At epoch 39 batch 1 of num_batches 0Average batch loss: 9.162734985351562\n",
      "Test loss without mask: at epoch 38 9.107909202575684 Test perplexity without mask: 9026.40234375\n",
      "torch.Size([])\n",
      "At epoch 40 batch 1 of num_batches 0Average batch loss: 9.1056547164917\n",
      "Test loss without mask: at epoch 39 9.054926872253418 Test perplexity without mask: 8560.611328125\n",
      "torch.Size([])\n",
      "At epoch 41 batch 1 of num_batches 0Average batch loss: 9.053498268127441\n",
      "Test loss without mask: at epoch 40 9.000480651855469 Test perplexity without mask: 8106.9794921875\n",
      "torch.Size([])\n",
      "At epoch 42 batch 1 of num_batches 0Average batch loss: 9.006087303161621\n",
      "Test loss without mask: at epoch 41 8.957058906555176 Test perplexity without mask: 7762.49365234375\n",
      "torch.Size([])\n",
      "At epoch 43 batch 1 of num_batches 0Average batch loss: 8.96199893951416\n",
      "Test loss without mask: at epoch 42 8.913884162902832 Test perplexity without mask: 7434.48193359375\n",
      "torch.Size([])\n",
      "At epoch 44 batch 1 of num_batches 0Average batch loss: 8.914833068847656\n",
      "Test loss without mask: at epoch 43 8.881707191467285 Test perplexity without mask: 7199.0703125\n",
      "torch.Size([])\n",
      "At epoch 45 batch 1 of num_batches 0Average batch loss: 8.877091407775879\n",
      "Test loss without mask: at epoch 44 8.845535278320312 Test perplexity without mask: 6943.31982421875\n",
      "torch.Size([])\n",
      "At epoch 46 batch 1 of num_batches 0Average batch loss: 8.851266860961914\n",
      "Test loss without mask: at epoch 45 8.822120666503906 Test perplexity without mask: 6782.63330078125\n",
      "torch.Size([])\n",
      "At epoch 47 batch 1 of num_batches 0Average batch loss: 8.821004867553711\n",
      "Test loss without mask: at epoch 46 8.800256729125977 Test perplexity without mask: 6635.947265625\n",
      "torch.Size([])\n",
      "At epoch 48 batch 1 of num_batches 0Average batch loss: 8.80074405670166\n",
      "Test loss without mask: at epoch 47 8.781886100769043 Test perplexity without mask: 6515.15380859375\n",
      "torch.Size([])\n",
      "At epoch 49 batch 1 of num_batches 0Average batch loss: 8.78111457824707\n",
      "Test loss without mask: at epoch 48 8.769881248474121 Test perplexity without mask: 6437.40771484375\n",
      "torch.Size([])\n",
      "At epoch 50 batch 1 of num_batches 0Average batch loss: 8.770676612854004\n",
      "Test loss without mask: at epoch 49 8.761636734008789 Test perplexity without mask: 6384.552734375\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config)\n",
    "torch.save(gpt2.state_dict(), config[\"model_path\"]) # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 5)\n",
    "mask = torch.tensor([0, 1])\n",
    "x[mask == 0] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GPT2 from config.model_path\n",
    "import os \n",
    "\n",
    "if os.path.exists(config['model_path']):\n",
    "    gpt2.load_state_dict(torch.load(config['model_path']))\n",
    "    print(\"model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'abcdabcdabcdabcdabcdabcdabcdabcdabcdabcd',\n",
       " 'input_words': ['ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd'],\n",
       " 'output_words': ['cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  'ab',\n",
       "  'cd',\n",
       "  '<|endoftext|>'],\n",
       " 'input_ids_raw': [397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210],\n",
       " 'output_ids_raw': [10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  50256],\n",
       " 'input_ids': [50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210],\n",
       " 'output_ids': [50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  397,\n",
       "  10210,\n",
       "  50256],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = tokenized_train_dataset[1]['input_ids']\n",
    "y = tokenized_train_dataset[1]['output_ids']\n",
    "mask = tokenized_train_dataset[1]['attention_mask']\n",
    "\n",
    "x, y, mask = torch.tensor(x), torch.tensor(y), torch.tensor(mask)\n",
    "\n",
    "x[mask==1], y[mask==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 10210]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"cd\", truncation=True, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "#next_token_decoded = wrapped_tokenizer.decode(next_token)\n",
    "#next_token_decoded\n",
    "next_token_decoded = wrapped_tokenizer.decode(prediction[0, -1].argmax().item())\n",
    "next_token_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab -> cdabcdabcdabcdabcdab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=10):\n",
    "    input_encoding = tokenizer(starting_text, return_tensors=\"pt\")\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "\n",
    "    for i in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -128:]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    output_text = tokenizer.decode(output_tokens)\n",
    "        #output_text += next_text\n",
    "    print(f\"{starting_text} -> {output_text}\")\n",
    "\n",
    "generate_text(\"ab\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
