{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = [ \".\".join([\"abcdefghijklm\" for _ in range(5)])]*10000\n",
    "train_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "test_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "val_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "\n",
    "# convert train_dataset, test_dataset, val_dataset to huggingface datasets\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm',\n",
       " 'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained on custom dataset with vocabulary size: 15\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import tiktoken\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import tempfile\n",
    "from tokenizers import AddedToken\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "\n",
    "# re-train tokenizer on train_dataset\n",
    "\n",
    "# Initialize a new tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<|endoftext|>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "trainer = BpeTrainer(special_tokens=special_tokens, vocab_size=10)\n",
    "\n",
    "# Create a temporary file with raw text for training\n",
    "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as f:\n",
    "    for text in train_dataset[\"text\"]:\n",
    "        f.write(text + \"\\n\")\n",
    "    temp_file_path = f.name\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(files=[temp_file_path], trainer=trainer)\n",
    "\n",
    "# Convert to Hugging Face tokenizer\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    unk_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|endoftext|>\",\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Clean up the temporary file\n",
    "os.unlink(temp_file_path)\n",
    "\n",
    "print(f\"Tokenizer trained on custom dataset with vocabulary size: {wrapped_tokenizer.vocab_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.tokenize(\"abcdefghijklm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db48954f48946df971521372c4ad335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead94ab448844941a5d8927c4ce09bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664d0f8f82c94b6194969da1a60a046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'output_ids', 'input_words', 'output_words', 'input_ids_raw', 'output_ids_raw'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def slide_window(text_batch, max_length=1024):\n",
    "    \"\"\"\n",
    "    More efficient version of slide_window that leverages batch processing.\n",
    "\n",
    "    Args:\n",
    "        text_batch (dict): A dictionary likely containing a 'text' key with a list of strings.\n",
    "        max_length (int): The maximum sequence length for padding and truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids', 'attention_mask', and 'output_ids' tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1 & 2: Tokenize, Add EOS, Create Shifted Inputs/Outputs (Raw) ---\n",
    "    \n",
    "    all_tokens = []\n",
    "    input_ids_raw = []\n",
    "    output_ids_raw = []\n",
    "    \n",
    "    eos_token_id = wrapped_tokenizer.convert_tokens_to_ids(wrapped_tokenizer.eos_token)\n",
    "\n",
    "    # Tokenize texts and prepare raw ID lists\n",
    "    # Using a loop here is often necessary for the custom EOS + shift logic, \n",
    "    # but the expensive padding step will be batched later.\n",
    "    for text in text_batch['text']:\n",
    "        tokens = wrapped_tokenizer.tokenize(text)\n",
    "        token_ids = wrapped_tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids.append(eos_token_id) # Add EOS ID\n",
    "\n",
    "        # Create input/output pairs (before padding/truncation)\n",
    "        current_input_ids = token_ids[:-1]\n",
    "        current_output_ids = token_ids[1:]\n",
    "        \n",
    "        input_ids_raw.append(current_input_ids)\n",
    "        output_ids_raw.append(current_output_ids)\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    # --- Step 3: Batch Pad Inputs and Generate Attention Mask ---\n",
    "    \n",
    "    # Let the tokenizer handle padding, truncation (via max_length), \n",
    "    # attention mask creation, and tensor conversion for the whole batch.\n",
    "    padded_inputs = wrapped_tokenizer.pad(\n",
    "        {\"input_ids\": input_ids_raw},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True, # Ask the tokenizer to create the mask\n",
    "    )\n",
    "\n",
    "    # --- Step 4: Batch Pad Outputs ---\n",
    "    \n",
    "    # Pad the output sequences. Usually, no attention mask is needed for labels.\n",
    "    padded_outputs = wrapped_tokenizer.pad(\n",
    "        {\"input_ids\": output_ids_raw},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Return the final batch ---\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": padded_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": padded_inputs[\"attention_mask\"],\n",
    "        \"output_ids\": padded_outputs[\"input_ids\"],\n",
    "        # Optional: Keep raw words/ids if needed for debugging, but remove for efficiency\n",
    "        'input_words': [tokens[:-1] for tokens in all_tokens], # Reconstruct if needed\n",
    "        'output_words': [tokens[1:] for tokens in all_tokens], # Reconstruct if needed\n",
    "        'input_ids_raw': input_ids_raw, # Keep if needed\n",
    "        'output_ids_raw': output_ids_raw, # Keep if needed\n",
    "    }\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True)\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fbde2d1a390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids'], item['attention_mask']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    attention_mask = [item[2] for item in batch]\n",
    "    input_ids_list = torch.tensor(input_ids)\n",
    "    output_ids_list = torch.tensor(output_ids)\n",
    "    attention_mask_list = torch.tensor(attention_mask)\n",
    "    return input_ids_list, output_ids_list, attention_mask_list\n",
    "\n",
    "batch_size = 20\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1024]), torch.Size([20, 1024]), torch.Size([20, 1024]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids, attention_masks = batch\n",
    "input_ids.shape, output_ids.shape, attention_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(15, 100)\n",
       "  (position_embedding): Embedding(100, 100)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_K): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_V): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (out_project): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=400, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=400, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_K): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_V): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (out_project): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=400, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=400, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=100, out_features=15, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_test_batches = tokenized_test_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 100,\n",
    "        \"heads\": 2,\n",
    "        \"layers\": 2,\n",
    "        \"vocab_size\": wrapped_tokenizer.vocab_size,\n",
    "        \"context_length\": 100,\n",
    "        \"device\": torch.device(\"cpu\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 2,\n",
    "        \"model_path\": \"../model_files/gpt2_abcd.pth\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-2,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.500992476940155\n",
      "Masked loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Test nn.CrossEntropyLoss for batch size of 20 and sequence length of 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create random logits (model output) with shape [batch_size, sequence_length, vocab_size]\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "vocab_size = 3\n",
    "\n",
    "logits = [\n",
    "    [\n",
    "        [0.1, 0.2, 0.3],  # First sequence 2\n",
    "        [0.4, 0.5, 0.6],  # Second sequence 2\n",
    "        [0.7, 0.8, 0.9],  # Third sequence 2\n",
    "        [1.0, 1.1, 1.2],  # Fourth sequence 2\n",
    "        [1.3, 1.4, 1.5]   # Fifth sequence 2\n",
    "    ],\n",
    "    [\n",
    "        [11.6, 1.7, 1.8],  # First sequence 0\n",
    "        [11.9, 2.0, 2.1],  # Second sequence 0\n",
    "        [22.2, 2.3, 2.4],  # Third sequence 0\n",
    "        [22.5, 2.6, 2.7],  # Fourth sequence 0\n",
    "        [22.8, 2.9, 3.0]   # Fifth sequence 0\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create random targets with shape [batch_size, sequence_length]\n",
    "targets = [\n",
    "    [2, 2, 2, 2, 2],  # First sequence\n",
    "    [0, 0, 0, 0, 0]   # Second sequence\n",
    "]\n",
    "\n",
    "# Convert to tensors\n",
    "logits = torch.tensor(logits, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Reshape logits for CrossEntropyLoss: [batch_size * sequence_length, vocab_size]\n",
    "logits_view = logits.reshape(-1, vocab_size)\n",
    "\n",
    "# Reshape targets for CrossEntropyLoss: [batch_size * sequence_length]\n",
    "targets_view = targets.reshape(-1)\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_view, targets_view)\n",
    "print(f\"Total loss: {loss}\")\n",
    "\n",
    "# With attention mask (ignoring padding)\n",
    "attention_mask = torch.ones_like(targets)\n",
    "attention_mask[:, :10] = 0  # Set first 20 positions as padding\n",
    "\n",
    "# Create mask to use for loss calculation\n",
    "mask = attention_mask.reshape(-1).bool()\n",
    "\n",
    "# Calculate masked loss (only on non-padded positions)\n",
    "masked_logits = logits_view[mask]\n",
    "masked_targets = targets_view[mask]\n",
    "masked_loss = criterion(masked_logits, masked_targets)\n",
    "print(f\"Masked loss: {masked_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_torch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [55,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m----> 3\u001b[0m train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/GPT2/notebooks/utils.py:22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, test_data, config, use_fp_16)\u001b[0m\n\u001b[1;32m     20\u001b[0m device_type \u001b[38;5;241m=\u001b[39m inpt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39muse_fp_16, device_type\u001b[38;5;241m=\u001b[39mdevice_type):\n\u001b[0;32m---> 22\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(inpt)\n\u001b[1;32m     23\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/GPT2/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/GPT2/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/GPT2/notebooks/models.py:53\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m position_vector \u001b[38;5;241m=\u001b[39m position_vector\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     51\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(position_vector)\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m token_embeddings \u001b[38;5;241m+\u001b[39m position_embeddings\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks(x)\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projection(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 5)\n",
    "mask = torch.tensor([0, 1])\n",
    "x[mask == 0] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row = tokenized_train_dataset[1]\n",
    "print(row['input_words'])\n",
    "print(row['output_words'])\n",
    "print(row['input_ids_raw'])\n",
    "print(row['output_ids_raw'])\n",
    "print(row['input_ids'])\n",
    "print(row['output_ids'])\n",
    "print(row['attention_mask'])\n",
    "\n",
    "mask_indices = row['attention_mask']\n",
    "input_unmasked = row['input_ids'][mask_indices == 1]\n",
    "output_unmasked = row['output_ids'][mask_indices == 1]\n",
    "print(input_unmasked)\n",
    "print(output_unmasked)\n",
    "\n",
    "type(row['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wrapped_tokenizer(\"abc\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "#next_token_decoded = wrapped_tokenizer.decode(next_token)\n",
    "#next_token_decoded\n",
    "next_token_decoded = wrapped_tokenizer.decode(prediction[0, -1].argmax().item())\n",
    "next_token_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=20):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -100:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\"ab\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
