{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on synthetic text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = [ \".\".join([\"abcdefghijklm\" for _ in range(5)])]*1000\n",
    "train_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "test_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "val_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "\n",
    "# convert train_dataset, test_dataset, val_dataset to huggingface datasets\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm',\n",
       " 'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained on custom dataset with vocabulary size: 15\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import tiktoken\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import tempfile\n",
    "from tokenizers import AddedToken\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "\n",
    "# re-train tokenizer on train_dataset\n",
    "\n",
    "# Initialize a new tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<|endoftext|>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "trainer = BpeTrainer(special_tokens=special_tokens, vocab_size=10)\n",
    "\n",
    "# Create a temporary file with raw text for training\n",
    "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as f:\n",
    "    for text in train_dataset[\"text\"]:\n",
    "        f.write(text + \"\\n\")\n",
    "    temp_file_path = f.name\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(files=[temp_file_path], trainer=trainer)\n",
    "\n",
    "# Convert to Hugging Face tokenizer\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    unk_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|endoftext|>\",\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# Clean up the temporary file\n",
    "os.unlink(temp_file_path)\n",
    "\n",
    "print(f\"Tokenizer trained on custom dataset with vocabulary size: {wrapped_tokenizer.vocab_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.tokenize(\"abcdefghijklm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d563b511ca14437f9944f093c5877070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56e6470947246a699c6ab85bc0f38ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4466bf59ea004b1387ab4daa699f9c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'output_ids', 'input_words', 'output_words', 'input_ids_raw', 'output_ids_raw'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def slide_window(text_batch, max_length=128):\n",
    "    \"\"\"\n",
    "    More efficient version of slide_window that leverages batch processing.\n",
    "\n",
    "    Args:\n",
    "        text_batch (dict): A dictionary likely containing a 'text' key with a list of strings.\n",
    "        max_length (int): The maximum sequence length for padding and truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids', 'attention_mask', and 'output_ids' tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1 & 2: Tokenize, Add EOS, Create Shifted Inputs/Outputs (Raw) ---\n",
    "    \n",
    "    all_tokens = []\n",
    "    input_ids_raw = []\n",
    "    output_ids_raw = []\n",
    "    \n",
    "    eos_token_id = wrapped_tokenizer.convert_tokens_to_ids(wrapped_tokenizer.eos_token)\n",
    "\n",
    "    # Tokenize texts and prepare raw ID lists\n",
    "    # Using a loop here is often necessary for the custom EOS + shift logic, \n",
    "    # but the expensive padding step will be batched later.\n",
    "    for text in text_batch['text']:\n",
    "        tokens = wrapped_tokenizer.tokenize(text)\n",
    "        token_ids = wrapped_tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids.append(eos_token_id) # Add EOS ID\n",
    "\n",
    "        # Create input/output pairs (before padding/truncation)\n",
    "        current_input_ids = token_ids[:-1]\n",
    "        current_output_ids = token_ids[1:]\n",
    "        \n",
    "        input_ids_raw.append(current_input_ids)\n",
    "        output_ids_raw.append(current_output_ids)\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    # --- Step 3: Batch Pad Inputs and Generate Attention Mask ---\n",
    "    \n",
    "    # Let the tokenizer handle padding, truncation (via max_length), \n",
    "    # attention mask creation, and tensor conversion for the whole batch.\n",
    "    padded_inputs = wrapped_tokenizer.pad(\n",
    "        {\"input_ids\": input_ids_raw},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True, # Ask the tokenizer to create the mask\n",
    "    )\n",
    "\n",
    "    # --- Step 4: Batch Pad Outputs ---\n",
    "    \n",
    "    # Pad the output sequences. Usually, no attention mask is needed for labels.\n",
    "    padded_outputs = wrapped_tokenizer.pad(\n",
    "        {\"input_ids\": output_ids_raw},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Return the final batch ---\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": padded_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": padded_inputs[\"attention_mask\"],\n",
    "        \"output_ids\": padded_outputs[\"input_ids\"],\n",
    "        # Optional: Keep raw words/ids if needed for debugging, but remove for efficiency\n",
    "        'input_words': [tokens[:-1] for tokens in all_tokens], # Reconstruct if needed\n",
    "        'output_words': [tokens[1:] for tokens in all_tokens], # Reconstruct if needed\n",
    "        'input_ids_raw': input_ids_raw, # Keep if needed\n",
    "        'output_ids_raw': output_ids_raw, # Keep if needed\n",
    "    }\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True)\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f6337f63210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids'], item['attention_mask']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    attention_mask = [item[2] for item in batch]\n",
    "    input_ids_list = torch.tensor(input_ids)\n",
    "    output_ids_list = torch.tensor(output_ids)\n",
    "    attention_mask_list = torch.tensor(attention_mask)\n",
    "    return input_ids_list, output_ids_list, attention_mask_list\n",
    "\n",
    "batch_size = 100\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 128]), torch.Size([100, 128]), torch.Size([100, 128]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids, attention_masks = batch\n",
    "input_ids.shape, output_ids.shape, attention_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(20, 100)\n",
       "  (position_embedding): Embedding(128, 100)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_K): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_V): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (out_project): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=400, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=400, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_K): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (W_V): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (out_project): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=400, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=400, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=100, out_features=20, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_test_batches = tokenized_test_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 100,\n",
    "        \"heads\": 2,\n",
    "        \"layers\": 2,\n",
    "        \"vocab_size\": wrapped_tokenizer.vocab_size + 5,\n",
    "        \"context_length\": 128,\n",
    "        \"device\": torch.device(\"cpu\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 2,\n",
    "        \"model_path\": \"../model_files/gpt2_abcd.pth\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-2,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.500992476940155\n",
      "Masked loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Test nn.CrossEntropyLoss for batch size of 20 and sequence length of 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create random logits (model output) with shape [batch_size, sequence_length, vocab_size]\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "vocab_size = 3\n",
    "\n",
    "logits = [\n",
    "    [\n",
    "        [0.1, 0.2, 0.3],  # First sequence 2\n",
    "        [0.4, 0.5, 0.6],  # Second sequence 2\n",
    "        [0.7, 0.8, 0.9],  # Third sequence 2\n",
    "        [1.0, 1.1, 1.2],  # Fourth sequence 2\n",
    "        [1.3, 1.4, 1.5]   # Fifth sequence 2\n",
    "    ],\n",
    "    [\n",
    "        [11.6, 1.7, 1.8],  # First sequence 0\n",
    "        [11.9, 2.0, 2.1],  # Second sequence 0\n",
    "        [22.2, 2.3, 2.4],  # Third sequence 0\n",
    "        [22.5, 2.6, 2.7],  # Fourth sequence 0\n",
    "        [22.8, 2.9, 3.0]   # Fifth sequence 0\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create random targets with shape [batch_size, sequence_length]\n",
    "targets = [\n",
    "    [2, 2, 2, 2, 2],  # First sequence\n",
    "    [0, 0, 0, 0, 0]   # Second sequence\n",
    "]\n",
    "\n",
    "# Convert to tensors\n",
    "logits = torch.tensor(logits, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Reshape logits for CrossEntropyLoss: [batch_size * sequence_length, vocab_size]\n",
    "logits_view = logits.reshape(-1, vocab_size)\n",
    "\n",
    "# Reshape targets for CrossEntropyLoss: [batch_size * sequence_length]\n",
    "targets_view = targets.reshape(-1)\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_view, targets_view)\n",
    "print(f\"Total loss: {loss}\")\n",
    "\n",
    "# With attention mask (ignoring padding)\n",
    "attention_mask = torch.ones_like(targets)\n",
    "attention_mask[:, :10] = 0  # Set first 20 positions as padding\n",
    "\n",
    "# Create mask to use for loss calculation\n",
    "mask = attention_mask.reshape(-1).bool()\n",
    "\n",
    "# Calculate masked loss (only on non-padded positions)\n",
    "masked_logits = logits_view[mask]\n",
    "masked_targets = targets_view[mask]\n",
    "masked_loss = criterion(masked_logits, masked_targets)\n",
    "print(f\"Masked loss: {masked_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_torch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 10 Average batch loss: 3.1923117637634277 Perplexity: 24.344640731811523\n",
      "At epoch 1 batch 1 of num_batches 10 Average test loss: 1.3809055089950562\n",
      "Test loss without mask: at epoch 0 1.3809276461601256 Test perplexity without mask: 3.978590726852417\n",
      "At epoch 2 batch 1 of num_batches 10 Average batch loss: 1.3809154033660889 Perplexity: 3.978541851043701\n",
      "At epoch 2 batch 1 of num_batches 10 Average test loss: 1.3813519477844238\n",
      "Test loss without mask: at epoch 1 1.3811291575431823 Test perplexity without mask: 3.9793922901153564\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.9459,  0.8687,  0.0835, -0.3238, -1.4536]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 5)\n",
    "mask = torch.tensor([0, 1])\n",
    "x[mask == 0] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 3, 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"abc\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "#next_token_decoded = wrapped_tokenizer.decode(next_token)\n",
    "#next_token_decoded\n",
    "next_token_decoded = wrapped_tokenizer.decode(prediction[0, -1].argmax().item())\n",
    "next_token_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab -> cdefghijklm.abcdefgh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=20):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -100:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\"ab\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
