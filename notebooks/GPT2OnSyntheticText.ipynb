{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on synthetic text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_text = [\".\".join([\"abcdefghijklm\" for _ in range(np.random.randint(3, 7))]) for _ in range(1000)]\n",
    "\n",
    "train_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "test_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "val_dataset = {\n",
    "    \"text\": raw_text\n",
    "}\n",
    "\n",
    "# convert train_dataset, test_dataset, val_dataset to huggingface datasets\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm',\n",
       " 'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm',\n",
       " 'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm',\n",
       " 'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm',\n",
       " 'abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm.abcdefghijklm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklm.abcdefghijklm.abcdefghijklm'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer trained on custom dataset with vocabulary size: 15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import get_train_tokenizer\n",
    "wrapped_tokenizer = get_train_tokenizer(train_dataset, vocab_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.tokenize(\"abcdefghijklm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29726096ef9a44c1bfdf840f34e52d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b724dfad514de2a80b1d579940d20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e705b0b4d8ce475c976218a56ce7f9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'output_ids'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from utils import slide_window\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f416d19d950>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    output_ids = torch.stack([torch.tensor(item[\"output_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"output_ids\": output_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "batch_size = 200\n",
    "train_torch_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_torch_dataloader = DataLoader(tokenized_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_torch_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "train_torch_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 256]), torch.Size([200, 256]), torch.Size([200, 256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "batch[\"input_ids\"].shape, batch[\"output_ids\"].shape, batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_test_batches = tokenized_test_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 100,\n",
    "        \"heads\": 2,\n",
    "        \"layers\": 2,\n",
    "        \"vocab_size\": wrapped_tokenizer.vocab_size + 5,\n",
    "        \"context_length\": MAX_SEQ_LEN,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 2,\n",
    "        \"model_path\": \"../model_files/gpt2_abcd.pth\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-2,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config[\"device\"])\n",
    "\n",
    "print(f\"loaded model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.500992476940155\n",
      "Masked loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Test nn.CrossEntropyLoss for batch size of 20 and sequence length of 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create random logits (model output) with shape [batch_size, sequence_length, vocab_size]\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "vocab_size = 3\n",
    "\n",
    "logits = [\n",
    "    [\n",
    "        [0.1, 0.2, 0.3],  # First sequence 2\n",
    "        [0.4, 0.5, 0.6],  # Second sequence 2\n",
    "        [0.7, 0.8, 0.9],  # Third sequence 2\n",
    "        [1.0, 1.1, 1.2],  # Fourth sequence 2\n",
    "        [1.3, 1.4, 1.5]   # Fifth sequence 2\n",
    "    ],\n",
    "    [\n",
    "        [11.6, 1.7, 1.8],  # First sequence 0\n",
    "        [11.9, 2.0, 2.1],  # Second sequence 0\n",
    "        [22.2, 2.3, 2.4],  # Third sequence 0\n",
    "        [22.5, 2.6, 2.7],  # Fourth sequence 0\n",
    "        [22.8, 2.9, 3.0]   # Fifth sequence 0\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create random targets with shape [batch_size, sequence_length]\n",
    "targets = [\n",
    "    [2, 2, 2, 2, 2],  # First sequence\n",
    "    [0, 0, 0, 0, 0]   # Second sequence\n",
    "]\n",
    "\n",
    "# Convert to tensors\n",
    "logits = torch.tensor(logits, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Reshape logits for CrossEntropyLoss: [batch_size * sequence_length, vocab_size]\n",
    "logits_view = logits.reshape(-1, vocab_size)\n",
    "\n",
    "# Reshape targets for CrossEntropyLoss: [batch_size * sequence_length]\n",
    "targets_view = targets.reshape(-1)\n",
    "\n",
    "# Calculate loss\n",
    "loss = criterion(logits_view, targets_view)\n",
    "print(f\"Total loss: {loss}\")\n",
    "\n",
    "# With attention mask (ignoring padding)\n",
    "attention_mask = torch.ones_like(targets)\n",
    "attention_mask[:, :10] = 0  # Set first 20 positions as padding\n",
    "\n",
    "# Create mask to use for loss calculation\n",
    "mask = attention_mask.reshape(-1).bool()\n",
    "\n",
    "# Calculate masked loss (only on non-padded positions)\n",
    "masked_logits = logits_view[mask]\n",
    "masked_targets = targets_view[mask]\n",
    "masked_loss = criterion(masked_logits, masked_targets)\n",
    "print(f\"Masked loss: {masked_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_torch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 5 Average batch loss: 3.0426719188690186 Perplexity: 20.96117401123047\n",
      "At epoch 1 batch 1 of num_batches 5 Average test loss: 2.279298782348633\n",
      "Test loss without mask: at epoch 0 2.276943588256836 Test perplexity without mask: 9.746845245361328\n",
      "At epoch 2 batch 1 of num_batches 5 Average batch loss: 2.2881996631622314 Perplexity: 9.857175827026367\n",
      "At epoch 2 batch 1 of num_batches 5 Average test loss: 2.278716802597046\n",
      "Test loss without mask: at epoch 1 2.276432180404663 Test perplexity without mask: 9.741862297058105\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.7376, -0.1823, -0.2441,  0.1329, -1.4179]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 5)\n",
    "mask = torch.tensor([0, 1])\n",
    "x[mask == 0] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 2, 3, 4]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"abc\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "#next_token_decoded = wrapped_tokenizer.decode(next_token)\n",
    "#next_token_decoded\n",
    "next_token_decoded = wrapped_tokenizer.decode(prediction[0, -1].argmax().item())\n",
    "next_token_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab -> cdefghijklm.abcdefgh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=20):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -100:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\"ab\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
