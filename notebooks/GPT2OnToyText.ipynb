{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "train_dataset = load_dataset(\"mteb/emotion\", split=\"train\")\n",
    "val_dataset = load_dataset(\"mteb/emotion\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"mteb/emotion\", split=\"test\")\n",
    "\n",
    "# select 100 rows from each dataset\n",
    "train_dataset = train_dataset.select(range(20))\n",
    "val_dataset, test_dataset = train_dataset, train_dataset\n",
    "\n",
    "#val_dataset = val_dataset.select(range(20))\n",
    "#test_dataset = test_dataset.select(range(20))\n",
    "\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i didnt feel humiliated'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "# get gpt2 tokenizer\n",
    "wrapped_tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "\n",
    "# set padding token\n",
    "wrapped_tokenizer.pad_token = wrapped_tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"<|endoftext|>\")['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text', 'input_words', 'output_words', 'input_ids_raw', 'output_ids_raw', 'tokens', 'input_text', 'output_text', 'input_ids', 'output_ids', 'attention_mask'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def slide_window(text_batch):\n",
    "    text_batch['input_words'] = []\n",
    "    text_batch['output_words'] = []\n",
    "    text_batch['input_ids_raw'] = []\n",
    "    text_batch['output_ids_raw'] = []\n",
    "    text_batch['tokens'] = []\n",
    "    text_batch['input_text'] = []\n",
    "    text_batch['output_text'] = []\n",
    "\n",
    "    text_batch['input_ids'] = []\n",
    "    text_batch['output_ids'] = []\n",
    "    text_batch['attention_mask'] = []\n",
    "\n",
    "    for text in text_batch['text']:\n",
    "\n",
    "        tokens = wrapped_tokenizer.tokenize(text)\n",
    "        \n",
    "\n",
    "        # add end of text token\n",
    "        # tokens.append(wrapped_tokenizer.eos_token)\n",
    "\n",
    "        text_batch['tokens'].append(tokens)\n",
    "\n",
    "        # Create input and output tokens for sliding window\n",
    "        input_tokens = tokens[:-1]\n",
    "        output_tokens = tokens[1:]\n",
    "\n",
    "        input_ids = wrapped_tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        output_ids = wrapped_tokenizer.convert_tokens_to_ids(output_tokens)\n",
    "\n",
    "        text_batch['input_words'].append(input_tokens)\n",
    "        text_batch['output_words'].append(output_tokens)\n",
    "\n",
    "        text_batch['input_ids_raw'].append(input_ids)\n",
    "        text_batch['output_ids_raw'].append(output_ids)\n",
    "    \n",
    "        input_text = wrapped_tokenizer.convert_tokens_to_string(input_tokens)\n",
    "        output_text = wrapped_tokenizer.convert_tokens_to_string(output_tokens)\n",
    "        \n",
    "        text_batch['input_text'].append(input_text)\n",
    "        text_batch['output_text'].append(output_text)\n",
    "\n",
    "        attention_mask = [0] * (128 - len(input_tokens)) + [1] * len(input_tokens) \n",
    "\n",
    "        assert len(attention_mask) == 128\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        text_batch['attention_mask'].append(attention_mask)\n",
    "\n",
    "        input_ids = wrapped_tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids[0]\n",
    "        output_ids = wrapped_tokenizer(output_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids[0]\n",
    "\n",
    "        text_batch['input_ids'].append(input_ids)\n",
    "        text_batch['output_ids'].append(output_ids)\n",
    "\n",
    "\n",
    "    return text_batch \n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True)\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       " 'label': 0,\n",
       " 'label_text': 'sadness',\n",
       " 'input_words': ['i',\n",
       "  'Ġcan',\n",
       "  'Ġgo',\n",
       "  'Ġfrom',\n",
       "  'Ġfeeling',\n",
       "  'Ġso',\n",
       "  'Ġhopeless',\n",
       "  'Ġto',\n",
       "  'Ġso',\n",
       "  'Ġdamned',\n",
       "  'Ġhopeful',\n",
       "  'Ġjust',\n",
       "  'Ġfrom',\n",
       "  'Ġbeing',\n",
       "  'Ġaround',\n",
       "  'Ġsomeone',\n",
       "  'Ġwho',\n",
       "  'Ġcares',\n",
       "  'Ġand',\n",
       "  'Ġis'],\n",
       " 'output_words': ['Ġcan',\n",
       "  'Ġgo',\n",
       "  'Ġfrom',\n",
       "  'Ġfeeling',\n",
       "  'Ġso',\n",
       "  'Ġhopeless',\n",
       "  'Ġto',\n",
       "  'Ġso',\n",
       "  'Ġdamned',\n",
       "  'Ġhopeful',\n",
       "  'Ġjust',\n",
       "  'Ġfrom',\n",
       "  'Ġbeing',\n",
       "  'Ġaround',\n",
       "  'Ġsomeone',\n",
       "  'Ġwho',\n",
       "  'Ġcares',\n",
       "  'Ġand',\n",
       "  'Ġis',\n",
       "  'Ġawake'],\n",
       " 'input_ids_raw': [72,\n",
       "  460,\n",
       "  467,\n",
       "  422,\n",
       "  4203,\n",
       "  523,\n",
       "  23292,\n",
       "  284,\n",
       "  523,\n",
       "  28911,\n",
       "  17836,\n",
       "  655,\n",
       "  422,\n",
       "  852,\n",
       "  1088,\n",
       "  2130,\n",
       "  508,\n",
       "  16609,\n",
       "  290,\n",
       "  318],\n",
       " 'output_ids_raw': [460,\n",
       "  467,\n",
       "  422,\n",
       "  4203,\n",
       "  523,\n",
       "  23292,\n",
       "  284,\n",
       "  523,\n",
       "  28911,\n",
       "  17836,\n",
       "  655,\n",
       "  422,\n",
       "  852,\n",
       "  1088,\n",
       "  2130,\n",
       "  508,\n",
       "  16609,\n",
       "  290,\n",
       "  318,\n",
       "  21693],\n",
       " 'tokens': ['i',\n",
       "  'Ġcan',\n",
       "  'Ġgo',\n",
       "  'Ġfrom',\n",
       "  'Ġfeeling',\n",
       "  'Ġso',\n",
       "  'Ġhopeless',\n",
       "  'Ġto',\n",
       "  'Ġso',\n",
       "  'Ġdamned',\n",
       "  'Ġhopeful',\n",
       "  'Ġjust',\n",
       "  'Ġfrom',\n",
       "  'Ġbeing',\n",
       "  'Ġaround',\n",
       "  'Ġsomeone',\n",
       "  'Ġwho',\n",
       "  'Ġcares',\n",
       "  'Ġand',\n",
       "  'Ġis',\n",
       "  'Ġawake'],\n",
       " 'input_text': 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is',\n",
       " 'output_text': ' can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       " 'input_ids': [50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  72,\n",
       "  460,\n",
       "  467,\n",
       "  422,\n",
       "  4203,\n",
       "  523,\n",
       "  23292,\n",
       "  284,\n",
       "  523,\n",
       "  28911,\n",
       "  17836,\n",
       "  655,\n",
       "  422,\n",
       "  852,\n",
       "  1088,\n",
       "  2130,\n",
       "  508,\n",
       "  16609,\n",
       "  290,\n",
       "  318],\n",
       " 'output_ids': [50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  460,\n",
       "  467,\n",
       "  422,\n",
       "  4203,\n",
       "  523,\n",
       "  23292,\n",
       "  284,\n",
       "  523,\n",
       "  28911,\n",
       "  17836,\n",
       "  655,\n",
       "  422,\n",
       "  852,\n",
       "  1088,\n",
       "  2130,\n",
       "  508,\n",
       "  16609,\n",
       "  290,\n",
       "  318,\n",
       "  21693],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f81c9d23390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids'], item['attention_mask']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    attention_mask = [item[2] for item in batch]\n",
    "    input_ids_list = torch.tensor(input_ids)\n",
    "    output_ids_list = torch.tensor(output_ids)\n",
    "    attention_mask_list = torch.tensor(attention_mask)\n",
    "    return input_ids_list, output_ids_list, attention_mask_list\n",
    "\n",
    "batch_size = 20\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 128]), torch.Size([20, 128]), torch.Size([20, 128]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids, attention_masks = batch\n",
    "input_ids.shape, output_ids.shape, attention_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(50257, 128)\n",
       "  (position_embedding): Embedding(128, 128)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (out_project): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (out_project): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=128, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 128,\n",
    "        \"heads\": 2,\n",
    "        \"layers\": 2,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 128,\n",
    "        \"device\": torch.device(\"cuda:0\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 50,\n",
    "        \"model_path\": \"../model_files/gpt2_emotion.pth\",\n",
    "        \"num_train_batches\" : num_train_batches\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "At epoch 1 batch 1 of num_batches 1Average batch loss: 10.868757247924805\n",
      "Test loss without mask: at epoch 0 10.86408519744873 Test perplexity without mask: 52265.15625\n",
      "torch.Size([])\n",
      "At epoch 2 batch 1 of num_batches 1Average batch loss: 10.86417293548584\n",
      "Test loss without mask: at epoch 1 10.859420776367188 Test perplexity without mask: 52021.9375\n",
      "torch.Size([])\n",
      "At epoch 3 batch 1 of num_batches 1Average batch loss: 10.859657287597656\n",
      "Test loss without mask: at epoch 2 10.854936599731445 Test perplexity without mask: 51789.18359375\n",
      "torch.Size([])\n",
      "At epoch 4 batch 1 of num_batches 1Average batch loss: 10.85366439819336\n",
      "Test loss without mask: at epoch 3 10.850471496582031 Test perplexity without mask: 51558.453125\n",
      "torch.Size([])\n",
      "At epoch 5 batch 1 of num_batches 1Average batch loss: 10.850580215454102\n",
      "Test loss without mask: at epoch 4 10.846010208129883 Test perplexity without mask: 51328.94921875\n",
      "torch.Size([])\n",
      "At epoch 6 batch 1 of num_batches 1Average batch loss: 10.844766616821289\n",
      "Test loss without mask: at epoch 5 10.840993881225586 Test perplexity without mask: 51072.11328125\n",
      "torch.Size([])\n",
      "At epoch 7 batch 1 of num_batches 1Average batch loss: 10.841124534606934\n",
      "Test loss without mask: at epoch 6 10.837138175964355 Test perplexity without mask: 50875.5703125\n",
      "torch.Size([])\n",
      "At epoch 8 batch 1 of num_batches 1Average batch loss: 10.836278915405273\n",
      "Test loss without mask: at epoch 7 10.832352638244629 Test perplexity without mask: 50632.6875\n",
      "torch.Size([])\n",
      "At epoch 9 batch 1 of num_batches 1Average batch loss: 10.832099914550781\n",
      "Test loss without mask: at epoch 8 10.826993942260742 Test perplexity without mask: 50362.0859375\n",
      "torch.Size([])\n",
      "At epoch 10 batch 1 of num_batches 1Average batch loss: 10.826627731323242\n",
      "Test loss without mask: at epoch 9 10.822271347045898 Test perplexity without mask: 50124.80859375\n",
      "torch.Size([])\n",
      "At epoch 11 batch 1 of num_batches 1Average batch loss: 10.82249641418457\n",
      "Test loss without mask: at epoch 10 10.81820297241211 Test perplexity without mask: 49921.296875\n",
      "torch.Size([])\n",
      "At epoch 12 batch 1 of num_batches 1Average batch loss: 10.817304611206055\n",
      "Test loss without mask: at epoch 11 10.812631607055664 Test perplexity without mask: 49643.94140625\n",
      "torch.Size([])\n",
      "At epoch 13 batch 1 of num_batches 1Average batch loss: 10.812811851501465\n",
      "Test loss without mask: at epoch 12 10.808046340942383 Test perplexity without mask: 49416.83203125\n",
      "torch.Size([])\n",
      "At epoch 14 batch 1 of num_batches 1Average batch loss: 10.808099746704102\n",
      "Test loss without mask: at epoch 13 10.802921295166016 Test perplexity without mask: 49164.21484375\n",
      "torch.Size([])\n",
      "At epoch 15 batch 1 of num_batches 1Average batch loss: 10.8033447265625\n",
      "Test loss without mask: at epoch 14 10.798261642456055 Test perplexity without mask: 48935.66015625\n",
      "torch.Size([])\n",
      "At epoch 16 batch 1 of num_batches 1Average batch loss: 10.79863166809082\n",
      "Test loss without mask: at epoch 15 10.793036460876465 Test perplexity without mask: 48680.62890625\n",
      "torch.Size([])\n",
      "At epoch 17 batch 1 of num_batches 1Average batch loss: 10.79304027557373\n",
      "Test loss without mask: at epoch 16 10.789657592773438 Test perplexity without mask: 48516.421875\n",
      "torch.Size([])\n",
      "At epoch 18 batch 1 of num_batches 1Average batch loss: 10.788822174072266\n",
      "Test loss without mask: at epoch 17 10.784055709838867 Test perplexity without mask: 48245.3984375\n",
      "torch.Size([])\n",
      "At epoch 19 batch 1 of num_batches 1Average batch loss: 10.783992767333984\n",
      "Test loss without mask: at epoch 18 10.778069496154785 Test perplexity without mask: 47957.453125\n",
      "torch.Size([])\n",
      "At epoch 20 batch 1 of num_batches 1Average batch loss: 10.778460502624512\n",
      "Test loss without mask: at epoch 19 10.773725509643555 Test perplexity without mask: 47749.578125\n",
      "torch.Size([])\n",
      "At epoch 21 batch 1 of num_batches 1Average batch loss: 10.773628234863281\n",
      "Test loss without mask: at epoch 20 10.768659591674805 Test perplexity without mask: 47508.29296875\n",
      "torch.Size([])\n",
      "At epoch 22 batch 1 of num_batches 1Average batch loss: 10.767843246459961\n",
      "Test loss without mask: at epoch 21 10.763130187988281 Test perplexity without mask: 47246.328125\n",
      "torch.Size([])\n",
      "At epoch 23 batch 1 of num_batches 1Average batch loss: 10.763656616210938\n",
      "Test loss without mask: at epoch 22 10.757941246032715 Test perplexity without mask: 47001.8046875\n",
      "torch.Size([])\n",
      "At epoch 24 batch 1 of num_batches 1Average batch loss: 10.756699562072754\n",
      "Test loss without mask: at epoch 23 10.751462936401367 Test perplexity without mask: 46698.296875\n",
      "torch.Size([])\n",
      "At epoch 25 batch 1 of num_batches 1Average batch loss: 10.75088882446289\n",
      "Test loss without mask: at epoch 24 10.747615814208984 Test perplexity without mask: 46518.984375\n",
      "torch.Size([])\n",
      "At epoch 26 batch 1 of num_batches 1Average batch loss: 10.746790885925293\n",
      "Test loss without mask: at epoch 25 10.741310119628906 Test perplexity without mask: 46226.57421875\n",
      "torch.Size([])\n",
      "At epoch 27 batch 1 of num_batches 1Average batch loss: 10.740116119384766\n",
      "Test loss without mask: at epoch 26 10.735304832458496 Test perplexity without mask: 45949.80078125\n",
      "torch.Size([])\n",
      "At epoch 28 batch 1 of num_batches 1Average batch loss: 10.734442710876465\n",
      "Test loss without mask: at epoch 27 10.729032516479492 Test perplexity without mask: 45662.4921875\n",
      "torch.Size([])\n",
      "At epoch 29 batch 1 of num_batches 1Average batch loss: 10.728616714477539\n",
      "Test loss without mask: at epoch 28 10.7214994430542 Test perplexity without mask: 45319.8046875\n",
      "torch.Size([])\n",
      "At epoch 30 batch 1 of num_batches 1Average batch loss: 10.722977638244629\n",
      "Test loss without mask: at epoch 29 10.715547561645508 Test perplexity without mask: 45050.87109375\n",
      "torch.Size([])\n",
      "At epoch 31 batch 1 of num_batches 1Average batch loss: 10.71697998046875\n",
      "Test loss without mask: at epoch 30 10.709774017333984 Test perplexity without mask: 44791.515625\n",
      "torch.Size([])\n",
      "At epoch 32 batch 1 of num_batches 1Average batch loss: 10.708649635314941\n",
      "Test loss without mask: at epoch 31 10.70224380493164 Test perplexity without mask: 44455.4921875\n",
      "torch.Size([])\n",
      "At epoch 33 batch 1 of num_batches 1Average batch loss: 10.701691627502441\n",
      "Test loss without mask: at epoch 32 10.694430351257324 Test perplexity without mask: 44109.49609375\n",
      "torch.Size([])\n",
      "At epoch 34 batch 1 of num_batches 1Average batch loss: 10.694978713989258\n",
      "Test loss without mask: at epoch 33 10.688061714172363 Test perplexity without mask: 43829.46875\n",
      "torch.Size([])\n",
      "At epoch 35 batch 1 of num_batches 1Average batch loss: 10.688794136047363\n",
      "Test loss without mask: at epoch 34 10.679665565490723 Test perplexity without mask: 43463.01171875\n",
      "torch.Size([])\n",
      "At epoch 36 batch 1 of num_batches 1Average batch loss: 10.68140983581543\n",
      "Test loss without mask: at epoch 35 10.672799110412598 Test perplexity without mask: 43165.59765625\n",
      "torch.Size([])\n",
      "At epoch 37 batch 1 of num_batches 1Average batch loss: 10.671121597290039\n",
      "Test loss without mask: at epoch 36 10.663739204406738 Test perplexity without mask: 42776.2890625\n",
      "torch.Size([])\n",
      "At epoch 38 batch 1 of num_batches 1Average batch loss: 10.66390609741211\n",
      "Test loss without mask: at epoch 37 10.65555477142334 Test perplexity without mask: 42427.6171875\n",
      "torch.Size([])\n",
      "At epoch 39 batch 1 of num_batches 1Average batch loss: 10.654156684875488\n",
      "Test loss without mask: at epoch 38 10.647955894470215 Test perplexity without mask: 42106.4375\n",
      "torch.Size([])\n",
      "At epoch 40 batch 1 of num_batches 1Average batch loss: 10.645357131958008\n",
      "Test loss without mask: at epoch 39 10.6370210647583 Test perplexity without mask: 41648.51953125\n",
      "torch.Size([])\n",
      "At epoch 41 batch 1 of num_batches 1Average batch loss: 10.637313842773438\n",
      "Test loss without mask: at epoch 40 10.626876831054688 Test perplexity without mask: 41228.16015625\n",
      "torch.Size([])\n",
      "At epoch 42 batch 1 of num_batches 1Average batch loss: 10.626811981201172\n",
      "Test loss without mask: at epoch 41 10.617819786071777 Test perplexity without mask: 40856.44140625\n",
      "torch.Size([])\n",
      "At epoch 43 batch 1 of num_batches 1Average batch loss: 10.618026733398438\n",
      "Test loss without mask: at epoch 42 10.608263969421387 Test perplexity without mask: 40467.88671875\n",
      "torch.Size([])\n",
      "At epoch 44 batch 1 of num_batches 1Average batch loss: 10.60787296295166\n",
      "Test loss without mask: at epoch 43 10.59709644317627 Test perplexity without mask: 40018.47265625\n",
      "torch.Size([])\n",
      "At epoch 45 batch 1 of num_batches 1Average batch loss: 10.596820831298828\n",
      "Test loss without mask: at epoch 44 10.583837509155273 Test perplexity without mask: 39491.37109375\n",
      "torch.Size([])\n",
      "At epoch 46 batch 1 of num_batches 1Average batch loss: 10.584159851074219\n",
      "Test loss without mask: at epoch 45 10.572823524475098 Test perplexity without mask: 39058.80078125\n",
      "torch.Size([])\n",
      "At epoch 47 batch 1 of num_batches 1Average batch loss: 10.572275161743164\n",
      "Test loss without mask: at epoch 46 10.562576293945312 Test perplexity without mask: 38660.6015625\n",
      "torch.Size([])\n",
      "At epoch 48 batch 1 of num_batches 1Average batch loss: 10.562246322631836\n",
      "Test loss without mask: at epoch 47 10.546606063842773 Test perplexity without mask: 38048.0859375\n",
      "torch.Size([])\n",
      "At epoch 49 batch 1 of num_batches 1Average batch loss: 10.5468111038208\n",
      "Test loss without mask: at epoch 48 10.534242630004883 Test perplexity without mask: 37580.578125\n",
      "torch.Size([])\n",
      "At epoch 50 batch 1 of num_batches 1Average batch loss: 10.535593032836914\n",
      "Test loss without mask: at epoch 49 10.522971153259277 Test perplexity without mask: 37159.3671875\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config)\n",
    "torch.save(gpt2.state_dict(), config[\"model_path\"]) # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.1622, -0.0849, -0.4210, -0.0517,  0.3988]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 5)\n",
    "mask = torch.tensor([0, 1])\n",
    "x[mask == 0] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# load GPT2 from config.model_path\n",
    "import os \n",
    "\n",
    "if os.path.exists(config['model_path']):\n",
    "    gpt2.load_state_dict(torch.load(config['model_path']))\n",
    "    print(\"model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text', 'input_words', 'output_words', 'input_ids_raw', 'output_ids_raw', 'tokens', 'input_text', 'output_text', 'input_ids', 'output_ids', 'attention_mask'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   72,   460,   467,   422,  4203,   523, 23292,   284,   523, 28911,\n",
       "         17836,   655,   422,   852,  1088,  2130,   508, 16609,   290,   318]),\n",
       " tensor([  460,   467,   422,  4203,   523, 23292,   284,   523, 28911, 17836,\n",
       "           655,   422,   852,  1088,  2130,   508, 16609,   290,   318, 21693]),\n",
       " 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tokenized_train_dataset[1]['input_ids']\n",
    "y = tokenized_train_dataset[1]['output_ids']\n",
    "mask = tokenized_train_dataset[1]['attention_mask']\n",
    "\n",
    "x, y, mask = torch.tensor(x), torch.tensor(y), torch.tensor(mask)\n",
    "\n",
    "x[mask==1], y[mask==1], tokenized_train_dataset[1]['input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256,    72,   460,   467,   422,  4203,   523]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' feel'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"i can go from feeling so\", truncation=True, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "#next_token_decoded = wrapped_tokenizer.decode(next_token)\n",
    "#next_token_decoded\n",
    "next_token_decoded = wrapped_tokenizer.decode(prediction[0, -1].argmax().item())\n",
    "next_token_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256,    72,   460,   467,   422,  4203,   523]],\n",
      "       device='cuda:0')\n",
      "tensor([1254], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256,    72,   460,   467,   422,  4203,   523,  1254]],\n",
      "       device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256,    72,   460,   467,   422,  4203,   523,  1254]],\n",
      "       device='cuda:0')\n",
      "tensor([16609], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "            72,   460,   467,   422,  4203,   523,  1254, 16609]],\n",
      "       device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "            72,   460,   467,   422,  4203,   523,  1254, 16609]],\n",
      "       device='cuda:0')\n",
      "tensor([16609], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    72,\n",
      "           460,   467,   422,  4203,   523,  1254, 16609, 16609]],\n",
      "       device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    72,\n",
      "           460,   467,   422,  4203,   523,  1254, 16609, 16609]],\n",
      "       device='cuda:0')\n",
      "tensor([523], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    72,   460,\n",
      "           467,   422,  4203,   523,  1254, 16609, 16609,   523]],\n",
      "       device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,    72,   460,\n",
      "           467,   422,  4203,   523,  1254, 16609, 16609,   523]],\n",
      "       device='cuda:0')\n",
      "tensor([1254], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256,    72,   460,   467,\n",
      "           422,  4203,   523,  1254, 16609, 16609,   523,  1254]],\n",
      "       device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256,    72,   460,   467,\n",
      "           422,  4203,   523,  1254, 16609, 16609,   523,  1254]],\n",
      "       device='cuda:0')\n",
      "tensor([16609], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256,    72,   460,   467,   422,\n",
      "          4203,   523,  1254, 16609, 16609,   523,  1254, 16609]],\n",
      "       device='cuda:0')\n",
      "i can go from feeling so ->  feel cares cares so feel cares\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config):\n",
    "    input_encoding = tokenizer(starting_text, return_tensors=\"pt\")\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "    num_output_tokens = min(10, tokenizer.model_max_length, len(input_encoding['input_ids'][0]))\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "\n",
    "    for i in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        \n",
    "        print(input_ids)\n",
    "        print(next_token)\n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -128:]\n",
    "        print(input_ids)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    output_text = tokenizer.decode(output_tokens)\n",
    "        #output_text += next_text\n",
    "    print(f\"{starting_text} -> {output_text}\")\n",
    "\n",
    "generate_text(\"i can go from feeling so\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
