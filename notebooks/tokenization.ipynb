{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "print(torch.__version__)\n",
    "print(tiktoken.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load verdict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import urllib\n",
    "\n",
    "remote_text_path = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "local_text_path = \"./input.txt\"\n",
    "\n",
    "if not os.path.exists(local_text_path):\n",
    "    urllib.request.urlretrieve(remote_text_path, local_text_path)\n",
    "\n",
    "with open(local_text_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "raw_text[:1000]  # Display the first 1000 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bytepair encoding\n",
    "\n",
    "- GPT2 uses bytepair encoding to breakdown sentences into tokens. \n",
    "- Words that are not defined in vocab can be broken down into subword units, easy way to handle UNK words. If there is a new word, unfamiliarword, it can be tokenized as [unfam, iliar, word]\n",
    "- BPE from tiktoken library is implemented in rust for computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 616, 1438, 318, 22028, 323, 24794, 555, 74, 555, 15418, 292, 67, 355, 6814]\n",
      "Hello my name is Ajayaaaa unk unknasd asda\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello my name is Ajayaaaa unk unknasd asda\"\n",
    "\n",
    "encoded_text = tokenizer.encode(sample_text)\n",
    "print(encoded_text)\n",
    "\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(encoded_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling with a sliding window\n",
    "- Predict target based on input text. Target is input shifted by 1. When we apply the causal LLM mask, things just work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1212, 318], Target: [318, 257]\n",
      "Decoded Input: This is, Decoded Target:  is a\n",
      "Input: [318, 257], Target: [257, 1332]\n",
      "Decoded Input:  is a, Decoded Target:  a test\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(\"This is a test\")\n",
    "context_size = 2\n",
    "for i in range(5):\n",
    "    inpt = encoded_text[i:i+context_size]\n",
    "    target = encoded_text[i+1:i+context_size+1]\n",
    "    if len(inpt) < context_size or len(target) < context_size:\n",
    "        break\n",
    "    decoded_input = tokenizer.decode(inpt)\n",
    "    decoded_target = tokenizer.decode(target)\n",
    "    print(f\"Input: {inpt}, Target: {target}\")\n",
    "    print(f\"Decoded Input: {decoded_input}, Decoded Target: {decoded_target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 1332]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is a test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a tokenizer on wikitext2-raw-v1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)), (',', (5, 6)), ('Ġworld', (6, 12)), ('!', (12, 13))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, world!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = tokenizers.trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(train_dataset[\"text\"], trainer=trainer)\n",
    "tokenizer.post_processor = tokenizers.processors.ByteLevel(trim_offsets=False)\n",
    "\n",
    "tokenizer.save(\"../data/tokenizer.json\")\n",
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "\n",
    "tokenizer.encode(\"Hello my name is Ajay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'ello', 'Ġmy', 'Ġname', 'Ġis', 'ĠAj', 'ay']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello my name is Ajay\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 14979, 1668, 1221, 301, 18603, 288]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Hello my name is Ajay\")\n",
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
