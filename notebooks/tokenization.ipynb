{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "print(torch.__version__)\n",
    "print(tiktoken.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load verdict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import urllib\n",
    "\n",
    "remote_text_path = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "local_text_path = \"./input.txt\"\n",
    "\n",
    "if not os.path.exists(local_text_path):\n",
    "    urllib.request.urlretrieve(remote_text_path, local_text_path)\n",
    "\n",
    "with open(local_text_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "raw_text[:1000]  # Display the first 1000 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bytepair encoding\n",
    "\n",
    "- GPT2 uses bytepair encoding to breakdown sentences into tokens. \n",
    "- Words that are not defined in vocab can be broken down into subword units, easy way to handle UNK words. If there is a new word, unfamiliarword, it can be tokenized as [unfam, iliar, word]\n",
    "- BPE from tiktoken library is implemented in rust for computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 616, 1438, 318, 22028, 323, 24794, 555, 74, 555, 15418, 292, 67, 355, 6814]\n",
      "Hello my name is Ajayaaaa unk unknasd asda\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello my name is Ajayaaaa unk unknasd asda\"\n",
    "\n",
    "encoded_text = tokenizer.encode(sample_text)\n",
    "print(encoded_text)\n",
    "\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(encoded_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sampling with a sliding window\n",
    "- Predict target based on input text. Target is input shifted by 1. When we apply the causal LLM mask, things just work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 257, 1332]\n",
      "Input: [1212, 318, 257], Target: [318, 257, 1332]\n",
      "Decoded Input: This is a, Decoded Target:  is a test\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(\"This is a test\")\n",
    "context_size = 3\n",
    "for i in range(5):\n",
    "    inpt = encoded_text[i:i+context_size]\n",
    "    target = encoded_text[i+1:i+context_size+1]\n",
    "    if len(inpt) < context_size or len(target) < context_size:\n",
    "        break\n",
    "    decoded_input = tokenizer.decode(inpt)\n",
    "    decoded_target = tokenizer.decode(target)\n",
    "    print(f\"Input: {inpt}, Target: {target}\")\n",
    "    print(f\"Decoded Input: {decoded_input}, Decoded Target: {decoded_target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 257, 1332]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
