{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 23767\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "# filter out empty lines\n",
    "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "val_dataset = val_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "test_dataset = test_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained on custom dataset with vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "from utils import get_train_tokenizer\n",
    "wrapped_tokenizer = get_train_tokenizer(train_dataset, vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"<|endoftext|>\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3252, 1226, 1470, 1865, 1030, 33, 74, 1068]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer([\"Hello my name is Ajay\"])['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6293bf95554f0ca3813b5d07b99bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'output_ids'],\n",
       "    num_rows: 25902\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from utils import slide_window\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  51,\n",
       "  3521,\n",
       "  182,\n",
       "  1302,\n",
       "  8850,\n",
       "  19,\n",
       "  26,\n",
       "  1309,\n",
       "  2228,\n",
       "  9634,\n",
       "  8,\n",
       "  3332,\n",
       "  26,\n",
       "  809,\n",
       "  757,\n",
       "  616,\n",
       "  692,\n",
       "  636,\n",
       "  688,\n",
       "  647,\n",
       "  684,\n",
       "  687,\n",
       "  637,\n",
       "  19,\n",
       "  12,\n",
       "  5982,\n",
       "  14,\n",
       "  8850,\n",
       "  1027,\n",
       "  1016,\n",
       "  3980,\n",
       "  1800,\n",
       "  19,\n",
       "  9,\n",
       "  12,\n",
       "  5558,\n",
       "  3794,\n",
       "  1032,\n",
       "  1028,\n",
       "  8850,\n",
       "  9634,\n",
       "  3103,\n",
       "  3150,\n",
       "  2420,\n",
       "  12,\n",
       "  1030,\n",
       "  65,\n",
       "  5428,\n",
       "  1210,\n",
       "  2159,\n",
       "  1060,\n",
       "  2835,\n",
       "  2143,\n",
       "  1398,\n",
       "  2509,\n",
       "  1079,\n",
       "  1666,\n",
       "  3643,\n",
       "  1026,\n",
       "  7836,\n",
       "  14,\n",
       "  54,\n",
       "  1810,\n",
       "  1052,\n",
       "  1016,\n",
       "  5250,\n",
       "  2327,\n",
       "  1294,\n",
       "  14,\n",
       "  1401,\n",
       "  1038,\n",
       "  1345,\n",
       "  1013,\n",
       "  2030,\n",
       "  2300,\n",
       "  1013,\n",
       "  2420,\n",
       "  12,\n",
       "  1033,\n",
       "  1030,\n",
       "  1016,\n",
       "  2053,\n",
       "  1398,\n",
       "  1013,\n",
       "  1016,\n",
       "  8850,\n",
       "  1559,\n",
       "  14,\n",
       "  3327,\n",
       "  2449,\n",
       "  1031,\n",
       "  1016,\n",
       "  1743,\n",
       "  8974,\n",
       "  1027,\n",
       "  5428,\n",
       "  1210,\n",
       "  1026,\n",
       "  2112,\n",
       "  1060,\n",
       "  1308,\n",
       "  6903,\n",
       "  1028,\n",
       "  1180,\n",
       "  7018,\n",
       "  1388,\n",
       "  12,\n",
       "  1016,\n",
       "  2066,\n",
       "  3919,\n",
       "  7766,\n",
       "  1032,\n",
       "  1016,\n",
       "  1211,\n",
       "  1398,\n",
       "  1026,\n",
       "  5164,\n",
       "  1016,\n",
       "  2,\n",
       "  8565,\n",
       "  8310,\n",
       "  2,\n",
       "  12,\n",
       "  65,\n",
       "  2355,\n",
       "  1024,\n",
       "  2334,\n",
       "  4260,\n",
       "  5191,\n",
       "  1016,\n",
       "  3478,\n",
       "  1027,\n",
       "  5277,\n",
       "  1215,\n",
       "  1393,\n",
       "  1016,\n",
       "  4074,\n",
       "  1798,\n",
       "  1493,\n",
       "  1015,\n",
       "  1771,\n",
       "  1244,\n",
       "  3630,\n",
       "  4526,\n",
       "  2575,\n",
       "  3104,\n",
       "  1026,\n",
       "  1105,\n",
       "  4052,\n",
       "  1444,\n",
       "  1587,\n",
       "  1016,\n",
       "  8040,\n",
       "  4260,\n",
       "  2,\n",
       "  4001,\n",
       "  1045,\n",
       "  1020,\n",
       "  89,\n",
       "  50,\n",
       "  3605,\n",
       "  2,\n",
       "  14],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'output_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3521,\n",
       "  182,\n",
       "  1302,\n",
       "  8850,\n",
       "  19,\n",
       "  26,\n",
       "  1309,\n",
       "  2228,\n",
       "  9634,\n",
       "  8,\n",
       "  3332,\n",
       "  26,\n",
       "  809,\n",
       "  757,\n",
       "  616,\n",
       "  692,\n",
       "  636,\n",
       "  688,\n",
       "  647,\n",
       "  684,\n",
       "  687,\n",
       "  637,\n",
       "  19,\n",
       "  12,\n",
       "  5982,\n",
       "  14,\n",
       "  8850,\n",
       "  1027,\n",
       "  1016,\n",
       "  3980,\n",
       "  1800,\n",
       "  19,\n",
       "  9,\n",
       "  12,\n",
       "  5558,\n",
       "  3794,\n",
       "  1032,\n",
       "  1028,\n",
       "  8850,\n",
       "  9634,\n",
       "  3103,\n",
       "  3150,\n",
       "  2420,\n",
       "  12,\n",
       "  1030,\n",
       "  65,\n",
       "  5428,\n",
       "  1210,\n",
       "  2159,\n",
       "  1060,\n",
       "  2835,\n",
       "  2143,\n",
       "  1398,\n",
       "  2509,\n",
       "  1079,\n",
       "  1666,\n",
       "  3643,\n",
       "  1026,\n",
       "  7836,\n",
       "  14,\n",
       "  54,\n",
       "  1810,\n",
       "  1052,\n",
       "  1016,\n",
       "  5250,\n",
       "  2327,\n",
       "  1294,\n",
       "  14,\n",
       "  1401,\n",
       "  1038,\n",
       "  1345,\n",
       "  1013,\n",
       "  2030,\n",
       "  2300,\n",
       "  1013,\n",
       "  2420,\n",
       "  12,\n",
       "  1033,\n",
       "  1030,\n",
       "  1016,\n",
       "  2053,\n",
       "  1398,\n",
       "  1013,\n",
       "  1016,\n",
       "  8850,\n",
       "  1559,\n",
       "  14,\n",
       "  3327,\n",
       "  2449,\n",
       "  1031,\n",
       "  1016,\n",
       "  1743,\n",
       "  8974,\n",
       "  1027,\n",
       "  5428,\n",
       "  1210,\n",
       "  1026,\n",
       "  2112,\n",
       "  1060,\n",
       "  1308,\n",
       "  6903,\n",
       "  1028,\n",
       "  1180,\n",
       "  7018,\n",
       "  1388,\n",
       "  12,\n",
       "  1016,\n",
       "  2066,\n",
       "  3919,\n",
       "  7766,\n",
       "  1032,\n",
       "  1016,\n",
       "  1211,\n",
       "  1398,\n",
       "  1026,\n",
       "  5164,\n",
       "  1016,\n",
       "  2,\n",
       "  8565,\n",
       "  8310,\n",
       "  2,\n",
       "  12,\n",
       "  65,\n",
       "  2355,\n",
       "  1024,\n",
       "  2334,\n",
       "  4260,\n",
       "  5191,\n",
       "  1016,\n",
       "  3478,\n",
       "  1027,\n",
       "  5277,\n",
       "  1215,\n",
       "  1393,\n",
       "  1016,\n",
       "  4074,\n",
       "  1798,\n",
       "  1493,\n",
       "  1015,\n",
       "  1771,\n",
       "  1244,\n",
       "  3630,\n",
       "  4526,\n",
       "  2575,\n",
       "  3104,\n",
       "  1026,\n",
       "  1105,\n",
       "  4052,\n",
       "  1444,\n",
       "  1587,\n",
       "  1016,\n",
       "  8040,\n",
       "  4260,\n",
       "  2,\n",
       "  4001,\n",
       "  1045,\n",
       "  1020,\n",
       "  89,\n",
       "  50,\n",
       "  3605,\n",
       "  2,\n",
       "  14,\n",
       "  0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7ff334908050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    output_ids = torch.stack([torch.tensor(item[\"output_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"output_ids\": output_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "batch_size = 30\n",
    "train_torch_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_torch_dataloader = DataLoader(tokenized_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_torch_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "train_torch_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 256]), torch.Size([30, 256]), torch.Size([30, 256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "batch[\"input_ids\"].shape, batch[\"output_ids\"].shape, batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model loaded to device'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 128,\n",
    "        \"heads\": 4,\n",
    "        \"layers\": 4,\n",
    "        \"vocab_size\": wrapped_tokenizer.vocab_size,\n",
    "        \"context_length\": MAX_SEQ_LEN,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 25,\n",
    "        \"model_path\": \"../model_files/gpt2.pth\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-4,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "\"model loaded to device\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 863 Average batch loss: 9.315194129943848 Perplexity: 11105.4814453125\n",
      "At epoch 1 batch 100 of num_batches 863 Average batch loss: 8.905252351760865 Perplexity: 7370.5859375\n",
      "At epoch 1 batch 200 of num_batches 863 Average batch loss: 8.685583033561706 Perplexity: 5916.98974609375\n",
      "At epoch 1 batch 300 of num_batches 863 Average batch loss: 8.589148400624593 Perplexity: 5373.03662109375\n",
      "At epoch 1 batch 400 of num_batches 863 Average batch loss: 8.535176451206206 Perplexity: 5090.72900390625\n",
      "At epoch 1 batch 500 of num_batches 863 Average batch loss: 8.497671630859376 Perplexity: 4903.3408203125\n",
      "At epoch 1 batch 600 of num_batches 863 Average batch loss: 8.466893697579701 Perplexity: 4754.72509765625\n",
      "At epoch 1 batch 700 of num_batches 863 Average batch loss: 8.445474405288696 Perplexity: 4653.9638671875\n",
      "At epoch 1 batch 800 of num_batches 863 Average batch loss: 8.42371909558773 Perplexity: 4553.8095703125\n",
      "At epoch 1 batch 1 of num_batches 89 Average test loss: 8.507019996643066\n",
      "Test loss without mask: at epoch 0 8.264940139982436 Test perplexity without mask: 3885.24072265625\n",
      "At epoch 2 batch 1 of num_batches 863 Average batch loss: 8.09067153930664 Perplexity: 3263.878662109375\n",
      "At epoch 2 batch 100 of num_batches 863 Average batch loss: 8.260759406089782 Perplexity: 3869.031005859375\n",
      "At epoch 2 batch 200 of num_batches 863 Average batch loss: 8.265171341896057 Perplexity: 3886.1376953125\n",
      "At epoch 2 batch 300 of num_batches 863 Average batch loss: 8.261617150306702 Perplexity: 3872.349365234375\n",
      "At epoch 2 batch 400 of num_batches 863 Average batch loss: 8.252654719352723 Perplexity: 3837.8017578125\n",
      "At epoch 2 batch 500 of num_batches 863 Average batch loss: 8.250316476821899 Perplexity: 3828.837890625\n",
      "At epoch 2 batch 600 of num_batches 863 Average batch loss: 8.243014084498087 Perplexity: 3800.98046875\n",
      "At epoch 2 batch 700 of num_batches 863 Average batch loss: 8.237891742161342 Perplexity: 3781.56103515625\n",
      "At epoch 2 batch 800 of num_batches 863 Average batch loss: 8.233673495650292 Perplexity: 3765.639892578125\n",
      "At epoch 2 batch 1 of num_batches 89 Average test loss: 8.44113826751709\n",
      "Test loss without mask: at epoch 1 8.184925397237143 Test perplexity without mask: 3586.474853515625\n",
      "At epoch 3 batch 1 of num_batches 863 Average batch loss: 8.441605567932129 Perplexity: 4635.9921875\n",
      "At epoch 3 batch 100 of num_batches 863 Average batch loss: 8.177526240348817 Perplexity: 3560.037841796875\n",
      "At epoch 3 batch 200 of num_batches 863 Average batch loss: 8.174766418933869 Perplexity: 3550.22607421875\n",
      "At epoch 3 batch 300 of num_batches 863 Average batch loss: 8.17343126296997 Perplexity: 3545.489013671875\n",
      "At epoch 3 batch 400 of num_batches 863 Average batch loss: 8.176015474796294 Perplexity: 3554.6640625\n",
      "At epoch 3 batch 500 of num_batches 863 Average batch loss: 8.172065371513368 Perplexity: 3540.650390625\n",
      "At epoch 3 batch 600 of num_batches 863 Average batch loss: 8.171602991422018 Perplexity: 3539.01318359375\n",
      "At epoch 3 batch 700 of num_batches 863 Average batch loss: 8.170179786000933 Perplexity: 3533.977783203125\n",
      "At epoch 3 batch 800 of num_batches 863 Average batch loss: 8.163718090057372 Perplexity: 3511.2177734375\n",
      "At epoch 3 batch 1 of num_batches 89 Average test loss: 8.389052391052246\n",
      "Test loss without mask: at epoch 2 8.131769042544894 Test perplexity without mask: 3400.810791015625\n",
      "At epoch 4 batch 1 of num_batches 863 Average batch loss: 8.001895904541016 Perplexity: 2986.614990234375\n",
      "At epoch 4 batch 100 of num_batches 863 Average batch loss: 8.10540475845337 Perplexity: 3312.322265625\n",
      "At epoch 4 batch 200 of num_batches 863 Average batch loss: 8.114315969944 Perplexity: 3341.970947265625\n",
      "At epoch 4 batch 300 of num_batches 863 Average batch loss: 8.113977325757345 Perplexity: 3340.839599609375\n",
      "At epoch 4 batch 400 of num_batches 863 Average batch loss: 8.11264724612236 Perplexity: 3336.39794921875\n",
      "At epoch 4 batch 500 of num_batches 863 Average batch loss: 8.113261118888856 Perplexity: 3338.44775390625\n",
      "At epoch 4 batch 600 of num_batches 863 Average batch loss: 8.114848142464956 Perplexity: 3343.749755859375\n",
      "At epoch 4 batch 700 of num_batches 863 Average batch loss: 8.10623149054391 Perplexity: 3315.062255859375\n",
      "At epoch 4 batch 800 of num_batches 863 Average batch loss: 8.105523554086686 Perplexity: 3312.714111328125\n",
      "At epoch 4 batch 1 of num_batches 89 Average test loss: 8.349987983703613\n",
      "Test loss without mask: at epoch 3 8.087004301283095 Test perplexity without mask: 3251.932373046875\n",
      "At epoch 5 batch 1 of num_batches 863 Average batch loss: 8.154706001281738 Perplexity: 3479.716064453125\n",
      "At epoch 5 batch 100 of num_batches 863 Average batch loss: 8.095113015174865 Perplexity: 3278.406494140625\n",
      "At epoch 5 batch 200 of num_batches 863 Average batch loss: 8.082630076408385 Perplexity: 3237.73779296875\n",
      "At epoch 5 batch 300 of num_batches 863 Average batch loss: 8.071827893257142 Perplexity: 3202.951171875\n",
      "At epoch 5 batch 400 of num_batches 863 Average batch loss: 8.069991439580917 Perplexity: 3197.073486328125\n",
      "At epoch 5 batch 500 of num_batches 863 Average batch loss: 8.070552853584289 Perplexity: 3198.86962890625\n",
      "At epoch 5 batch 600 of num_batches 863 Average batch loss: 8.063709093729654 Perplexity: 3177.052734375\n",
      "At epoch 5 batch 700 of num_batches 863 Average batch loss: 8.063450251306806 Perplexity: 3176.228759765625\n",
      "At epoch 5 batch 800 of num_batches 863 Average batch loss: 8.062514725327492 Perplexity: 3173.258544921875\n",
      "At epoch 5 batch 1 of num_batches 89 Average test loss: 8.316433906555176\n",
      "Test loss without mask: at epoch 4 8.051940298080444 Test perplexity without mask: 3139.88037109375\n",
      "At epoch 6 batch 1 of num_batches 863 Average batch loss: 8.213500022888184 Perplexity: 3690.4365234375\n",
      "At epoch 6 batch 100 of num_batches 863 Average batch loss: 8.043835854530334 Perplexity: 3114.53662109375\n",
      "At epoch 6 batch 200 of num_batches 863 Average batch loss: 8.036620576381683 Perplexity: 3092.14453125\n",
      "At epoch 6 batch 300 of num_batches 863 Average batch loss: 8.030234977404277 Perplexity: 3072.464599609375\n",
      "At epoch 6 batch 400 of num_batches 863 Average batch loss: 8.029753645658493 Perplexity: 3070.985107421875\n",
      "At epoch 6 batch 500 of num_batches 863 Average batch loss: 8.022822721481322 Perplexity: 3049.772705078125\n",
      "At epoch 6 batch 600 of num_batches 863 Average batch loss: 8.020265990098318 Perplexity: 3041.985107421875\n",
      "At epoch 6 batch 700 of num_batches 863 Average batch loss: 8.02188163961683 Perplexity: 3046.906494140625\n",
      "At epoch 6 batch 800 of num_batches 863 Average batch loss: 8.020159579515457 Perplexity: 3041.6630859375\n",
      "At epoch 6 batch 1 of num_batches 89 Average test loss: 8.292048454284668\n",
      "Test loss without mask: at epoch 5 8.022536987728543 Test perplexity without mask: 3048.9033203125\n",
      "At epoch 7 batch 1 of num_batches 863 Average batch loss: 7.9050750732421875 Perplexity: 2711.006103515625\n",
      "At epoch 7 batch 100 of num_batches 863 Average batch loss: 7.983883237838745 Perplexity: 2933.300048828125\n",
      "At epoch 7 batch 200 of num_batches 863 Average batch loss: 7.985210030078888 Perplexity: 2937.19384765625\n",
      "At epoch 7 batch 300 of num_batches 863 Average batch loss: 7.9931294695536295 Perplexity: 2960.546875\n",
      "At epoch 7 batch 400 of num_batches 863 Average batch loss: 7.988084946870804 Perplexity: 2945.650146484375\n",
      "At epoch 7 batch 500 of num_batches 863 Average batch loss: 7.990892281532288 Perplexity: 2953.931884765625\n",
      "At epoch 7 batch 600 of num_batches 863 Average batch loss: 7.99057723124822 Perplexity: 2953.0009765625\n",
      "At epoch 7 batch 700 of num_batches 863 Average batch loss: 7.9896721921648295 Perplexity: 2950.32958984375\n",
      "At epoch 7 batch 800 of num_batches 863 Average batch loss: 7.987513538002968 Perplexity: 2943.9677734375\n",
      "At epoch 7 batch 1 of num_batches 89 Average test loss: 8.273881912231445\n",
      "Test loss without mask: at epoch 6 7.994717650943333 Test perplexity without mask: 2965.2529296875\n",
      "At epoch 8 batch 1 of num_batches 863 Average batch loss: 8.061468124389648 Perplexity: 3169.940673828125\n",
      "At epoch 8 batch 100 of num_batches 863 Average batch loss: 7.95455376625061 Perplexity: 2848.516357421875\n",
      "At epoch 8 batch 200 of num_batches 863 Average batch loss: 7.941865167617798 Perplexity: 2812.60107421875\n",
      "At epoch 8 batch 300 of num_batches 863 Average batch loss: 7.94197439511617 Perplexity: 2812.908203125\n",
      "At epoch 8 batch 400 of num_batches 863 Average batch loss: 7.943853635787963 Perplexity: 2818.200439453125\n",
      "At epoch 8 batch 500 of num_batches 863 Average batch loss: 7.949183556556702 Perplexity: 2833.260498046875\n",
      "At epoch 8 batch 600 of num_batches 863 Average batch loss: 7.953423426151276 Perplexity: 2845.299072265625\n",
      "At epoch 8 batch 700 of num_batches 863 Average batch loss: 7.953213906969343 Perplexity: 2844.702392578125\n",
      "At epoch 8 batch 800 of num_batches 863 Average batch loss: 7.951815513372421 Perplexity: 2840.72802734375\n",
      "At epoch 8 batch 1 of num_batches 89 Average test loss: 8.261903762817383\n",
      "Test loss without mask: at epoch 7 7.971121581395467 Test perplexity without mask: 2896.104248046875\n",
      "At epoch 9 batch 1 of num_batches 863 Average batch loss: 7.696588039398193 Perplexity: 2200.826171875\n",
      "At epoch 9 batch 100 of num_batches 863 Average batch loss: 7.9682519912719725 Perplexity: 2887.8056640625\n",
      "At epoch 9 batch 200 of num_batches 863 Average batch loss: 7.951913886070251 Perplexity: 2841.007080078125\n",
      "At epoch 9 batch 300 of num_batches 863 Average batch loss: 7.936260606447855 Perplexity: 2796.882568359375\n",
      "At epoch 9 batch 400 of num_batches 863 Average batch loss: 7.9313216042518615 Perplexity: 2783.1025390625\n",
      "At epoch 9 batch 500 of num_batches 863 Average batch loss: 7.933907123565674 Perplexity: 2790.307373046875\n",
      "At epoch 9 batch 600 of num_batches 863 Average batch loss: 7.928045414288839 Perplexity: 2773.9990234375\n",
      "At epoch 9 batch 700 of num_batches 863 Average batch loss: 7.927312617301941 Perplexity: 2771.968017578125\n",
      "At epoch 9 batch 800 of num_batches 863 Average batch loss: 7.926180132627487 Perplexity: 2768.83056640625\n",
      "At epoch 9 batch 1 of num_batches 89 Average test loss: 8.234082221984863\n",
      "Test loss without mask: at epoch 8 7.950411171383328 Test perplexity without mask: 2836.741455078125\n",
      "At epoch 10 batch 1 of num_batches 863 Average batch loss: 8.158013343811035 Perplexity: 3491.243896484375\n",
      "At epoch 10 batch 100 of num_batches 863 Average batch loss: 7.90065526008606 Perplexity: 2699.05029296875\n",
      "At epoch 10 batch 200 of num_batches 863 Average batch loss: 7.901414635181427 Perplexity: 2701.101318359375\n",
      "At epoch 10 batch 300 of num_batches 863 Average batch loss: 7.894315830866495 Perplexity: 2681.99365234375\n",
      "At epoch 10 batch 400 of num_batches 863 Average batch loss: 7.910652062892914 Perplexity: 2726.167724609375\n",
      "At epoch 10 batch 500 of num_batches 863 Average batch loss: 7.900007277488709 Perplexity: 2697.3017578125\n",
      "At epoch 10 batch 600 of num_batches 863 Average batch loss: 7.897388774553935 Perplexity: 2690.248779296875\n",
      "At epoch 10 batch 700 of num_batches 863 Average batch loss: 7.892390931674412 Perplexity: 2676.8359375\n",
      "At epoch 10 batch 800 of num_batches 863 Average batch loss: 7.893307637572288 Perplexity: 2679.29150390625\n",
      "At epoch 10 batch 1 of num_batches 89 Average test loss: 8.221949577331543\n",
      "Test loss without mask: at epoch 9 7.93213168780009 Test perplexity without mask: 2785.358154296875\n",
      "At epoch 11 batch 1 of num_batches 863 Average batch loss: 7.888910293579102 Perplexity: 2667.535400390625\n",
      "At epoch 11 batch 100 of num_batches 863 Average batch loss: 7.859486784934997 Perplexity: 2590.190185546875\n",
      "At epoch 11 batch 200 of num_batches 863 Average batch loss: 7.875766854286194 Perplexity: 2632.7041015625\n",
      "At epoch 11 batch 300 of num_batches 863 Average batch loss: 7.871031724611918 Perplexity: 2620.267578125\n",
      "At epoch 11 batch 400 of num_batches 863 Average batch loss: 7.866814645528794 Perplexity: 2609.240966796875\n",
      "At epoch 11 batch 500 of num_batches 863 Average batch loss: 7.863655097961426 Perplexity: 2601.010009765625\n",
      "At epoch 11 batch 600 of num_batches 863 Average batch loss: 7.866776377360026 Perplexity: 2609.141357421875\n",
      "At epoch 11 batch 700 of num_batches 863 Average batch loss: 7.864376828329903 Perplexity: 2602.888427734375\n",
      "At epoch 11 batch 800 of num_batches 863 Average batch loss: 7.8649483305215835 Perplexity: 2604.375732421875\n",
      "At epoch 11 batch 1 of num_batches 89 Average test loss: 8.201719284057617\n",
      "Test loss without mask: at epoch 10 7.916258425182766 Test perplexity without mask: 2741.494140625\n",
      "At epoch 12 batch 1 of num_batches 863 Average batch loss: 7.975644111633301 Perplexity: 2909.231201171875\n",
      "At epoch 12 batch 100 of num_batches 863 Average batch loss: 7.880207047462464 Perplexity: 2644.420166015625\n",
      "At epoch 12 batch 200 of num_batches 863 Average batch loss: 7.85394581079483 Perplexity: 2575.8779296875\n",
      "At epoch 12 batch 300 of num_batches 863 Average batch loss: 7.856665790875753 Perplexity: 2582.8935546875\n",
      "At epoch 12 batch 400 of num_batches 863 Average batch loss: 7.857579790353775 Perplexity: 2585.255615234375\n",
      "At epoch 12 batch 500 of num_batches 863 Average batch loss: 7.855860779762268 Perplexity: 2580.8154296875\n",
      "At epoch 12 batch 600 of num_batches 863 Average batch loss: 7.849646744728088 Perplexity: 2564.82763671875\n",
      "At epoch 12 batch 700 of num_batches 863 Average batch loss: 7.8493129321507045 Perplexity: 2563.9716796875\n",
      "At epoch 12 batch 800 of num_batches 863 Average batch loss: 7.844625163674355 Perplexity: 2551.98046875\n",
      "At epoch 12 batch 1 of num_batches 89 Average test loss: 8.19765567779541\n",
      "Test loss without mask: at epoch 11 7.8986531893412275 Test perplexity without mask: 2693.651611328125\n",
      "At epoch 13 batch 1 of num_batches 863 Average batch loss: 7.812083721160889 Perplexity: 2470.2724609375\n",
      "At epoch 13 batch 100 of num_batches 863 Average batch loss: 7.816459131240845 Perplexity: 2481.104736328125\n",
      "At epoch 13 batch 200 of num_batches 863 Average batch loss: 7.815692865848542 Perplexity: 2479.2041015625\n",
      "At epoch 13 batch 300 of num_batches 863 Average batch loss: 7.83368318716685 Perplexity: 2524.208984375\n",
      "At epoch 13 batch 400 of num_batches 863 Average batch loss: 7.832312722206115 Perplexity: 2520.752197265625\n",
      "At epoch 13 batch 500 of num_batches 863 Average batch loss: 7.824409855842591 Perplexity: 2500.91015625\n",
      "At epoch 13 batch 600 of num_batches 863 Average batch loss: 7.8216619348526 Perplexity: 2494.046875\n",
      "At epoch 13 batch 700 of num_batches 863 Average batch loss: 7.823682670593262 Perplexity: 2499.092041015625\n",
      "At epoch 13 batch 800 of num_batches 863 Average batch loss: 7.823060245513916 Perplexity: 2497.536376953125\n",
      "At epoch 13 batch 1 of num_batches 89 Average test loss: 8.181206703186035\n",
      "Test loss without mask: at epoch 12 7.88410193655226 Test perplexity without mask: 2654.73974609375\n",
      "At epoch 14 batch 1 of num_batches 863 Average batch loss: 7.8675856590271 Perplexity: 2611.25341796875\n",
      "At epoch 14 batch 100 of num_batches 863 Average batch loss: 7.7968046808242795 Perplexity: 2432.81640625\n",
      "At epoch 14 batch 200 of num_batches 863 Average batch loss: 7.806892070770264 Perplexity: 2457.48046875\n",
      "At epoch 14 batch 300 of num_batches 863 Average batch loss: 7.802183704376221 Perplexity: 2445.937255859375\n",
      "At epoch 14 batch 400 of num_batches 863 Average batch loss: 7.81054187297821 Perplexity: 2466.467041015625\n",
      "At epoch 14 batch 500 of num_batches 863 Average batch loss: 7.806526385307312 Perplexity: 2456.581787109375\n",
      "At epoch 14 batch 600 of num_batches 863 Average batch loss: 7.8021244963010155 Perplexity: 2445.79248046875\n",
      "At epoch 14 batch 700 of num_batches 863 Average batch loss: 7.805081844329834 Perplexity: 2453.036376953125\n",
      "At epoch 14 batch 800 of num_batches 863 Average batch loss: 7.803431181311607 Perplexity: 2448.990234375\n",
      "At epoch 14 batch 1 of num_batches 89 Average test loss: 8.17615032196045\n",
      "Test loss without mask: at epoch 13 7.874655305014716 Test perplexity without mask: 2629.779541015625\n",
      "At epoch 15 batch 1 of num_batches 863 Average batch loss: 8.350648880004883 Perplexity: 4232.9267578125\n",
      "At epoch 15 batch 100 of num_batches 863 Average batch loss: 7.7756774663925174 Perplexity: 2381.95703125\n",
      "At epoch 15 batch 200 of num_batches 863 Average batch loss: 7.783414921760559 Perplexity: 2400.4580078125\n",
      "At epoch 15 batch 300 of num_batches 863 Average batch loss: 7.767930316925049 Perplexity: 2363.57470703125\n",
      "At epoch 15 batch 400 of num_batches 863 Average batch loss: 7.774679201841354 Perplexity: 2379.579833984375\n",
      "At epoch 15 batch 500 of num_batches 863 Average batch loss: 7.786214012145996 Perplexity: 2407.186279296875\n",
      "At epoch 15 batch 600 of num_batches 863 Average batch loss: 7.783669445514679 Perplexity: 2401.0693359375\n",
      "At epoch 15 batch 700 of num_batches 863 Average batch loss: 7.785142267772129 Perplexity: 2404.608642578125\n",
      "At epoch 15 batch 800 of num_batches 863 Average batch loss: 7.78190968990326 Perplexity: 2396.84716796875\n",
      "At epoch 15 batch 1 of num_batches 89 Average test loss: 8.171599388122559\n",
      "Test loss without mask: at epoch 14 7.860590097639296 Test perplexity without mask: 2593.0498046875\n",
      "At epoch 16 batch 1 of num_batches 863 Average batch loss: 8.303474426269531 Perplexity: 4037.87744140625\n",
      "At epoch 16 batch 100 of num_batches 863 Average batch loss: 7.75619421005249 Perplexity: 2335.9970703125\n",
      "At epoch 16 batch 200 of num_batches 863 Average batch loss: 7.751704206466675 Perplexity: 2325.5322265625\n",
      "At epoch 16 batch 300 of num_batches 863 Average batch loss: 7.753003431955974 Perplexity: 2328.555908203125\n",
      "At epoch 16 batch 400 of num_batches 863 Average batch loss: 7.759119119644165 Perplexity: 2342.83984375\n",
      "At epoch 16 batch 500 of num_batches 863 Average batch loss: 7.767676750183106 Perplexity: 2362.975341796875\n",
      "At epoch 16 batch 600 of num_batches 863 Average batch loss: 7.773327356179555 Perplexity: 2376.365234375\n",
      "At epoch 16 batch 700 of num_batches 863 Average batch loss: 7.765885263851711 Perplexity: 2358.745849609375\n",
      "At epoch 16 batch 800 of num_batches 863 Average batch loss: 7.768001566529274 Perplexity: 2363.74267578125\n",
      "At epoch 16 batch 1 of num_batches 89 Average test loss: 8.153562545776367\n",
      "Test loss without mask: at epoch 15 7.849203740225898 Test perplexity without mask: 2563.691650390625\n",
      "At epoch 17 batch 1 of num_batches 863 Average batch loss: 7.537070274353027 Perplexity: 1876.3248291015625\n",
      "At epoch 17 batch 100 of num_batches 863 Average batch loss: 7.785149579048157 Perplexity: 2404.625732421875\n",
      "At epoch 17 batch 200 of num_batches 863 Average batch loss: 7.784606409072876 Perplexity: 2403.320068359375\n",
      "At epoch 17 batch 300 of num_batches 863 Average batch loss: 7.765580921173096 Perplexity: 2358.0283203125\n",
      "At epoch 17 batch 400 of num_batches 863 Average batch loss: 7.752754113674164 Perplexity: 2327.975341796875\n",
      "At epoch 17 batch 500 of num_batches 863 Average batch loss: 7.750194464683533 Perplexity: 2322.024169921875\n",
      "At epoch 17 batch 600 of num_batches 863 Average batch loss: 7.745686804453532 Perplexity: 2311.5810546875\n",
      "At epoch 17 batch 700 of num_batches 863 Average batch loss: 7.749212258883885 Perplexity: 2319.744384765625\n",
      "At epoch 17 batch 800 of num_batches 863 Average batch loss: 7.753640766739846 Perplexity: 2330.039794921875\n",
      "At epoch 17 batch 1 of num_batches 89 Average test loss: 8.150335311889648\n",
      "Test loss without mask: at epoch 16 7.8388971328735355 Test perplexity without mask: 2537.405029296875\n",
      "At epoch 18 batch 1 of num_batches 863 Average batch loss: 7.954495429992676 Perplexity: 2848.350830078125\n",
      "At epoch 18 batch 100 of num_batches 863 Average batch loss: 7.746422619819641 Perplexity: 2313.282470703125\n",
      "At epoch 18 batch 200 of num_batches 863 Average batch loss: 7.73361448764801 Perplexity: 2283.842041015625\n",
      "At epoch 18 batch 300 of num_batches 863 Average batch loss: 7.739931596120199 Perplexity: 2298.315185546875\n",
      "At epoch 18 batch 400 of num_batches 863 Average batch loss: 7.739523402452469 Perplexity: 2297.377197265625\n",
      "At epoch 18 batch 500 of num_batches 863 Average batch loss: 7.739422451019287 Perplexity: 2297.14501953125\n",
      "At epoch 18 batch 600 of num_batches 863 Average batch loss: 7.744256628354391 Perplexity: 2308.276611328125\n",
      "At epoch 18 batch 700 of num_batches 863 Average batch loss: 7.7417119761875695 Perplexity: 2302.410888671875\n",
      "At epoch 18 batch 800 of num_batches 863 Average batch loss: 7.738116493821144 Perplexity: 2294.146728515625\n",
      "At epoch 18 batch 1 of num_batches 89 Average test loss: 8.150086402893066\n",
      "Test loss without mask: at epoch 17 7.831654956605699 Test perplexity without mask: 2519.09521484375\n",
      "At epoch 19 batch 1 of num_batches 863 Average batch loss: 6.974316596984863 Perplexity: 1068.8265380859375\n",
      "At epoch 19 batch 100 of num_batches 863 Average batch loss: 7.698426103591919 Perplexity: 2204.875244140625\n",
      "At epoch 19 batch 200 of num_batches 863 Average batch loss: 7.711119511127472 Perplexity: 2233.041015625\n",
      "At epoch 19 batch 300 of num_batches 863 Average batch loss: 7.712335836092631 Perplexity: 2235.759033203125\n",
      "At epoch 19 batch 400 of num_batches 863 Average batch loss: 7.715037769079208 Perplexity: 2241.8076171875\n",
      "At epoch 19 batch 500 of num_batches 863 Average batch loss: 7.719648169517517 Perplexity: 2252.16748046875\n",
      "At epoch 19 batch 600 of num_batches 863 Average batch loss: 7.718347235520681 Perplexity: 2249.23876953125\n",
      "At epoch 19 batch 700 of num_batches 863 Average batch loss: 7.720458337920053 Perplexity: 2253.992919921875\n",
      "At epoch 19 batch 800 of num_batches 863 Average batch loss: 7.7220142012834545 Perplexity: 2257.502685546875\n",
      "At epoch 19 batch 1 of num_batches 89 Average test loss: 8.127277374267578\n",
      "Test loss without mask: at epoch 18 7.822225353452894 Test perplexity without mask: 2495.453125\n",
      "At epoch 20 batch 1 of num_batches 863 Average batch loss: 7.718987941741943 Perplexity: 2250.6806640625\n",
      "At epoch 20 batch 100 of num_batches 863 Average batch loss: 7.683151125907898 Perplexity: 2171.451904296875\n",
      "At epoch 20 batch 200 of num_batches 863 Average batch loss: 7.6947384190559385 Perplexity: 2196.759033203125\n",
      "At epoch 20 batch 300 of num_batches 863 Average batch loss: 7.702843743960063 Perplexity: 2214.63671875\n",
      "At epoch 20 batch 400 of num_batches 863 Average batch loss: 7.709922053813934 Perplexity: 2230.367919921875\n",
      "At epoch 20 batch 500 of num_batches 863 Average batch loss: 7.709127520561219 Perplexity: 2228.5966796875\n",
      "At epoch 20 batch 600 of num_batches 863 Average batch loss: 7.710279188156128 Perplexity: 2231.16455078125\n",
      "At epoch 20 batch 700 of num_batches 863 Average batch loss: 7.711730774470738 Perplexity: 2234.406494140625\n",
      "At epoch 20 batch 800 of num_batches 863 Average batch loss: 7.707457756996154 Perplexity: 2224.87841796875\n",
      "At epoch 20 batch 1 of num_batches 89 Average test loss: 8.122112274169922\n",
      "Test loss without mask: at epoch 19 7.8153947936164005 Test perplexity without mask: 2478.46533203125\n",
      "At epoch 21 batch 1 of num_batches 863 Average batch loss: 7.667176723480225 Perplexity: 2137.03955078125\n",
      "At epoch 21 batch 100 of num_batches 863 Average batch loss: 7.716840772628784 Perplexity: 2245.85302734375\n",
      "At epoch 21 batch 200 of num_batches 863 Average batch loss: 7.70909245967865 Perplexity: 2228.519287109375\n",
      "At epoch 21 batch 300 of num_batches 863 Average batch loss: 7.7124239428838095 Perplexity: 2235.955322265625\n",
      "At epoch 21 batch 400 of num_batches 863 Average batch loss: 7.707741032838822 Perplexity: 2225.509765625\n",
      "At epoch 21 batch 500 of num_batches 863 Average batch loss: 7.7001059513092045 Perplexity: 2208.58251953125\n",
      "At epoch 21 batch 600 of num_batches 863 Average batch loss: 7.703873184521993 Perplexity: 2216.91796875\n",
      "At epoch 21 batch 700 of num_batches 863 Average batch loss: 7.704485802650452 Perplexity: 2218.276611328125\n",
      "At epoch 21 batch 800 of num_batches 863 Average batch loss: 7.702185159921646 Perplexity: 2213.178955078125\n",
      "At epoch 21 batch 1 of num_batches 89 Average test loss: 8.113953590393066\n",
      "Test loss without mask: at epoch 20 7.806372276941935 Test perplexity without mask: 2456.20361328125\n",
      "At epoch 22 batch 1 of num_batches 863 Average batch loss: 7.879574775695801 Perplexity: 2642.74853515625\n",
      "At epoch 22 batch 100 of num_batches 863 Average batch loss: 7.724240183830261 Perplexity: 2262.532958984375\n",
      "At epoch 22 batch 200 of num_batches 863 Average batch loss: 7.703670542240143 Perplexity: 2216.46875\n",
      "At epoch 22 batch 300 of num_batches 863 Average batch loss: 7.700553560256958 Perplexity: 2209.570556640625\n",
      "At epoch 22 batch 400 of num_batches 863 Average batch loss: 7.700649563074112 Perplexity: 2209.783203125\n",
      "At epoch 22 batch 500 of num_batches 863 Average batch loss: 7.695234181404114 Perplexity: 2197.8486328125\n",
      "At epoch 22 batch 600 of num_batches 863 Average batch loss: 7.6973317400614425 Perplexity: 2202.4638671875\n",
      "At epoch 22 batch 700 of num_batches 863 Average batch loss: 7.691824569702148 Perplexity: 2190.3671875\n",
      "At epoch 22 batch 800 of num_batches 863 Average batch loss: 7.685204966068268 Perplexity: 2175.916015625\n",
      "At epoch 22 batch 1 of num_batches 89 Average test loss: 8.111106872558594\n",
      "Test loss without mask: at epoch 21 7.801803048451742 Test perplexity without mask: 2445.006591796875\n",
      "At epoch 23 batch 1 of num_batches 863 Average batch loss: 7.623687744140625 Perplexity: 2046.09375\n",
      "At epoch 23 batch 100 of num_batches 863 Average batch loss: 7.628674054145813 Perplexity: 2056.321533203125\n",
      "At epoch 23 batch 200 of num_batches 863 Average batch loss: 7.646809039115905 Perplexity: 2093.953369140625\n",
      "At epoch 23 batch 300 of num_batches 863 Average batch loss: 7.6662700017293295 Perplexity: 2135.102294921875\n",
      "At epoch 23 batch 400 of num_batches 863 Average batch loss: 7.659873856306076 Perplexity: 2121.489990234375\n",
      "At epoch 23 batch 500 of num_batches 863 Average batch loss: 7.663276418685913 Perplexity: 2128.72021484375\n",
      "At epoch 23 batch 600 of num_batches 863 Average batch loss: 7.66489484389623 Perplexity: 2132.1689453125\n",
      "At epoch 23 batch 700 of num_batches 863 Average batch loss: 7.676275612967355 Perplexity: 2156.5732421875\n",
      "At epoch 23 batch 800 of num_batches 863 Average batch loss: 7.6761059910058975 Perplexity: 2156.20703125\n",
      "At epoch 23 batch 1 of num_batches 89 Average test loss: 8.106977462768555\n",
      "Test loss without mask: at epoch 22 7.79386207792494 Test perplexity without mask: 2425.667236328125\n",
      "At epoch 24 batch 1 of num_batches 863 Average batch loss: 7.749324321746826 Perplexity: 2320.00439453125\n",
      "At epoch 24 batch 100 of num_batches 863 Average batch loss: 7.637000102996826 Perplexity: 2073.51416015625\n",
      "At epoch 24 batch 200 of num_batches 863 Average batch loss: 7.652902100086212 Perplexity: 2106.750732421875\n",
      "At epoch 24 batch 300 of num_batches 863 Average batch loss: 7.661964705785116 Perplexity: 2125.9306640625\n",
      "At epoch 24 batch 400 of num_batches 863 Average batch loss: 7.663180575370789 Perplexity: 2128.51611328125\n",
      "At epoch 24 batch 500 of num_batches 863 Average batch loss: 7.663994428634644 Perplexity: 2130.249267578125\n",
      "At epoch 24 batch 600 of num_batches 863 Average batch loss: 7.670547022024791 Perplexity: 2144.254150390625\n",
      "At epoch 24 batch 700 of num_batches 863 Average batch loss: 7.664386192730495 Perplexity: 2131.08447265625\n",
      "At epoch 24 batch 800 of num_batches 863 Average batch loss: 7.665879840254783 Perplexity: 2134.26953125\n",
      "At epoch 24 batch 1 of num_batches 89 Average test loss: 8.094313621520996\n",
      "Test loss without mask: at epoch 23 7.787417554855347 Test perplexity without mask: 2410.085205078125\n",
      "At epoch 25 batch 1 of num_batches 863 Average batch loss: 7.78948450088501 Perplexity: 2415.072265625\n",
      "At epoch 25 batch 100 of num_batches 863 Average batch loss: 7.651153283119202 Perplexity: 2103.0693359375\n",
      "At epoch 25 batch 200 of num_batches 863 Average batch loss: 7.63759290933609 Perplexity: 2074.743408203125\n",
      "At epoch 25 batch 300 of num_batches 863 Average batch loss: 7.6651380920410155 Perplexity: 2132.6875\n",
      "At epoch 25 batch 400 of num_batches 863 Average batch loss: 7.653769949674606 Perplexity: 2108.579833984375\n",
      "At epoch 25 batch 500 of num_batches 863 Average batch loss: 7.6566038494110105 Perplexity: 2114.563720703125\n",
      "At epoch 25 batch 600 of num_batches 863 Average batch loss: 7.653705891768138 Perplexity: 2108.445068359375\n",
      "At epoch 25 batch 700 of num_batches 863 Average batch loss: 7.652268633842469 Perplexity: 2105.416015625\n",
      "At epoch 25 batch 800 of num_batches 863 Average batch loss: 7.65316762804985 Perplexity: 2107.310302734375\n",
      "At epoch 25 batch 1 of num_batches 89 Average test loss: 8.09683895111084\n",
      "Test loss without mask: at epoch 24 7.7824704382154675 Test perplexity without mask: 2398.191650390625\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 3252,\n",
      "         1226, 1470, 1865, 1030]], device='cuda:0')\n",
      "torch.Size([1, 100, 10000])\n"
     ]
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"Hello my name is\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = tokenized['attention_mask'].to(config[\"device\"])\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "\n",
    "print(attention_mask)\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "next_token = prediction.argmax(dim=-1)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m#output_text += next_text\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output_text)\n\u001b[0;32m---> 38\u001b[0m generate_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpt2, wrapped_tokenizer, config)\n",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(starting_text, model, tokenizer, config, num_output_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m input_encoding \u001b[38;5;241m=\u001b[39m tokenizer(starting_text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mMAX_SEQ_LEN, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m output_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstarting_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_output_tokens):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=20):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=MAX_SEQ_LEN, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -MAX_SEQ_LEN:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\"The\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.encode(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
