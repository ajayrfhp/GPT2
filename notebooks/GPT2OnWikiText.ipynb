{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 23767\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "# filter out empty lines\n",
    "train_dataset = train_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "val_dataset = val_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "test_dataset = test_dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained on custom dataset with vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "from utils import get_train_tokenizer\n",
    "wrapped_tokenizer = get_train_tokenizer(train_dataset, vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"<|endoftext|>\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3252, 1226, 1470, 1865, 1030, 33, 74, 1068]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer([\"Hello my name is Ajay\"])['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6293bf95554f0ca3813b5d07b99bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'output_ids'],\n",
       "    num_rows: 25902\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from utils import slide_window\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True, fn_kwargs={\"wrapped_tokenizer\": wrapped_tokenizer, \"max_length\": MAX_SEQ_LEN}, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  51,\n",
       "  3521,\n",
       "  182,\n",
       "  1302,\n",
       "  8850,\n",
       "  19,\n",
       "  26,\n",
       "  1309,\n",
       "  2228,\n",
       "  9634,\n",
       "  8,\n",
       "  3332,\n",
       "  26,\n",
       "  809,\n",
       "  757,\n",
       "  616,\n",
       "  692,\n",
       "  636,\n",
       "  688,\n",
       "  647,\n",
       "  684,\n",
       "  687,\n",
       "  637,\n",
       "  19,\n",
       "  12,\n",
       "  5982,\n",
       "  14,\n",
       "  8850,\n",
       "  1027,\n",
       "  1016,\n",
       "  3980,\n",
       "  1800,\n",
       "  19,\n",
       "  9,\n",
       "  12,\n",
       "  5558,\n",
       "  3794,\n",
       "  1032,\n",
       "  1028,\n",
       "  8850,\n",
       "  9634,\n",
       "  3103,\n",
       "  3150,\n",
       "  2420,\n",
       "  12,\n",
       "  1030,\n",
       "  65,\n",
       "  5428,\n",
       "  1210,\n",
       "  2159,\n",
       "  1060,\n",
       "  2835,\n",
       "  2143,\n",
       "  1398,\n",
       "  2509,\n",
       "  1079,\n",
       "  1666,\n",
       "  3643,\n",
       "  1026,\n",
       "  7836,\n",
       "  14,\n",
       "  54,\n",
       "  1810,\n",
       "  1052,\n",
       "  1016,\n",
       "  5250,\n",
       "  2327,\n",
       "  1294,\n",
       "  14,\n",
       "  1401,\n",
       "  1038,\n",
       "  1345,\n",
       "  1013,\n",
       "  2030,\n",
       "  2300,\n",
       "  1013,\n",
       "  2420,\n",
       "  12,\n",
       "  1033,\n",
       "  1030,\n",
       "  1016,\n",
       "  2053,\n",
       "  1398,\n",
       "  1013,\n",
       "  1016,\n",
       "  8850,\n",
       "  1559,\n",
       "  14,\n",
       "  3327,\n",
       "  2449,\n",
       "  1031,\n",
       "  1016,\n",
       "  1743,\n",
       "  8974,\n",
       "  1027,\n",
       "  5428,\n",
       "  1210,\n",
       "  1026,\n",
       "  2112,\n",
       "  1060,\n",
       "  1308,\n",
       "  6903,\n",
       "  1028,\n",
       "  1180,\n",
       "  7018,\n",
       "  1388,\n",
       "  12,\n",
       "  1016,\n",
       "  2066,\n",
       "  3919,\n",
       "  7766,\n",
       "  1032,\n",
       "  1016,\n",
       "  1211,\n",
       "  1398,\n",
       "  1026,\n",
       "  5164,\n",
       "  1016,\n",
       "  2,\n",
       "  8565,\n",
       "  8310,\n",
       "  2,\n",
       "  12,\n",
       "  65,\n",
       "  2355,\n",
       "  1024,\n",
       "  2334,\n",
       "  4260,\n",
       "  5191,\n",
       "  1016,\n",
       "  3478,\n",
       "  1027,\n",
       "  5277,\n",
       "  1215,\n",
       "  1393,\n",
       "  1016,\n",
       "  4074,\n",
       "  1798,\n",
       "  1493,\n",
       "  1015,\n",
       "  1771,\n",
       "  1244,\n",
       "  3630,\n",
       "  4526,\n",
       "  2575,\n",
       "  3104,\n",
       "  1026,\n",
       "  1105,\n",
       "  4052,\n",
       "  1444,\n",
       "  1587,\n",
       "  1016,\n",
       "  8040,\n",
       "  4260,\n",
       "  2,\n",
       "  4001,\n",
       "  1045,\n",
       "  1020,\n",
       "  89,\n",
       "  50,\n",
       "  3605,\n",
       "  2,\n",
       "  14],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'output_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3521,\n",
       "  182,\n",
       "  1302,\n",
       "  8850,\n",
       "  19,\n",
       "  26,\n",
       "  1309,\n",
       "  2228,\n",
       "  9634,\n",
       "  8,\n",
       "  3332,\n",
       "  26,\n",
       "  809,\n",
       "  757,\n",
       "  616,\n",
       "  692,\n",
       "  636,\n",
       "  688,\n",
       "  647,\n",
       "  684,\n",
       "  687,\n",
       "  637,\n",
       "  19,\n",
       "  12,\n",
       "  5982,\n",
       "  14,\n",
       "  8850,\n",
       "  1027,\n",
       "  1016,\n",
       "  3980,\n",
       "  1800,\n",
       "  19,\n",
       "  9,\n",
       "  12,\n",
       "  5558,\n",
       "  3794,\n",
       "  1032,\n",
       "  1028,\n",
       "  8850,\n",
       "  9634,\n",
       "  3103,\n",
       "  3150,\n",
       "  2420,\n",
       "  12,\n",
       "  1030,\n",
       "  65,\n",
       "  5428,\n",
       "  1210,\n",
       "  2159,\n",
       "  1060,\n",
       "  2835,\n",
       "  2143,\n",
       "  1398,\n",
       "  2509,\n",
       "  1079,\n",
       "  1666,\n",
       "  3643,\n",
       "  1026,\n",
       "  7836,\n",
       "  14,\n",
       "  54,\n",
       "  1810,\n",
       "  1052,\n",
       "  1016,\n",
       "  5250,\n",
       "  2327,\n",
       "  1294,\n",
       "  14,\n",
       "  1401,\n",
       "  1038,\n",
       "  1345,\n",
       "  1013,\n",
       "  2030,\n",
       "  2300,\n",
       "  1013,\n",
       "  2420,\n",
       "  12,\n",
       "  1033,\n",
       "  1030,\n",
       "  1016,\n",
       "  2053,\n",
       "  1398,\n",
       "  1013,\n",
       "  1016,\n",
       "  8850,\n",
       "  1559,\n",
       "  14,\n",
       "  3327,\n",
       "  2449,\n",
       "  1031,\n",
       "  1016,\n",
       "  1743,\n",
       "  8974,\n",
       "  1027,\n",
       "  5428,\n",
       "  1210,\n",
       "  1026,\n",
       "  2112,\n",
       "  1060,\n",
       "  1308,\n",
       "  6903,\n",
       "  1028,\n",
       "  1180,\n",
       "  7018,\n",
       "  1388,\n",
       "  12,\n",
       "  1016,\n",
       "  2066,\n",
       "  3919,\n",
       "  7766,\n",
       "  1032,\n",
       "  1016,\n",
       "  1211,\n",
       "  1398,\n",
       "  1026,\n",
       "  5164,\n",
       "  1016,\n",
       "  2,\n",
       "  8565,\n",
       "  8310,\n",
       "  2,\n",
       "  12,\n",
       "  65,\n",
       "  2355,\n",
       "  1024,\n",
       "  2334,\n",
       "  4260,\n",
       "  5191,\n",
       "  1016,\n",
       "  3478,\n",
       "  1027,\n",
       "  5277,\n",
       "  1215,\n",
       "  1393,\n",
       "  1016,\n",
       "  4074,\n",
       "  1798,\n",
       "  1493,\n",
       "  1015,\n",
       "  1771,\n",
       "  1244,\n",
       "  3630,\n",
       "  4526,\n",
       "  2575,\n",
       "  3104,\n",
       "  1026,\n",
       "  1105,\n",
       "  4052,\n",
       "  1444,\n",
       "  1587,\n",
       "  1016,\n",
       "  8040,\n",
       "  4260,\n",
       "  2,\n",
       "  4001,\n",
       "  1045,\n",
       "  1020,\n",
       "  89,\n",
       "  50,\n",
       "  3605,\n",
       "  2,\n",
       "  14,\n",
       "  0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7ff334908050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    output_ids = torch.stack([torch.tensor(item[\"output_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"output_ids\": output_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "batch_size = 30\n",
    "train_torch_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_torch_dataloader = DataLoader(tokenized_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_torch_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "train_torch_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 256]), torch.Size([30, 256]), torch.Size([30, 256]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "batch[\"input_ids\"].shape, batch[\"output_ids\"].shape, batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model loaded to device'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 128,\n",
    "        \"heads\": 4,\n",
    "        \"layers\": 4,\n",
    "        \"vocab_size\": wrapped_tokenizer.vocab_size,\n",
    "        \"context_length\": MAX_SEQ_LEN,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 25,\n",
    "        \"model_path\": \"../model_files/gpt2.pth\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-4,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "\"model loaded to device\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 863 Average batch loss: 9.315194129943848 Perplexity: 11105.4814453125\n",
      "At epoch 1 batch 100 of num_batches 863 Average batch loss: 8.905252351760865 Perplexity: 7370.5859375\n",
      "At epoch 1 batch 200 of num_batches 863 Average batch loss: 8.685583033561706 Perplexity: 5916.98974609375\n",
      "At epoch 1 batch 300 of num_batches 863 Average batch loss: 8.589148400624593 Perplexity: 5373.03662109375\n",
      "At epoch 1 batch 400 of num_batches 863 Average batch loss: 8.535176451206206 Perplexity: 5090.72900390625\n",
      "At epoch 1 batch 500 of num_batches 863 Average batch loss: 8.497671630859376 Perplexity: 4903.3408203125\n",
      "At epoch 1 batch 600 of num_batches 863 Average batch loss: 8.466893697579701 Perplexity: 4754.72509765625\n",
      "At epoch 1 batch 700 of num_batches 863 Average batch loss: 8.445474405288696 Perplexity: 4653.9638671875\n",
      "At epoch 1 batch 800 of num_batches 863 Average batch loss: 8.42371909558773 Perplexity: 4553.8095703125\n",
      "At epoch 1 batch 1 of num_batches 89 Average test loss: 8.507019996643066\n",
      "Test loss without mask: at epoch 0 8.264940139982436 Test perplexity without mask: 3885.24072265625\n",
      "At epoch 2 batch 1 of num_batches 863 Average batch loss: 8.09067153930664 Perplexity: 3263.878662109375\n",
      "At epoch 2 batch 100 of num_batches 863 Average batch loss: 8.260759406089782 Perplexity: 3869.031005859375\n",
      "At epoch 2 batch 200 of num_batches 863 Average batch loss: 8.265171341896057 Perplexity: 3886.1376953125\n",
      "At epoch 2 batch 300 of num_batches 863 Average batch loss: 8.261617150306702 Perplexity: 3872.349365234375\n",
      "At epoch 2 batch 400 of num_batches 863 Average batch loss: 8.252654719352723 Perplexity: 3837.8017578125\n",
      "At epoch 2 batch 500 of num_batches 863 Average batch loss: 8.250316476821899 Perplexity: 3828.837890625\n",
      "At epoch 2 batch 600 of num_batches 863 Average batch loss: 8.243014084498087 Perplexity: 3800.98046875\n",
      "At epoch 2 batch 700 of num_batches 863 Average batch loss: 8.237891742161342 Perplexity: 3781.56103515625\n",
      "At epoch 2 batch 800 of num_batches 863 Average batch loss: 8.233673495650292 Perplexity: 3765.639892578125\n",
      "At epoch 2 batch 1 of num_batches 89 Average test loss: 8.44113826751709\n",
      "Test loss without mask: at epoch 1 8.184925397237143 Test perplexity without mask: 3586.474853515625\n",
      "At epoch 3 batch 1 of num_batches 863 Average batch loss: 8.441605567932129 Perplexity: 4635.9921875\n",
      "At epoch 3 batch 100 of num_batches 863 Average batch loss: 8.177526240348817 Perplexity: 3560.037841796875\n",
      "At epoch 3 batch 200 of num_batches 863 Average batch loss: 8.174766418933869 Perplexity: 3550.22607421875\n",
      "At epoch 3 batch 300 of num_batches 863 Average batch loss: 8.17343126296997 Perplexity: 3545.489013671875\n",
      "At epoch 3 batch 400 of num_batches 863 Average batch loss: 8.176015474796294 Perplexity: 3554.6640625\n",
      "At epoch 3 batch 500 of num_batches 863 Average batch loss: 8.172065371513368 Perplexity: 3540.650390625\n",
      "At epoch 3 batch 600 of num_batches 863 Average batch loss: 8.171602991422018 Perplexity: 3539.01318359375\n",
      "At epoch 3 batch 700 of num_batches 863 Average batch loss: 8.170179786000933 Perplexity: 3533.977783203125\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wrapped_tokenizer(\"Hello my name is\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = tokenized['attention_mask'].to(config[\"device\"])\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "\n",
    "print(attention_mask)\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "next_token = prediction.argmax(dim=-1)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=100):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=1024, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -1024:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\"The capital is\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.encode(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
