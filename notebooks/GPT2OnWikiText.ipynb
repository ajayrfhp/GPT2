{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read GPT2 tokenizer with padding side left, truncation and max length 1024. Pad token is end of text token.\n",
    "from transformers import GPT2Tokenizer\n",
    "wrapped_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "wrapped_tokenizer.pad_token = wrapped_tokenizer.eos_token\n",
    "wrapped_tokenizer.padding_side = \"left\"\n",
    "wrapped_tokenizer.truncation = True\n",
    "wrapped_tokenizer.max_length = 1024\n",
    "\n",
    "wrapped_tokenizer.encode(\"Hello, my dog is cute\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer(\"<|endoftext|>\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer([\"Hello my name is Ajay\"])['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def slide_window(text_batch, max_length=1024):\n",
    "    text_batch['input_words'] = []\n",
    "    text_batch['output_words'] = []\n",
    "    text_batch['input_ids_raw'] = []\n",
    "    text_batch['output_ids_raw'] = []\n",
    "\n",
    "    text_batch['input_ids'] = []\n",
    "    text_batch['output_ids'] = []\n",
    "    text_batch['attention_mask'] = []\n",
    "\n",
    "    for text in text_batch['text']:\n",
    "        \n",
    "        tokens = wrapped_tokenizer.tokenize(text)\n",
    "        tokens.append(wrapped_tokenizer.eos_token)  # add eos token to the end of the tokens\n",
    "        input_tokens = tokens[:-1]\n",
    "        output_tokens = tokens[1:]\n",
    "\n",
    "        \n",
    "        text_batch['input_words'].append(input_tokens)\n",
    "        text_batch['output_words'].append(output_tokens)\n",
    "\n",
    "        input_ids_raw = wrapped_tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        output_ids_raw = wrapped_tokenizer.convert_tokens_to_ids(output_tokens)\n",
    "\n",
    "        text_batch['input_ids_raw'].append(input_ids_raw)\n",
    "        text_batch['output_ids_raw'].append(output_ids_raw)\n",
    "\n",
    "        # pad, truncate, and convert to tensor\n",
    "        input_ids = wrapped_tokenizer.pad({\"input_ids\": input_ids_raw}, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        output_ids = wrapped_tokenizer.pad({\"input_ids\": output_ids_raw}, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        attention_mask = [0] * (max_length - len(input_ids_raw)) + [1] * len(input_ids_raw)\n",
    "\n",
    "\n",
    "        assert len(attention_mask) == max_length\n",
    "        text_batch['input_ids'].append(input_ids)\n",
    "        text_batch['output_ids'].append(output_ids)\n",
    "        text_batch['attention_mask'].append(attention_mask)\n",
    "    \n",
    "\n",
    "    return text_batch \n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(slide_window, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(slide_window, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(slide_window, batched=True)\n",
    "\n",
    "\n",
    "# filter to remove samples where length of input_ids is 0\n",
    "tokenized_train_dataset = tokenized_train_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "tokenized_val_dataset = tokenized_val_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "tokenized_test_dataset = tokenized_test_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids'], item['attention_mask']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    attention_mask = [item[2] for item in batch]\n",
    "\n",
    "    \n",
    "\n",
    "    # set dtype to long\n",
    "\n",
    "    input_ids_list = torch.tensor(input_ids, dtype=torch.long)\n",
    "    output_ids_list = torch.tensor(output_ids, dtype=torch.long)\n",
    "    attention_mask_list = torch.tensor(attention_mask, dtype=torch.long)\n",
    "    return input_ids_list, output_ids_list, attention_mask_list\n",
    "\n",
    "batch_size = 2\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids, attention_mask = batch\n",
    "input_ids.shape, output_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 768,\n",
    "        \"heads\": 12,\n",
    "        \"layers\": 12,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 1,\n",
    "        \"model_path\": \"../model_files/gpt2.pth\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-4,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config)\n",
    "torch.save(gpt2.state_dict(), config[\"model_path\"]) # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GPT2 from config.model_path\n",
    "import os \n",
    "\n",
    "if os.path.exists(config['model_path']):\n",
    "    gpt2.load_state_dict(torch.load(config['model_path']))\n",
    "    print(\"model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = wrapped_tokenizer(\"Hello my name is\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = tokenized['attention_mask'].to(config[\"device\"])\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "\n",
    "print(attention_mask)\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "next_token = prediction.argmax(dim=-1)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config):\n",
    "    input_encoding = tokenizer(starting_text, return_tensors=\"pt\")\n",
    "    device = config[\"device\"]\n",
    "    output_text = \"\"\n",
    "    output_tokens = min(100, tokenizer.model_max_length, len(input_encoding['input_ids'][0]))\n",
    "    for i in range(output_tokens):\n",
    "        text = starting_text + output_text\n",
    "        input_encoding = tokenizer(text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = input_encoding['input_ids'].to(device)\n",
    "        input_attention_mask = input_encoding['attention_mask'].to(device)[0]\n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        \n",
    "\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "        # next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        # next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "        print(next_token)\n",
    "        next_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        output_text += next_text\n",
    "    print(f\"{starting_text} -> {output_text}\")\n",
    "\n",
    "generate_text(\"The capital of United States of America\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.encode(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
