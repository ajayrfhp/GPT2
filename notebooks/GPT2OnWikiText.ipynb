{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "val_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['H', 'ello', 'Ġmy', 'Ġname', 'Ġis', 'ĠAj', 'ay']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "trainer = tokenizers.trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\", \"<pad>\"])\n",
    "tokenizer.train_from_iterator(train_dataset[\"text\"], trainer=trainer)\n",
    "tokenizer.post_processor = tokenizers.processors.ByteLevel(trim_offsets=False)\n",
    "\n",
    "tokenizer.save(\"../data/tokenizer.json\")\n",
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "\n",
    "wrapped_tokenizer = transformers.PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    padding_side=\"left\",\n",
    "    pad_token=\"<pad>\",\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.encode(\"Hello my name is Ajay\").tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|endoftext|>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 14980, 1669, 1222, 302, 18604, 289]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"Hello my name is Ajay\")['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'output_ids'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    inpt_text = examples['text']\n",
    "    examples['input_ids'] = wrapped_tokenizer(inpt_text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")['input_ids']\n",
    "    \n",
    "    \n",
    "\n",
    "    return examples\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n',\n",
       " 'input_ids': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  239,\n",
       "  8577,\n",
       "  9442,\n",
       "  2988,\n",
       "  239,\n",
       "  160],\n",
       " 'output_ids': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  30,\n",
       "  8577,\n",
       "  9442,\n",
       "  2988,\n",
       "  239,\n",
       "  160]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9084675340>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    input_ids_list = torch.tensor(input_ids)\n",
    "    output_ids_list = torch.tensor(output_ids)\n",
    "    return input_ids_list, output_ids_list\n",
    "\n",
    "batch_size = 200\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 100]), torch.Size([200, 100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids = batch\n",
    "input_ids.shape, output_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (position_embedding): Embedding(128, 768)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 768,\n",
    "        \"heads\": 12,\n",
    "        \"layers\": 12,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 128,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 5,\n",
    "        \"model_path\": \"../model_files/gpt2.pth\",\n",
    "        \"num_train_batches\" : num_train_batches\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 183Average batch loss: 11.264799118041992\n",
      "At epoch 1 batch 10 of num_batches 183Average batch loss: 8.239819622039795\n",
      "At epoch 1 batch 20 of num_batches 183Average batch loss: 3.9928173303604124\n",
      "At epoch 1 batch 30 of num_batches 183Average batch loss: 2.4394912242889406\n",
      "At epoch 1 batch 40 of num_batches 183Average batch loss: 1.7395467281341552\n",
      "At epoch 1 batch 50 of num_batches 183Average batch loss: 1.3633330726623536\n",
      "At epoch 1 batch 60 of num_batches 183Average batch loss: 1.0980746189753214\n",
      "At epoch 1 batch 70 of num_batches 183Average batch loss: 0.9173863819667272\n",
      "At epoch 1 batch 80 of num_batches 183Average batch loss: 0.7894062042236328\n",
      "At epoch 1 batch 90 of num_batches 183Average batch loss: 0.6895569960276285\n",
      "At epoch 1 batch 100 of num_batches 183Average batch loss: 0.607052526473999\n",
      "At epoch 1 batch 110 of num_batches 183Average batch loss: 0.5380614237351851\n",
      "At epoch 1 batch 120 of num_batches 183Average batch loss: 0.46701579093933104\n",
      "At epoch 1 batch 130 of num_batches 183Average batch loss: 0.4179812211256761\n",
      "At epoch 1 batch 140 of num_batches 183Average batch loss: 0.3830326761518206\n",
      "At epoch 1 batch 150 of num_batches 183Average batch loss: 0.35216325759887696\n",
      "At epoch 1 batch 160 of num_batches 183Average batch loss: 0.31835442781448364\n",
      "At epoch 1 batch 170 of num_batches 183Average batch loss: 0.28455873938167797\n",
      "At epoch 1 batch 180 of num_batches 183Average batch loss: 0.274271117316352\n",
      "Epoch 1/5, Loss with mask: 4.267577648162842 Perplexity with mask: 71.34859466552734\n",
      "Test loss without mask: at epoch 0 4.773046368046811 Test perplexity without mask: 118.2790298461914\n",
      "At epoch 2 batch 1 of num_batches 183Average batch loss: 4.489568710327148\n",
      "At epoch 2 batch 10 of num_batches 183Average batch loss: 3.955264949798584\n",
      "At epoch 2 batch 20 of num_batches 183Average batch loss: 2.2409440517425536\n",
      "At epoch 2 batch 30 of num_batches 183Average batch loss: 1.467979621887207\n",
      "At epoch 2 batch 40 of num_batches 183Average batch loss: 1.0706830620765686\n",
      "At epoch 2 batch 50 of num_batches 183Average batch loss: 0.8197184514999389\n",
      "At epoch 2 batch 60 of num_batches 183Average batch loss: 0.6727384646733602\n",
      "At epoch 2 batch 70 of num_batches 183Average batch loss: 0.5592375823429653\n",
      "At epoch 2 batch 80 of num_batches 183Average batch loss: 0.48188901245594024\n",
      "At epoch 2 batch 90 of num_batches 183Average batch loss: 0.41594196955362955\n",
      "At epoch 2 batch 100 of num_batches 183Average batch loss: 0.3730387425422668\n",
      "At epoch 2 batch 110 of num_batches 183Average batch loss: 0.31335142742503774\n",
      "At epoch 2 batch 120 of num_batches 183Average batch loss: 0.29310343265533445\n",
      "At epoch 2 batch 130 of num_batches 183Average batch loss: 0.24851170686575083\n",
      "At epoch 2 batch 140 of num_batches 183Average batch loss: 0.22644939763205393\n",
      "At epoch 2 batch 150 of num_batches 183Average batch loss: 0.20579639911651612\n",
      "At epoch 2 batch 160 of num_batches 183Average batch loss: 0.18330041021108628\n",
      "At epoch 2 batch 170 of num_batches 183Average batch loss: 0.15961707620059742\n",
      "At epoch 2 batch 180 of num_batches 183Average batch loss: 0.14244109789530437\n",
      "Epoch 2/5, Loss with mask: 2.408198356628418 Perplexity with mask: 11.113920211791992\n",
      "Test loss without mask: at epoch 1 2.5912086461719714 Test perplexity without mask: 13.345892906188965\n",
      "At epoch 3 batch 1 of num_batches 183Average batch loss: 2.175358295440674\n",
      "At epoch 3 batch 10 of num_batches 183Average batch loss: 1.9519279241561889\n",
      "At epoch 3 batch 20 of num_batches 183Average batch loss: 1.0238485276699065\n",
      "At epoch 3 batch 30 of num_batches 183Average batch loss: 0.6530832846959432\n",
      "At epoch 3 batch 40 of num_batches 183Average batch loss: 0.4712179183959961\n",
      "At epoch 3 batch 50 of num_batches 183Average batch loss: 0.3453711628913879\n",
      "At epoch 3 batch 60 of num_batches 183Average batch loss: 0.30190741618474326\n",
      "At epoch 3 batch 70 of num_batches 183Average batch loss: 0.22272664308547974\n",
      "At epoch 3 batch 80 of num_batches 183Average batch loss: 0.1996426224708557\n",
      "At epoch 3 batch 90 of num_batches 183Average batch loss: 0.17417998711268107\n",
      "At epoch 3 batch 100 of num_batches 183Average batch loss: 0.13864225387573242\n",
      "At epoch 3 batch 110 of num_batches 183Average batch loss: 0.122953416000713\n",
      "At epoch 3 batch 120 of num_batches 183Average batch loss: 0.11560594936211904\n",
      "At epoch 3 batch 130 of num_batches 183Average batch loss: 0.09379003323041475\n",
      "At epoch 3 batch 140 of num_batches 183Average batch loss: 0.08484524360724859\n",
      "At epoch 3 batch 150 of num_batches 183Average batch loss: 0.08201583464940389\n",
      "At epoch 3 batch 160 of num_batches 183Average batch loss: 0.06763638332486152\n",
      "At epoch 3 batch 170 of num_batches 183Average batch loss: 0.05982537620207843\n",
      "At epoch 3 batch 180 of num_batches 183Average batch loss: 0.06472989254527622\n",
      "Epoch 3/5, Loss with mask: 0.9876437187194824 Perplexity with mask: 2.6849007606506348\n",
      "Test loss without mask: at epoch 2 1.2621973502008539 Test perplexity without mask: 3.5331766605377197\n",
      "At epoch 4 batch 1 of num_batches 183Average batch loss: 0.8394757509231567\n",
      "At epoch 4 batch 10 of num_batches 183Average batch loss: 0.6787500262260437\n",
      "At epoch 4 batch 20 of num_batches 183Average batch loss: 0.3825671344995499\n",
      "At epoch 4 batch 30 of num_batches 183Average batch loss: 0.221466459830602\n",
      "At epoch 4 batch 40 of num_batches 183Average batch loss: 0.1883156806230545\n",
      "At epoch 4 batch 50 of num_batches 183Average batch loss: 0.13131125152111053\n",
      "At epoch 4 batch 60 of num_batches 183Average batch loss: 0.11750048448642095\n",
      "At epoch 4 batch 70 of num_batches 183Average batch loss: 0.09549537258488791\n",
      "At epoch 4 batch 80 of num_batches 183Average batch loss: 0.08463702723383904\n",
      "At epoch 4 batch 90 of num_batches 183Average batch loss: 0.07244484159681532\n",
      "At epoch 4 batch 100 of num_batches 183Average batch loss: 0.05747551500797272\n",
      "At epoch 4 batch 110 of num_batches 183Average batch loss: 0.057150708003477615\n",
      "At epoch 4 batch 120 of num_batches 183Average batch loss: 0.04855535750587781\n",
      "At epoch 4 batch 130 of num_batches 183Average batch loss: 0.03867924396808331\n",
      "At epoch 4 batch 140 of num_batches 183Average batch loss: 0.04035995411021369\n",
      "At epoch 4 batch 150 of num_batches 183Average batch loss: 0.0384098881483078\n",
      "At epoch 4 batch 160 of num_batches 183Average batch loss: 0.038506519980728626\n",
      "At epoch 4 batch 170 of num_batches 183Average batch loss: 0.027675743664012237\n",
      "At epoch 4 batch 180 of num_batches 183Average batch loss: 0.029510923557811312\n",
      "Epoch 4/5, Loss with mask: 0.7095112204551697 Perplexity with mask: 2.0329971313476562\n",
      "Test loss without mask: at epoch 3 0.9163358086033871 Test perplexity without mask: 2.500112771987915\n",
      "At epoch 5 batch 1 of num_batches 183Average batch loss: 0.3334057033061981\n",
      "At epoch 5 batch 10 of num_batches 183Average batch loss: 0.33565541803836824\n",
      "At epoch 5 batch 20 of num_batches 183Average batch loss: 0.19430743157863617\n",
      "At epoch 5 batch 30 of num_batches 183Average batch loss: 0.13222486873467762\n",
      "At epoch 5 batch 40 of num_batches 183Average batch loss: 0.07979712784290313\n",
      "At epoch 5 batch 50 of num_batches 183Average batch loss: 0.07388080537319183\n",
      "At epoch 5 batch 60 of num_batches 183Average batch loss: 0.0600197970867157\n",
      "At epoch 5 batch 70 of num_batches 183Average batch loss: 0.04814739397593907\n",
      "At epoch 5 batch 80 of num_batches 183Average batch loss: 0.048879124969244\n",
      "At epoch 5 batch 90 of num_batches 183Average batch loss: 0.033457460006078084\n",
      "At epoch 5 batch 100 of num_batches 183Average batch loss: 0.03637279555201531\n",
      "At epoch 5 batch 110 of num_batches 183Average batch loss: 0.030824858221140776\n",
      "At epoch 5 batch 120 of num_batches 183Average batch loss: 0.026787084341049195\n",
      "At epoch 5 batch 130 of num_batches 183Average batch loss: 0.02597185247219526\n",
      "At epoch 5 batch 140 of num_batches 183Average batch loss: 0.02469756475516728\n",
      "At epoch 5 batch 150 of num_batches 183Average batch loss: 0.021420252720514933\n",
      "At epoch 5 batch 160 of num_batches 183Average batch loss: 0.023211142234504222\n",
      "At epoch 5 batch 170 of num_batches 183Average batch loss: 0.018850141588379356\n",
      "At epoch 5 batch 180 of num_batches 183Average batch loss: 0.013522878620359632\n",
      "Epoch 5/5, Loss with mask: 0.29096871614456177 Perplexity with mask: 1.337722659111023\n",
      "Test loss without mask: at epoch 4 0.8033808077636518 Test perplexity without mask: 2.2330777645111084\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config)\n",
    "torch.save(gpt2.state_dict(), config[\"model_path\"]) # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# load GPT2 from config.model_path\n",
    "import os \n",
    "\n",
    "if os.path.exists(config['model_path']):\n",
    "    gpt2.load_state_dict(torch.load(config['model_path']))\n",
    "    print(\"model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_call_one',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_pad',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_set_model_specific_special_tokens',\n",
       " '_set_processor_class',\n",
       " '_special_tokens_map',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_upload_modified_files',\n",
       " 'add_prefix_space',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'extra_special_tokens',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_chat_template',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,    41, 14980,  1669,  1222,   302]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 100, 50257])\n"
     ]
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"Hello my name is\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = tokenized['attention_mask'].to(config[\"device\"])\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "\n",
    "print(attention_mask)\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "next_token = prediction.argmax(dim=-1)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 50257])\n",
      "tensor([1855], device='cuda:0')\n",
      "The capital of United States of America ->  America America America America America America America America\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config):\n",
    "    input_encoding = tokenizer(starting_text, return_tensors=\"pt\")\n",
    "    device = config[\"device\"]\n",
    "    output_text = \"\"\n",
    "    output_tokens = min(100, tokenizer.model_max_length, len(input_encoding['input_ids'][0]))\n",
    "    for i in range(output_tokens):\n",
    "        text = starting_text + output_text\n",
    "        input_encoding = tokenizer(text, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = input_encoding['input_ids'].to(device)\n",
    "        input_attention_mask = input_encoding['attention_mask'].to(device)[0]\n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        \n",
    "\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "        # next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        # next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "        print(next_token)\n",
    "        next_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        output_text += next_text\n",
    "    print(f\"{starting_text} -> {output_text}\")\n",
    "\n",
    "generate_text(\"The capital of United States of America\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 11624, 302, 199, 3090, 219, 199, 914, 1213, 32]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.encode(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
