{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Train GPT2 on wiki text\n",
    "\n",
    "## Steps\n",
    "- Read, download data\n",
    "- Train tokenizer\n",
    "- Prepare sliding window data loader\n",
    "- Use GPT2 model\n",
    "- Use train/test loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1801350\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"wikitext\"\n",
    "config_name = \"wikitext-103-raw-v1\"\n",
    "\n",
    "# Load the train and test splits into a DatasetDict\n",
    "train_dataset = load_dataset(dataset_path, config_name, split='train')\n",
    "val_dataset = load_dataset(dataset_path, config_name, split='validation')\n",
    "test_dataset = load_dataset(dataset_path, config_name, split='test')\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 616, 3290, 318, 13779]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read GPT2 tokenizer with padding side left, truncation and max length 1024. Pad token is end of text token.\n",
    "from transformers import GPT2Tokenizer\n",
    "wrapped_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "wrapped_tokenizer.pad_token = wrapped_tokenizer.eos_token\n",
    "wrapped_tokenizer.padding_side = \"left\"\n",
    "wrapped_tokenizer.truncation = True\n",
    "wrapped_tokenizer.max_length = 1024\n",
    "\n",
    "wrapped_tokenizer.encode(\"Hello, my dog is cute\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"<|endoftext|>\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15496, 616, 1438, 318, 22028, 323]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer([\"Hello my name is Ajay\"])['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sliding window data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   3%|▎         | 53000/1801350 [01:17<40:18, 722.92 examples/s]"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def slide_window(text_batch, max_length=1024):\n",
    "    \"\"\"\n",
    "    More efficient version of slide_window that leverages batch processing.\n",
    "\n",
    "    Args:\n",
    "        text_batch (dict): A dictionary likely containing a 'text' key with a list of strings.\n",
    "        max_length (int): The maximum sequence length for padding and truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'input_ids', 'attention_mask', and 'output_ids' tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1 & 2: Tokenize, Add EOS, Create Shifted Inputs/Outputs (Raw) ---\n",
    "    \n",
    "    all_tokens = []\n",
    "    input_ids_raw = []\n",
    "    output_ids_raw = []\n",
    "    \n",
    "    eos_token_id = wrapped_tokenizer.convert_tokens_to_ids(wrapped_tokenizer.eos_token)\n",
    "\n",
    "    # Tokenize texts and prepare raw ID lists\n",
    "    # Using a loop here is often necessary for the custom EOS + shift logic, \n",
    "    # but the expensive padding step will be batched later.\n",
    "    for text in text_batch['text']:\n",
    "        tokens = wrapped_tokenizer.tokenize(text)\n",
    "        token_ids = wrapped_tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids.append(eos_token_id) # Add EOS ID\n",
    "\n",
    "        # Create input/output pairs (before padding/truncation)\n",
    "        current_input_ids = token_ids[:-1]\n",
    "        current_output_ids = token_ids[1:]\n",
    "        \n",
    "        input_ids_raw.append(current_input_ids)\n",
    "        output_ids_raw.append(current_output_ids)\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    # --- Step 3: Batch Pad Inputs and Generate Attention Mask ---\n",
    "    \n",
    "    # Let the tokenizer handle padding, truncation (via max_length), \n",
    "    # attention mask creation, and tensor conversion for the whole batch.\n",
    "    padded_inputs = wrapped_tokenizer.pad(\n",
    "        {\"input_ids\": input_ids_raw},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True, # Ask the tokenizer to create the mask\n",
    "    )\n",
    "\n",
    "    # --- Step 4: Batch Pad Outputs ---\n",
    "    \n",
    "    # Pad the output sequences. Usually, no attention mask is needed for labels.\n",
    "    padded_outputs = wrapped_tokenizer.pad(\n",
    "        {\"input_ids\": output_ids_raw},\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Return the final batch ---\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": padded_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": padded_inputs[\"attention_mask\"],\n",
    "        \"output_ids\": padded_outputs[\"input_ids\"],\n",
    "        # Optional: Keep raw words/ids if needed for debugging, but remove for efficiency\n",
    "        'input_words': [tokens[:-1] for tokens in all_tokens], # Reconstruct if needed\n",
    "        'output_words': [tokens[1:] for tokens in all_tokens], # Reconstruct if needed\n",
    "        'input_ids_raw': input_ids_raw, # Keep if needed\n",
    "        'output_ids_raw': output_ids_raw, # Keep if needed\n",
    "    }\n",
    "\n",
    "\n",
    "# filter to remove samples where length of input_ids is 0\n",
    "tokenized_train_dataset = tokenized_train_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "tokenized_val_dataset = tokenized_val_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "tokenized_test_dataset = tokenized_test_dataset.filter(lambda x: len(x['input_ids']) > 0)\n",
    "\n",
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       " 'input_words': ['ĠSen',\n",
       "  'j',\n",
       "  'Åį',\n",
       "  'Ġno',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'Ġ3',\n",
       "  'Ġ:',\n",
       "  'ĠUn',\n",
       "  'recorded',\n",
       "  'ĠChronicles',\n",
       "  'Ġ(',\n",
       "  'ĠJapanese',\n",
       "  'Ġ:',\n",
       "  'Ġæ',\n",
       "  'Ī',\n",
       "  '¦',\n",
       "  'å',\n",
       "  'ł',\n",
       "  '´',\n",
       "  'ãģ®',\n",
       "  'ãĥ´ãĤ¡',\n",
       "  'ãĥ«',\n",
       "  'ãĤŃ',\n",
       "  'ãĥ¥',\n",
       "  'ãĥª',\n",
       "  'ãĤ¢',\n",
       "  '3',\n",
       "  'Ġ,',\n",
       "  'Ġlit',\n",
       "  'Ġ.',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'Ġof',\n",
       "  'Ġthe',\n",
       "  'ĠBattlefield',\n",
       "  'Ġ3',\n",
       "  'Ġ)',\n",
       "  'Ġ,',\n",
       "  'Ġcommonly',\n",
       "  'Ġreferred',\n",
       "  'Ġto',\n",
       "  'Ġas',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'ĠChronicles',\n",
       "  'ĠIII',\n",
       "  'Ġoutside',\n",
       "  'ĠJapan',\n",
       "  'Ġ,',\n",
       "  'Ġis',\n",
       "  'Ġa',\n",
       "  'Ġtactical',\n",
       "  'Ġrole',\n",
       "  'Ġ@',\n",
       "  '-',\n",
       "  '@',\n",
       "  'Ġplaying',\n",
       "  'Ġvideo',\n",
       "  'Ġgame',\n",
       "  'Ġdeveloped',\n",
       "  'Ġby',\n",
       "  'ĠSega',\n",
       "  'Ġand',\n",
       "  'ĠMedia',\n",
       "  '.',\n",
       "  'Vision',\n",
       "  'Ġfor',\n",
       "  'Ġthe',\n",
       "  'ĠPlayStation',\n",
       "  'ĠPortable',\n",
       "  'Ġ.',\n",
       "  'ĠReleased',\n",
       "  'Ġin',\n",
       "  'ĠJanuary',\n",
       "  'Ġ2011',\n",
       "  'Ġin',\n",
       "  'ĠJapan',\n",
       "  'Ġ,',\n",
       "  'Ġit',\n",
       "  'Ġis',\n",
       "  'Ġthe',\n",
       "  'Ġthird',\n",
       "  'Ġgame',\n",
       "  'Ġin',\n",
       "  'Ġthe',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'Ġseries',\n",
       "  'Ġ.',\n",
       "  'ĠEmploy',\n",
       "  'ing',\n",
       "  'Ġthe',\n",
       "  'Ġsame',\n",
       "  'Ġfusion',\n",
       "  'Ġof',\n",
       "  'Ġtactical',\n",
       "  'Ġand',\n",
       "  'Ġreal',\n",
       "  'Ġ@',\n",
       "  '-',\n",
       "  '@',\n",
       "  'Ġtime',\n",
       "  'Ġgameplay',\n",
       "  'Ġas',\n",
       "  'Ġits',\n",
       "  'Ġpredecessors',\n",
       "  'Ġ,',\n",
       "  'Ġthe',\n",
       "  'Ġstory',\n",
       "  'Ġruns',\n",
       "  'Ġparallel',\n",
       "  'Ġto',\n",
       "  'Ġthe',\n",
       "  'Ġfirst',\n",
       "  'Ġgame',\n",
       "  'Ġand',\n",
       "  'Ġfollows',\n",
       "  'Ġthe',\n",
       "  'Ġ\"',\n",
       "  'ĠNam',\n",
       "  'eless',\n",
       "  'Ġ\"',\n",
       "  'Ġ,',\n",
       "  'Ġa',\n",
       "  'Ġpenal',\n",
       "  'Ġmilitary',\n",
       "  'Ġunit',\n",
       "  'Ġserving',\n",
       "  'Ġthe',\n",
       "  'Ġnation',\n",
       "  'Ġof',\n",
       "  'ĠGall',\n",
       "  'ia',\n",
       "  'Ġduring',\n",
       "  'Ġthe',\n",
       "  'ĠSecond',\n",
       "  'ĠEuro',\n",
       "  'pan',\n",
       "  'ĠWar',\n",
       "  'Ġwho',\n",
       "  'Ġperform',\n",
       "  'Ġsecret',\n",
       "  'Ġblack',\n",
       "  'Ġoperations',\n",
       "  'Ġand',\n",
       "  'Ġare',\n",
       "  'Ġpitted',\n",
       "  'Ġagainst',\n",
       "  'Ġthe',\n",
       "  'ĠImperial',\n",
       "  'Ġunit',\n",
       "  'Ġ\"',\n",
       "  'ĠCal',\n",
       "  'am',\n",
       "  'at',\n",
       "  'y',\n",
       "  'ĠRaven',\n",
       "  'Ġ\"',\n",
       "  'Ġ.',\n",
       "  'Ġ',\n",
       "  'Ċ'],\n",
       " 'output_words': ['j',\n",
       "  'Åį',\n",
       "  'Ġno',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'Ġ3',\n",
       "  'Ġ:',\n",
       "  'ĠUn',\n",
       "  'recorded',\n",
       "  'ĠChronicles',\n",
       "  'Ġ(',\n",
       "  'ĠJapanese',\n",
       "  'Ġ:',\n",
       "  'Ġæ',\n",
       "  'Ī',\n",
       "  '¦',\n",
       "  'å',\n",
       "  'ł',\n",
       "  '´',\n",
       "  'ãģ®',\n",
       "  'ãĥ´ãĤ¡',\n",
       "  'ãĥ«',\n",
       "  'ãĤŃ',\n",
       "  'ãĥ¥',\n",
       "  'ãĥª',\n",
       "  'ãĤ¢',\n",
       "  '3',\n",
       "  'Ġ,',\n",
       "  'Ġlit',\n",
       "  'Ġ.',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'Ġof',\n",
       "  'Ġthe',\n",
       "  'ĠBattlefield',\n",
       "  'Ġ3',\n",
       "  'Ġ)',\n",
       "  'Ġ,',\n",
       "  'Ġcommonly',\n",
       "  'Ġreferred',\n",
       "  'Ġto',\n",
       "  'Ġas',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'ĠChronicles',\n",
       "  'ĠIII',\n",
       "  'Ġoutside',\n",
       "  'ĠJapan',\n",
       "  'Ġ,',\n",
       "  'Ġis',\n",
       "  'Ġa',\n",
       "  'Ġtactical',\n",
       "  'Ġrole',\n",
       "  'Ġ@',\n",
       "  '-',\n",
       "  '@',\n",
       "  'Ġplaying',\n",
       "  'Ġvideo',\n",
       "  'Ġgame',\n",
       "  'Ġdeveloped',\n",
       "  'Ġby',\n",
       "  'ĠSega',\n",
       "  'Ġand',\n",
       "  'ĠMedia',\n",
       "  '.',\n",
       "  'Vision',\n",
       "  'Ġfor',\n",
       "  'Ġthe',\n",
       "  'ĠPlayStation',\n",
       "  'ĠPortable',\n",
       "  'Ġ.',\n",
       "  'ĠReleased',\n",
       "  'Ġin',\n",
       "  'ĠJanuary',\n",
       "  'Ġ2011',\n",
       "  'Ġin',\n",
       "  'ĠJapan',\n",
       "  'Ġ,',\n",
       "  'Ġit',\n",
       "  'Ġis',\n",
       "  'Ġthe',\n",
       "  'Ġthird',\n",
       "  'Ġgame',\n",
       "  'Ġin',\n",
       "  'Ġthe',\n",
       "  'ĠV',\n",
       "  'alky',\n",
       "  'ria',\n",
       "  'Ġseries',\n",
       "  'Ġ.',\n",
       "  'ĠEmploy',\n",
       "  'ing',\n",
       "  'Ġthe',\n",
       "  'Ġsame',\n",
       "  'Ġfusion',\n",
       "  'Ġof',\n",
       "  'Ġtactical',\n",
       "  'Ġand',\n",
       "  'Ġreal',\n",
       "  'Ġ@',\n",
       "  '-',\n",
       "  '@',\n",
       "  'Ġtime',\n",
       "  'Ġgameplay',\n",
       "  'Ġas',\n",
       "  'Ġits',\n",
       "  'Ġpredecessors',\n",
       "  'Ġ,',\n",
       "  'Ġthe',\n",
       "  'Ġstory',\n",
       "  'Ġruns',\n",
       "  'Ġparallel',\n",
       "  'Ġto',\n",
       "  'Ġthe',\n",
       "  'Ġfirst',\n",
       "  'Ġgame',\n",
       "  'Ġand',\n",
       "  'Ġfollows',\n",
       "  'Ġthe',\n",
       "  'Ġ\"',\n",
       "  'ĠNam',\n",
       "  'eless',\n",
       "  'Ġ\"',\n",
       "  'Ġ,',\n",
       "  'Ġa',\n",
       "  'Ġpenal',\n",
       "  'Ġmilitary',\n",
       "  'Ġunit',\n",
       "  'Ġserving',\n",
       "  'Ġthe',\n",
       "  'Ġnation',\n",
       "  'Ġof',\n",
       "  'ĠGall',\n",
       "  'ia',\n",
       "  'Ġduring',\n",
       "  'Ġthe',\n",
       "  'ĠSecond',\n",
       "  'ĠEuro',\n",
       "  'pan',\n",
       "  'ĠWar',\n",
       "  'Ġwho',\n",
       "  'Ġperform',\n",
       "  'Ġsecret',\n",
       "  'Ġblack',\n",
       "  'Ġoperations',\n",
       "  'Ġand',\n",
       "  'Ġare',\n",
       "  'Ġpitted',\n",
       "  'Ġagainst',\n",
       "  'Ġthe',\n",
       "  'ĠImperial',\n",
       "  'Ġunit',\n",
       "  'Ġ\"',\n",
       "  'ĠCal',\n",
       "  'am',\n",
       "  'at',\n",
       "  'y',\n",
       "  'ĠRaven',\n",
       "  'Ġ\"',\n",
       "  'Ġ.',\n",
       "  'Ġ',\n",
       "  'Ċ',\n",
       "  '<|endoftext|>'],\n",
       " 'input_ids_raw': [2311,\n",
       "  73,\n",
       "  13090,\n",
       "  645,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  513,\n",
       "  1058,\n",
       "  791,\n",
       "  47398,\n",
       "  17740,\n",
       "  357,\n",
       "  4960,\n",
       "  1058,\n",
       "  10545,\n",
       "  230,\n",
       "  99,\n",
       "  161,\n",
       "  254,\n",
       "  112,\n",
       "  5641,\n",
       "  44444,\n",
       "  9202,\n",
       "  25084,\n",
       "  24440,\n",
       "  12675,\n",
       "  11839,\n",
       "  18,\n",
       "  837,\n",
       "  6578,\n",
       "  764,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  286,\n",
       "  262,\n",
       "  30193,\n",
       "  513,\n",
       "  1267,\n",
       "  837,\n",
       "  8811,\n",
       "  6412,\n",
       "  284,\n",
       "  355,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  17740,\n",
       "  6711,\n",
       "  2354,\n",
       "  2869,\n",
       "  837,\n",
       "  318,\n",
       "  257,\n",
       "  16106,\n",
       "  2597,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  2712,\n",
       "  2008,\n",
       "  983,\n",
       "  4166,\n",
       "  416,\n",
       "  29490,\n",
       "  290,\n",
       "  6343,\n",
       "  13,\n",
       "  44206,\n",
       "  329,\n",
       "  262,\n",
       "  14047,\n",
       "  44685,\n",
       "  764,\n",
       "  28728,\n",
       "  287,\n",
       "  3269,\n",
       "  2813,\n",
       "  287,\n",
       "  2869,\n",
       "  837,\n",
       "  340,\n",
       "  318,\n",
       "  262,\n",
       "  2368,\n",
       "  983,\n",
       "  287,\n",
       "  262,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  2168,\n",
       "  764,\n",
       "  12645,\n",
       "  278,\n",
       "  262,\n",
       "  976,\n",
       "  21748,\n",
       "  286,\n",
       "  16106,\n",
       "  290,\n",
       "  1103,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  640,\n",
       "  11327,\n",
       "  355,\n",
       "  663,\n",
       "  27677,\n",
       "  837,\n",
       "  262,\n",
       "  1621,\n",
       "  4539,\n",
       "  10730,\n",
       "  284,\n",
       "  262,\n",
       "  717,\n",
       "  983,\n",
       "  290,\n",
       "  5679,\n",
       "  262,\n",
       "  366,\n",
       "  17871,\n",
       "  5321,\n",
       "  366,\n",
       "  837,\n",
       "  257,\n",
       "  23634,\n",
       "  2422,\n",
       "  4326,\n",
       "  7351,\n",
       "  262,\n",
       "  3277,\n",
       "  286,\n",
       "  7096,\n",
       "  544,\n",
       "  1141,\n",
       "  262,\n",
       "  5498,\n",
       "  1898,\n",
       "  6839,\n",
       "  1810,\n",
       "  508,\n",
       "  1620,\n",
       "  3200,\n",
       "  2042,\n",
       "  4560,\n",
       "  290,\n",
       "  389,\n",
       "  46852,\n",
       "  1028,\n",
       "  262,\n",
       "  11773,\n",
       "  4326,\n",
       "  366,\n",
       "  2199,\n",
       "  321,\n",
       "  265,\n",
       "  88,\n",
       "  12552,\n",
       "  366,\n",
       "  764,\n",
       "  220,\n",
       "  198],\n",
       " 'output_ids_raw': [73,\n",
       "  13090,\n",
       "  645,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  513,\n",
       "  1058,\n",
       "  791,\n",
       "  47398,\n",
       "  17740,\n",
       "  357,\n",
       "  4960,\n",
       "  1058,\n",
       "  10545,\n",
       "  230,\n",
       "  99,\n",
       "  161,\n",
       "  254,\n",
       "  112,\n",
       "  5641,\n",
       "  44444,\n",
       "  9202,\n",
       "  25084,\n",
       "  24440,\n",
       "  12675,\n",
       "  11839,\n",
       "  18,\n",
       "  837,\n",
       "  6578,\n",
       "  764,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  286,\n",
       "  262,\n",
       "  30193,\n",
       "  513,\n",
       "  1267,\n",
       "  837,\n",
       "  8811,\n",
       "  6412,\n",
       "  284,\n",
       "  355,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  17740,\n",
       "  6711,\n",
       "  2354,\n",
       "  2869,\n",
       "  837,\n",
       "  318,\n",
       "  257,\n",
       "  16106,\n",
       "  2597,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  2712,\n",
       "  2008,\n",
       "  983,\n",
       "  4166,\n",
       "  416,\n",
       "  29490,\n",
       "  290,\n",
       "  6343,\n",
       "  13,\n",
       "  44206,\n",
       "  329,\n",
       "  262,\n",
       "  14047,\n",
       "  44685,\n",
       "  764,\n",
       "  28728,\n",
       "  287,\n",
       "  3269,\n",
       "  2813,\n",
       "  287,\n",
       "  2869,\n",
       "  837,\n",
       "  340,\n",
       "  318,\n",
       "  262,\n",
       "  2368,\n",
       "  983,\n",
       "  287,\n",
       "  262,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  2168,\n",
       "  764,\n",
       "  12645,\n",
       "  278,\n",
       "  262,\n",
       "  976,\n",
       "  21748,\n",
       "  286,\n",
       "  16106,\n",
       "  290,\n",
       "  1103,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  640,\n",
       "  11327,\n",
       "  355,\n",
       "  663,\n",
       "  27677,\n",
       "  837,\n",
       "  262,\n",
       "  1621,\n",
       "  4539,\n",
       "  10730,\n",
       "  284,\n",
       "  262,\n",
       "  717,\n",
       "  983,\n",
       "  290,\n",
       "  5679,\n",
       "  262,\n",
       "  366,\n",
       "  17871,\n",
       "  5321,\n",
       "  366,\n",
       "  837,\n",
       "  257,\n",
       "  23634,\n",
       "  2422,\n",
       "  4326,\n",
       "  7351,\n",
       "  262,\n",
       "  3277,\n",
       "  286,\n",
       "  7096,\n",
       "  544,\n",
       "  1141,\n",
       "  262,\n",
       "  5498,\n",
       "  1898,\n",
       "  6839,\n",
       "  1810,\n",
       "  508,\n",
       "  1620,\n",
       "  3200,\n",
       "  2042,\n",
       "  4560,\n",
       "  290,\n",
       "  389,\n",
       "  46852,\n",
       "  1028,\n",
       "  262,\n",
       "  11773,\n",
       "  4326,\n",
       "  366,\n",
       "  2199,\n",
       "  321,\n",
       "  265,\n",
       "  88,\n",
       "  12552,\n",
       "  366,\n",
       "  764,\n",
       "  220,\n",
       "  198,\n",
       "  50256],\n",
       " 'input_ids': [50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  2311,\n",
       "  73,\n",
       "  13090,\n",
       "  645,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  513,\n",
       "  1058,\n",
       "  791,\n",
       "  47398,\n",
       "  17740,\n",
       "  357,\n",
       "  4960,\n",
       "  1058,\n",
       "  10545,\n",
       "  230,\n",
       "  99,\n",
       "  161,\n",
       "  254,\n",
       "  112,\n",
       "  5641,\n",
       "  44444,\n",
       "  9202,\n",
       "  25084,\n",
       "  24440,\n",
       "  12675,\n",
       "  11839,\n",
       "  18,\n",
       "  837,\n",
       "  6578,\n",
       "  764,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  286,\n",
       "  262,\n",
       "  30193,\n",
       "  513,\n",
       "  1267,\n",
       "  837,\n",
       "  8811,\n",
       "  6412,\n",
       "  284,\n",
       "  355,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  17740,\n",
       "  6711,\n",
       "  2354,\n",
       "  2869,\n",
       "  837,\n",
       "  318,\n",
       "  257,\n",
       "  16106,\n",
       "  2597,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  2712,\n",
       "  2008,\n",
       "  983,\n",
       "  4166,\n",
       "  416,\n",
       "  29490,\n",
       "  290,\n",
       "  6343,\n",
       "  13,\n",
       "  44206,\n",
       "  329,\n",
       "  262,\n",
       "  14047,\n",
       "  44685,\n",
       "  764,\n",
       "  28728,\n",
       "  287,\n",
       "  3269,\n",
       "  2813,\n",
       "  287,\n",
       "  2869,\n",
       "  837,\n",
       "  340,\n",
       "  318,\n",
       "  262,\n",
       "  2368,\n",
       "  983,\n",
       "  287,\n",
       "  262,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  2168,\n",
       "  764,\n",
       "  12645,\n",
       "  278,\n",
       "  262,\n",
       "  976,\n",
       "  21748,\n",
       "  286,\n",
       "  16106,\n",
       "  290,\n",
       "  1103,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  640,\n",
       "  11327,\n",
       "  355,\n",
       "  663,\n",
       "  27677,\n",
       "  837,\n",
       "  262,\n",
       "  1621,\n",
       "  4539,\n",
       "  10730,\n",
       "  284,\n",
       "  262,\n",
       "  717,\n",
       "  983,\n",
       "  290,\n",
       "  5679,\n",
       "  262,\n",
       "  366,\n",
       "  17871,\n",
       "  5321,\n",
       "  366,\n",
       "  837,\n",
       "  257,\n",
       "  23634,\n",
       "  2422,\n",
       "  4326,\n",
       "  7351,\n",
       "  262,\n",
       "  3277,\n",
       "  286,\n",
       "  7096,\n",
       "  544,\n",
       "  1141,\n",
       "  262,\n",
       "  5498,\n",
       "  1898,\n",
       "  ...],\n",
       " 'output_ids': [50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  73,\n",
       "  13090,\n",
       "  645,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  513,\n",
       "  1058,\n",
       "  791,\n",
       "  47398,\n",
       "  17740,\n",
       "  357,\n",
       "  4960,\n",
       "  1058,\n",
       "  10545,\n",
       "  230,\n",
       "  99,\n",
       "  161,\n",
       "  254,\n",
       "  112,\n",
       "  5641,\n",
       "  44444,\n",
       "  9202,\n",
       "  25084,\n",
       "  24440,\n",
       "  12675,\n",
       "  11839,\n",
       "  18,\n",
       "  837,\n",
       "  6578,\n",
       "  764,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  286,\n",
       "  262,\n",
       "  30193,\n",
       "  513,\n",
       "  1267,\n",
       "  837,\n",
       "  8811,\n",
       "  6412,\n",
       "  284,\n",
       "  355,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  17740,\n",
       "  6711,\n",
       "  2354,\n",
       "  2869,\n",
       "  837,\n",
       "  318,\n",
       "  257,\n",
       "  16106,\n",
       "  2597,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  2712,\n",
       "  2008,\n",
       "  983,\n",
       "  4166,\n",
       "  416,\n",
       "  29490,\n",
       "  290,\n",
       "  6343,\n",
       "  13,\n",
       "  44206,\n",
       "  329,\n",
       "  262,\n",
       "  14047,\n",
       "  44685,\n",
       "  764,\n",
       "  28728,\n",
       "  287,\n",
       "  3269,\n",
       "  2813,\n",
       "  287,\n",
       "  2869,\n",
       "  837,\n",
       "  340,\n",
       "  318,\n",
       "  262,\n",
       "  2368,\n",
       "  983,\n",
       "  287,\n",
       "  262,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  2168,\n",
       "  764,\n",
       "  12645,\n",
       "  278,\n",
       "  262,\n",
       "  976,\n",
       "  21748,\n",
       "  286,\n",
       "  16106,\n",
       "  290,\n",
       "  1103,\n",
       "  2488,\n",
       "  12,\n",
       "  31,\n",
       "  640,\n",
       "  11327,\n",
       "  355,\n",
       "  663,\n",
       "  27677,\n",
       "  837,\n",
       "  262,\n",
       "  1621,\n",
       "  4539,\n",
       "  10730,\n",
       "  284,\n",
       "  262,\n",
       "  717,\n",
       "  983,\n",
       "  290,\n",
       "  5679,\n",
       "  262,\n",
       "  366,\n",
       "  17871,\n",
       "  5321,\n",
       "  366,\n",
       "  837,\n",
       "  257,\n",
       "  23634,\n",
       "  2422,\n",
       "  4326,\n",
       "  7351,\n",
       "  262,\n",
       "  3277,\n",
       "  286,\n",
       "  7096,\n",
       "  544,\n",
       "  1141,\n",
       "  262,\n",
       "  5498,\n",
       "  1898,\n",
       "  6839,\n",
       "  ...],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f56f03995e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a Hugging Face Dataset to be used with a PyTorch DataLoader.\n",
    "\n",
    "    Assumes the Hugging Face dataset has 'input' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset: HFDataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item['input_ids'], item['output_ids'], item['attention_mask']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    output_ids = [item[1] for item in batch]\n",
    "    attention_mask = [item[2] for item in batch]\n",
    "\n",
    "    \n",
    "\n",
    "    # set dtype to long\n",
    "\n",
    "    input_ids_list = torch.tensor(input_ids, dtype=torch.long)\n",
    "    output_ids_list = torch.tensor(output_ids, dtype=torch.long)\n",
    "    attention_mask_list = torch.tensor(attention_mask, dtype=torch.long)\n",
    "    return input_ids_list, output_ids_list, attention_mask_list\n",
    "\n",
    "batch_size = 20\n",
    "train_torch_dataset = HuggingFaceDataset(tokenized_train_dataset)\n",
    "val_torch_dataset = HuggingFaceDataset(tokenized_val_dataset)\n",
    "test_torch_dataset = HuggingFaceDataset(tokenized_test_dataset)\n",
    "\n",
    "train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    train_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    val_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "    test_torch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "train_torch_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1024]), torch.Size([20, 1024]), torch.Size([20, 1024]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_torch_dataloader)) # (input_ids, output_ids)\n",
    "input_ids, output_ids, attention_mask = batch\n",
    "input_ids.shape, output_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (token_embedding): Embedding(50257, 768)\n",
       "  (position_embedding): Embedding(1024, 768)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (self_attention_block): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_project): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import GPT2\n",
    "\n",
    "num_train_batches = tokenized_train_dataset.num_rows // batch_size\n",
    "num_val_batches = tokenized_val_dataset.num_rows // batch_size\n",
    "\n",
    "config = {\n",
    "        \"emb_dim\": 768,\n",
    "        \"heads\": 12,\n",
    "        \"layers\": 12,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        \"drop_out\": 0.1,\n",
    "        \"train_test_split\": 0.8,\n",
    "        \"num_epochs\": 25,\n",
    "        \"model_path\": \"../model_files/gpt2_large\",\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"num_train_batches\" : num_train_batches,\n",
    "        \"learning_rate\" : 1e-4,\n",
    "        \"num_test_batches\" : num_val_batches,\n",
    "    }\n",
    "\n",
    "gpt2 = GPT2(config)\n",
    "gpt2.to(config['device'])\n",
    "gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1 batch 1 of num_batches 1188 Average batch loss: 10.862738609313965 Perplexity: 52194.82421875\n",
      "At epoch 1 batch 100 of num_batches 1188 Average batch loss: 10.533329162597656 Perplexity: 37546.2578125\n",
      "At epoch 1 batch 200 of num_batches 1188 Average batch loss: 10.490552954673767 Perplexity: 35974.04296875\n",
      "At epoch 1 batch 300 of num_batches 1188 Average batch loss: 10.465597756703694 Perplexity: 35087.42578125\n",
      "At epoch 1 batch 400 of num_batches 1188 Average batch loss: 10.449358038902282 Perplexity: 34522.203125\n",
      "At epoch 1 batch 500 of num_batches 1188 Average batch loss: 10.43773995399475 Perplexity: 34123.45703125\n",
      "At epoch 1 batch 600 of num_batches 1188 Average batch loss: 10.428114438056946 Perplexity: 33796.578125\n",
      "At epoch 1 batch 700 of num_batches 1188 Average batch loss: 10.419732453482492 Perplexity: 33514.453125\n",
      "At epoch 1 batch 800 of num_batches 1188 Average batch loss: 10.414098329544068 Perplexity: 33326.1875\n",
      "At epoch 1 batch 900 of num_batches 1188 Average batch loss: 10.407231124242147 Perplexity: 33098.10546875\n",
      "At epoch 1 batch 1000 of num_batches 1188 Average batch loss: 10.401897539138794 Perplexity: 32922.03515625\n",
      "At epoch 1 batch 1100 of num_batches 1188 Average batch loss: 10.398295443274758 Perplexity: 32803.66015625\n",
      "At epoch 1 batch 1 of num_batches 123 Average test loss: 10.494869232177734\n",
      "At epoch 1 batch 100 of num_batches 123 Average test loss: 10.356963996887208\n",
      "Test loss without mask: at epoch 0 10.365756919307094 Test perplexity without mask: 31753.4609375\n",
      "At epoch 2 batch 1 of num_batches 1188 Average batch loss: 10.269684791564941 Perplexity: 28844.79296875\n",
      "At epoch 2 batch 100 of num_batches 1188 Average batch loss: 10.322439203262329 Perplexity: 30407.3359375\n",
      "At epoch 2 batch 200 of num_batches 1188 Average batch loss: 10.310570373535157 Perplexity: 30048.580078125\n",
      "At epoch 2 batch 300 of num_batches 1188 Average batch loss: 10.310957797368367 Perplexity: 30060.21875\n",
      "At epoch 2 batch 400 of num_batches 1188 Average batch loss: 10.308732068538665 Perplexity: 29993.3828125\n",
      "At epoch 2 batch 500 of num_batches 1188 Average batch loss: 10.30497462272644 Perplexity: 29880.89453125\n",
      "At epoch 2 batch 600 of num_batches 1188 Average batch loss: 10.302897523244223 Perplexity: 29818.892578125\n",
      "At epoch 2 batch 700 of num_batches 1188 Average batch loss: 10.301998916353499 Perplexity: 29792.1171875\n",
      "At epoch 2 batch 800 of num_batches 1188 Average batch loss: 10.300700491666793 Perplexity: 29753.4453125\n",
      "At epoch 2 batch 900 of num_batches 1188 Average batch loss: 10.299367933273315 Perplexity: 29713.830078125\n",
      "At epoch 2 batch 1000 of num_batches 1188 Average batch loss: 10.29656536579132 Perplexity: 29630.6640625\n",
      "At epoch 2 batch 1100 of num_batches 1188 Average batch loss: 10.293861895474521 Perplexity: 29550.689453125\n",
      "At epoch 2 batch 1 of num_batches 123 Average test loss: 10.459834098815918\n",
      "At epoch 2 batch 100 of num_batches 123 Average test loss: 10.320776376724243\n",
      "Test loss without mask: at epoch 1 10.331431934910436 Test perplexity without mask: 30682.02734375\n",
      "At epoch 3 batch 1 of num_batches 1188 Average batch loss: 10.117594718933105 Perplexity: 24775.107421875\n",
      "At epoch 3 batch 100 of num_batches 1188 Average batch loss: 10.218407077789307 Perplexity: 27402.970703125\n",
      "At epoch 3 batch 200 of num_batches 1188 Average batch loss: 10.22404040336609 Perplexity: 27557.775390625\n",
      "At epoch 3 batch 300 of num_batches 1188 Average batch loss: 10.22993816057841 Perplexity: 27720.8046875\n",
      "At epoch 3 batch 400 of num_batches 1188 Average batch loss: 10.235361380577087 Perplexity: 27871.533203125\n",
      "At epoch 3 batch 500 of num_batches 1188 Average batch loss: 10.239627593994141 Perplexity: 27990.70703125\n",
      "At epoch 3 batch 600 of num_batches 1188 Average batch loss: 10.239360295931498 Perplexity: 27983.20703125\n",
      "At epoch 3 batch 700 of num_batches 1188 Average batch loss: 10.237894434247698 Perplexity: 27942.21875\n",
      "At epoch 3 batch 800 of num_batches 1188 Average batch loss: 10.234887938499451 Perplexity: 27858.3515625\n",
      "At epoch 3 batch 900 of num_batches 1188 Average batch loss: 10.230284990734525 Perplexity: 27730.404296875\n",
      "At epoch 3 batch 1000 of num_batches 1188 Average batch loss: 10.229470045089721 Perplexity: 27707.828125\n",
      "At epoch 3 batch 1100 of num_batches 1188 Average batch loss: 10.230301166014238 Perplexity: 27730.853515625\n",
      "At epoch 3 batch 1 of num_batches 123 Average test loss: 10.446915626525879\n",
      "At epoch 3 batch 100 of num_batches 123 Average test loss: 10.30496256828308\n",
      "Test loss without mask: at epoch 2 10.316205601538382 Test perplexity without mask: 30218.390625\n",
      "At epoch 4 batch 1 of num_batches 1188 Average batch loss: 10.188637733459473 Perplexity: 26599.234375\n",
      "At epoch 4 batch 100 of num_batches 1188 Average batch loss: 10.175157175064086 Perplexity: 26243.078125\n",
      "At epoch 4 batch 200 of num_batches 1188 Average batch loss: 10.175383653640747 Perplexity: 26249.009765625\n",
      "At epoch 4 batch 300 of num_batches 1188 Average batch loss: 10.175521408716838 Perplexity: 26252.640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_torch_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_torch_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fp_16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GPT2/notebooks/utils.py:34\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_data, test_data, config, use_fp_16)\u001b[39m\n\u001b[32m     31\u001b[39m     loss = criterion(predictions, target)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_fp_16:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     scaler.step(optimizer)\n\u001b[32m     36\u001b[39m     scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/root/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/root/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/root/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(gpt2, train_torch_dataloader, val_torch_dataloader, config, use_fp_16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1]], device='cuda:0')\n",
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 15496,   616,  1438,   318]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 100, 50257])\n"
     ]
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(\"Hello my name is\", truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "attention_mask = tokenized['attention_mask'].to(config[\"device\"])\n",
    "input_ids = tokenized['input_ids'].to(config[\"device\"])\n",
    "\n",
    "print(attention_mask)\n",
    "print(input_ids)\n",
    "\n",
    "prediction = gpt2(input_ids)\n",
    "next_token = prediction.argmax(dim=-1)\n",
    "\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital is ->  a species of the<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(starting_text, model, tokenizer, config, num_output_tokens=100):\n",
    "    device = config[\"device\"]\n",
    "    output_tokens = []\n",
    "\n",
    "    input_encoding = tokenizer(starting_text, truncation=True, max_length=1024, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding['input_ids'].to(device)\n",
    "    \n",
    "    output_text = f\"{starting_text} -> \"\n",
    "    for _ in range(num_output_tokens):\n",
    "        \n",
    "        next_token_logits = model(input_ids)[:,-1,:]\n",
    "        next_token = next_token_logits.argmax(dim=-1)\n",
    "\n",
    "        output_tokens.append(next_token.item())\n",
    "\n",
    "        next_token = next_token.to(device)\n",
    "        next_token_decoded = tokenizer.decode(next_token.item())\n",
    "        output_text += next_token_decoded\n",
    "        \n",
    "\n",
    "        # Append the predicted token to the input for the next iteration\n",
    "        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)\n",
    "        input_ids = input_ids[:, -1024:]\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #output_text += next_text\n",
    "    print(output_text)\n",
    "\n",
    "generate_text(\"The capital is\", gpt2, wrapped_tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer.encode(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
