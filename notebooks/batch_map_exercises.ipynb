{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55782ee58b7b45d382ba7a07f4d1713f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 4, 'c': 7, 'd': 2}\n",
      "{'a': 2, 'b': 5, 'c': 8, 'd': 4}\n",
      "{'a': 3, 'b': 6, 'c': 9, 'd': 6}\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({\n",
    "    \"a\": [1, 2, 3],\n",
    "    \"b\": [4, 5, 6],\n",
    "    \"c\": [7, 8, 9]\n",
    "})\n",
    "\n",
    "mapped_dataset= dataset.map(\n",
    "    lambda x: {\"d\": [i * 2 for i in x[\"a\"]]},\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print the mapped dataset with keys and values\n",
    "for row in mapped_dataset:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(type(tokenizer))\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.add_prefix_space = True  # Ensure the tokenizer adds a space before tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb69439dd77437d8129e4a3ae025990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text: hello this is ajay\n",
      "input_words: ['hello', 'Ġthis', 'Ġis', 'Ġa']\n",
      "output_words: ['Ġthis', 'Ġis', 'Ġa', 'jay']\n",
      "input_ids_raw: [31373, 428, 318, 257]\n",
      "output_ids_raw: [428, 318, 257, 33708]\n",
      "tokens: ['hello', 'Ġthis', 'Ġis', 'Ġa', 'jay']\n",
      "input_text: hello this is a\n",
      "output_text:  this is ajay\n",
      "input_ids: [50257, 50257, 50257, 50257, 50257, 50257, 31373, 428, 318, 257]\n",
      "output_ids: [50257, 50257, 50257, 50257, 50257, 50257, 428, 318, 257, 33708]\n"
     ]
    }
   ],
   "source": [
    "def slide_window(text_batch):\n",
    "    text_batch['input_words'] = []\n",
    "    text_batch['output_words'] = []\n",
    "    text_batch['input_ids_raw'] = []\n",
    "    text_batch['output_ids_raw'] = []\n",
    "    text_batch['tokens'] = []\n",
    "    text_batch['input_text'] = []\n",
    "    text_batch['output_text'] = []\n",
    "\n",
    "    text_batch['input_ids'] = []\n",
    "    text_batch['output_ids'] = []\n",
    "\n",
    "    for text in text_batch['raw_text']:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        text_batch['tokens'].append(tokens)\n",
    "\n",
    "        # Create input and output tokens for sliding window\n",
    "        input_tokens = tokens[:-1]\n",
    "        output_tokens = tokens[1:]\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        output_ids = tokenizer.convert_tokens_to_ids(output_tokens)\n",
    "\n",
    "        text_batch['input_words'].append(input_tokens)\n",
    "        text_batch['output_words'].append(output_tokens)\n",
    "\n",
    "        text_batch['input_ids_raw'].append(input_ids)\n",
    "        text_batch['output_ids_raw'].append(output_ids)\n",
    "    \n",
    "        input_text = tokenizer.convert_tokens_to_string(input_tokens)\n",
    "        output_text = tokenizer.convert_tokens_to_string(output_tokens)\n",
    "        \n",
    "        text_batch['input_text'].append(input_text)\n",
    "        text_batch['output_text'].append(output_text)\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=10).input_ids[0]\n",
    "        output_ids = tokenizer(output_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=10).input_ids[0]\n",
    "\n",
    "        text_batch['input_ids'].append(input_ids)\n",
    "        text_batch['output_ids'].append(output_ids)\n",
    "\n",
    "\n",
    "    return text_batch \n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"raw_text\": [\"hello this is ajay\"],\n",
    "})\n",
    "\n",
    "slided_window_dataset = dataset.map(\n",
    "    slide_window,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "for row in slided_window_dataset:\n",
    "    for key, value in row.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Let', \"'s\", 'Ġtest', 'Ġthis', 'Ġtoken', 'izer', '.']\n",
      "Input IDs: [5756, 338, 1332, 428, 11241, 7509, 13]\n",
      "Input IDs with Special Tokens: [5756, 338, 1332, 428, 11241, 7509, 13]\n",
      "Padded Input IDs: [5756, 338, 1332, 428, 11241, 7509, 13, 50256, 50256, 50256]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token to EOS (GPT-2 does not have a default PAD token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Input text\n",
    "text = \"Let's test this tokenizer.\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "# Example output: ['Let', \"'s\", 'test', 'this', 'tokenizer', '.']\n",
    "\n",
    "# Step 2: Conversion to Input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "# Example output: [1212, 39, 2649, 428, 25641, 13]\n",
    "\n",
    "# Step 3: Adding Special Tokens (GPT-2 does not use special tokens like [CLS] or [SEP])\n",
    "input_ids_with_special_tokens = input_ids\n",
    "print(\"Input IDs with Special Tokens:\", input_ids_with_special_tokens)\n",
    "\n",
    "# Step 4: Padding (manually pad to a fixed length, e.g., 10)\n",
    "max_length = 10\n",
    "padded_input_ids = input_ids_with_special_tokens + [tokenizer.pad_token_id] * (max_length - len(input_ids_with_special_tokens))\n",
    "print(\"Padded Input IDs:\", padded_input_ids)\n",
    "# Example output: [1212, 39, 2649, 428, 25641, 13, 50256, 50256, 50256, 50256]\n",
    "\n",
    "# Step 5: Generating Attention Mask\n",
    "attention_mask = [1 if token != tokenizer.pad_token_id else 0 for token in padded_input_ids]\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "# Example output: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 5756,   338,  1332,   428, 11241,  7509,    13],\n",
      "        [16438,  2420,    13, 50256, 50256, 50256, 50256]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajrfhp/anaconda3/envs/GPT2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Batch processing with automatic padding\n",
    "encoded_inputs = tokenizer(\n",
    "    [\"Let's test this tokenizer.\", \"Short text.\"],\n",
    "    padding=True,\n",
    "    max_length=10,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encoded_inputs['input_ids'])\n",
    "# Example output:\n",
    "# tensor([[1212,   39, 2649,  428, 25641,   13, 50256, 50256, 50256, 50256],\n",
    "#         [1212,   39,   13,     ...,     ...,     ...,     ...,     ...]])\n",
    "\n",
    "print(\"Attention Mask:\", encoded_inputs['attention_mask'])\n",
    "# Example output:\n",
    "# tensor([[1,   1,   1,   ...,   ...,   ...,   ...],\n",
    "#         [1 , ..., ..., ..., ..., ..., ...]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
